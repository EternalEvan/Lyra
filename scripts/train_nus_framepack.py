import torch
import torch.nn as nn
import lightning as pl
import wandb
import os
import copy
from diffsynth import WanVideoReCamMasterPipeline, ModelManager
import os
import json
import torch
import numpy as np
from PIL import Image
import imageio
import random
from torchvision.transforms import v2
from einops import rearrange
from pose_classifier import PoseClassifier

class DynamicNuScenesDataset(torch.utils.data.Dataset):
    """æ”¯æŒFramePackæœºåˆ¶çš„åŠ¨æ€å†å²é•¿åº¦NuScenesæ•°æ®é›†"""
    
    def __init__(self, base_path, steps_per_epoch, 
                 min_condition_frames=10, max_condition_frames=40,
                 target_frames=10, height=900, width=1600):
        self.base_path = base_path
        self.scenes_path = os.path.join(base_path, "scenes")
        self.min_condition_frames = min_condition_frames
        self.max_condition_frames = max_condition_frames
        self.target_frames = target_frames
        self.height = height
        self.width = width
        self.steps_per_epoch = steps_per_epoch
        self.pose_classifier = PoseClassifier()
        
        # ğŸ”§ æ–°å¢ï¼šVAEæ—¶é—´å‹ç¼©æ¯”ä¾‹
        self.time_compression_ratio = 4  # VAEå°†æ—¶é—´ç»´åº¦å‹ç¼©4å€
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤„ç†å¥½çš„åœºæ™¯
        self.scene_dirs = []
        if os.path.exists(self.scenes_path):
            for item in os.listdir(self.scenes_path):
                scene_dir = os.path.join(self.scenes_path, item)
                if os.path.isdir(scene_dir):
                    scene_info_path = os.path.join(scene_dir, "scene_info.json")
                    if os.path.exists(scene_info_path):
                        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼–ç çš„tensoræ–‡ä»¶
                        encoded_path = os.path.join(scene_dir, "encoded_video-480p.pth")
                        if os.path.exists(encoded_path):
                            self.scene_dirs.append(scene_dir)
        
        assert len(self.scene_dirs) > 0, "No encoded scenes found!"

    def calculate_relative_rotation(self, current_rotation, reference_rotation):
        """è®¡ç®—ç›¸å¯¹æ—‹è½¬å››å…ƒæ•°"""
        q_current = torch.tensor(current_rotation, dtype=torch.float32)
        q_ref = torch.tensor(reference_rotation, dtype=torch.float32)

        q_ref_inv = torch.tensor([q_ref[0], -q_ref[1], -q_ref[2], -q_ref[3]])

        w1, x1, y1, z1 = q_ref_inv
        w2, x2, y2, z2 = q_current

        relative_rotation = torch.tensor([
            w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,
            w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,
            w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2,
            w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2
        ])

        return relative_rotation
    
    def select_dynamic_segment_framepack(self, scene_info):
        """ğŸ”§ FramePacké£æ ¼çš„åŠ¨æ€é€‰æ‹©æ¡ä»¶å¸§å’Œç›®æ ‡å¸§"""
        keyframe_indices = scene_info['keyframe_indices']  # åŸå§‹å¸§ç´¢å¼•
        total_frames = scene_info['total_frames']  # åŸå§‹æ€»å¸§æ•°
        
        if len(keyframe_indices) < 2:
            print('error1____________')
            return None
        
        # ğŸ”§ è®¡ç®—å‹ç¼©åçš„å¸§æ•°
        compressed_total_frames = total_frames // self.time_compression_ratio
        compressed_keyframe_indices = [idx // self.time_compression_ratio for idx in keyframe_indices]
        
        # éšæœºé€‰æ‹©æ¡ä»¶å¸§é•¿åº¦ï¼ˆåŸºäºå‹ç¼©åçš„å¸§æ•°ï¼‰
        min_condition_compressed = self.min_condition_frames // self.time_compression_ratio
        max_condition_compressed = self.max_condition_frames // self.time_compression_ratio
        target_frames_compressed = self.target_frames // self.time_compression_ratio
        
        # ğŸ”§ FramePacké£æ ¼çš„é‡‡æ ·ç­–ç•¥
        ratio = random.random()
        print('ratio:', ratio)
        if ratio < 0.15:
            condition_frames_compressed = 1
        elif 0.15 <= ratio < 0.9:
            condition_frames_compressed = random.randint(min_condition_compressed, max_condition_compressed)
        else:
            condition_frames_compressed = target_frames_compressed
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§æ•°
        min_required_frames = condition_frames_compressed + target_frames_compressed
        if compressed_total_frames < min_required_frames:
            print(f"å‹ç¼©åå¸§æ•°ä¸è¶³: {compressed_total_frames} < {min_required_frames}")
            return None
        
        # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®ï¼ˆåŸºäºå‹ç¼©åçš„å¸§æ•°ï¼‰
        max_start = compressed_total_frames - min_required_frames - 1
        start_frame_compressed = random.randint(0, max_start)
        
        condition_end_compressed = start_frame_compressed + condition_frames_compressed
        target_end_compressed = condition_end_compressed + target_frames_compressed
        
        # ğŸ”§ FramePacké£æ ¼çš„ç´¢å¼•å¤„ç†
        latent_indices = torch.arange(condition_end_compressed, target_end_compressed)  # åªé¢„æµ‹æœªæ¥å¸§
        
        # 1xå¸§ï¼šèµ·å§‹å¸§ + æœ€å1å¸§
        clean_latent_indices_start = torch.tensor([start_frame_compressed])
        clean_latent_1x_indices = torch.tensor([condition_end_compressed - 1])
        clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices])
        
        # ğŸ”§ 2xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®š
        if condition_frames_compressed >= 2:
            # å–æœ€å2å¸§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            clean_latent_2x_start = max(start_frame_compressed, condition_end_compressed - 2-1)
            clean_latent_2x_indices = torch.arange(clean_latent_2x_start, condition_end_compressed-1)
        else:
            # å¦‚æœconditionå¸§æ•°ä¸è¶³2å¸§ï¼Œåˆ›å»ºç©ºç´¢å¼•
            clean_latent_2x_indices = torch.tensor([], dtype=torch.long)
        
        # ğŸ”§ 4xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®šï¼Œæœ€å¤š16å¸§
        if condition_frames_compressed > 3:
            # å–æœ€å¤š16å¸§çš„å†å²ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            clean_4x_start = max(start_frame_compressed, condition_end_compressed - 16-3)
            clean_latent_4x_indices = torch.arange(clean_4x_start, condition_end_compressed-3)
        else:
            clean_latent_4x_indices = torch.tensor([], dtype=torch.long)
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨å‹ç¼©ç©ºé—´ä¸­æŸ¥æ‰¾å…³é”®å¸§
        condition_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                        if start_frame_compressed <= idx < condition_end_compressed]
        
        target_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                     if condition_end_compressed <= idx < target_end_compressed]
        
        if not condition_keyframes_compressed:
            print(f"æ¡ä»¶æ®µå†…æ— å…³é”®å¸§: {start_frame_compressed}-{condition_end_compressed}")
            return None
        
        # ä½¿ç”¨æ¡ä»¶æ®µçš„æœ€åä¸€ä¸ªå…³é”®å¸§ä½œä¸ºreference
        reference_keyframe_compressed = max(condition_keyframes_compressed)
        
        # ğŸ”§ æ‰¾åˆ°å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•ç”¨äºposeæŸ¥æ‰¾
        reference_keyframe_original_idx = None
        for i, compressed_idx in enumerate(compressed_keyframe_indices):
            if compressed_idx == reference_keyframe_compressed:
                reference_keyframe_original_idx = i
                break
        
        if reference_keyframe_original_idx is None:
            print(f"æ— æ³•æ‰¾åˆ°referenceå…³é”®å¸§çš„åŸå§‹ç´¢å¼•")
            return None
        
        # æ‰¾åˆ°ç›®æ ‡æ®µå¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        target_keyframes_original_indices = []
        for compressed_idx in target_keyframes_compressed:
            for i, comp_idx in enumerate(compressed_keyframe_indices):
                if comp_idx == compressed_idx:
                    target_keyframes_original_indices.append(i)
                    break
        
        return {
            'start_frame': start_frame_compressed,  # å‹ç¼©åçš„èµ·å§‹å¸§
            'condition_frames': condition_frames_compressed,  # å‹ç¼©åçš„æ¡ä»¶å¸§æ•°
            'target_frames': target_frames_compressed,  # å‹ç¼©åçš„ç›®æ ‡å¸§æ•°
            'condition_range': (start_frame_compressed, condition_end_compressed),
            'target_range': (condition_end_compressed, target_end_compressed),
            'reference_keyframe_idx': reference_keyframe_original_idx,  # åŸå§‹å…³é”®å¸§ç´¢å¼•
            'target_keyframe_indices': target_keyframes_original_indices,  # åŸå§‹å…³é”®å¸§ç´¢å¼•åˆ—è¡¨
            'original_condition_frames': condition_frames_compressed * self.time_compression_ratio,  # ç”¨äºè®°å½•
            'original_target_frames': target_frames_compressed * self.time_compression_ratio,
            
            # ğŸ”§ FramePacké£æ ¼çš„ç´¢å¼•
            'latent_indices': latent_indices,
            'clean_latent_indices': clean_latent_indices,
            'clean_latent_2x_indices': clean_latent_2x_indices,
            'clean_latent_4x_indices': clean_latent_4x_indices,
        }

    def create_pose_embeddings(self, scene_info, segment_info):
        """ğŸ”§ ä¸ºæ‰€æœ‰å¸§ï¼ˆcondition + targetï¼‰åˆ›å»ºpose embeddings - FramePacké£æ ¼"""
        keyframe_poses = scene_info['keyframe_poses']
        reference_keyframe_idx = segment_info['reference_keyframe_idx']
        target_keyframe_indices = segment_info['target_keyframe_indices']
        
        if reference_keyframe_idx >= len(keyframe_poses):
            return None
        
        reference_pose = keyframe_poses[reference_keyframe_idx]
        
        # ğŸ”§ ä¸ºæ‰€æœ‰å¸§ï¼ˆcondition + targetï¼‰è®¡ç®—pose embeddings
        start_frame = segment_info['start_frame']
        condition_end_compressed = start_frame + segment_info['condition_frames']
        target_end_compressed = condition_end_compressed + segment_info['target_frames']
        
        # å‹ç¼©åçš„å…³é”®å¸§ç´¢å¼•
        compressed_keyframe_indices = [idx // self.time_compression_ratio for idx in scene_info['keyframe_indices']]
        
        # æ‰¾åˆ°conditionæ®µçš„å…³é”®å¸§
        condition_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                        if start_frame <= idx < condition_end_compressed]
        
        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        condition_keyframes_original_indices = []
        for compressed_idx in condition_keyframes_compressed:
            for i, comp_idx in enumerate(compressed_keyframe_indices):
                if comp_idx == compressed_idx:
                    condition_keyframes_original_indices.append(i)
                    break
        
        pose_vecs = []
        frame_types = []
        
        # ğŸ”§ ä¸ºconditionå¸§è®¡ç®—pose
        for i in range(segment_info['condition_frames']):
            if not condition_keyframes_original_indices:
                translation = torch.zeros(3, dtype=torch.float32)
                rotation = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)
            else:
                # ä¸ºconditionå¸§åˆ†é…pose
                if len(condition_keyframes_original_indices) == 1:
                    keyframe_idx = condition_keyframes_original_indices[0]
                else:
                    if segment_info['condition_frames'] == 1:
                        keyframe_idx = condition_keyframes_original_indices[0]
                    else:
                        interp_ratio = i / (segment_info['condition_frames'] - 1)
                        interp_idx = int(interp_ratio * (len(condition_keyframes_original_indices) - 1))
                        keyframe_idx = condition_keyframes_original_indices[interp_idx]
                
                if keyframe_idx >= len(keyframe_poses):
                    translation = torch.zeros(3, dtype=torch.float32)
                    rotation = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)
                else:
                    condition_pose = keyframe_poses[keyframe_idx]
                    
                    translation = torch.tensor(
                        np.array(condition_pose['translation']) - np.array(reference_pose['translation']),
                        dtype=torch.float32
                    )
                    
                    relative_rotation = self.calculate_relative_rotation(
                        condition_pose['rotation'],
                        reference_pose['rotation']
                    )
                    
                    rotation = relative_rotation
            
            # ğŸ”§ æ·»åŠ frame type embeddingï¼š0è¡¨ç¤ºcondition
            pose_vec = torch.cat([translation, rotation, torch.tensor([0.0], dtype=torch.float32)], dim=0)  # [8D]
            pose_vecs.append(pose_vec)
            frame_types.append('condition')
        
        # ğŸ”§ ä¸ºtargetå¸§è®¡ç®—pose
        if not target_keyframe_indices:
            for i in range(segment_info['target_frames']):
                pose_vec = torch.cat([
                    torch.zeros(3, dtype=torch.float32),
                    torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32),
                    torch.tensor([1.0], dtype=torch.float32)  # frame type: 1è¡¨ç¤ºtarget
                ], dim=0)
                pose_vecs.append(pose_vec)
                frame_types.append('target')
        else:
            for i in range(segment_info['target_frames']):
                if len(target_keyframe_indices) == 1:
                    target_keyframe_idx = target_keyframe_indices[0]
                else:
                    if segment_info['target_frames'] == 1:
                        target_keyframe_idx = target_keyframe_indices[0]
                    else:
                        interp_ratio = i / (segment_info['target_frames'] - 1)
                        interp_idx = int(interp_ratio * (len(target_keyframe_indices) - 1))
                        target_keyframe_idx = target_keyframe_indices[interp_idx]
                
                if target_keyframe_idx >= len(keyframe_poses):
                    pose_vec = torch.cat([
                        torch.zeros(3, dtype=torch.float32),
                        torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32),
                        torch.tensor([1.0], dtype=torch.float32)
                    ], dim=0)
                else:
                    target_pose = keyframe_poses[target_keyframe_idx]
                    
                    relative_translation = torch.tensor(
                        np.array(target_pose['translation']) - np.array(reference_pose['translation']),
                        dtype=torch.float32
                    )
                    
                    relative_rotation = self.calculate_relative_rotation(
                        target_pose['rotation'],
                        reference_pose['rotation']
                    )
                    
                    # ğŸ”§ æ·»åŠ frame type embeddingï¼š1è¡¨ç¤ºtarget
                    pose_vec = torch.cat([
                        relative_translation, 
                        relative_rotation, 
                        torch.tensor([1.0], dtype=torch.float32)
                    ], dim=0)
                
                pose_vecs.append(pose_vec)
                frame_types.append('target')
        
        if not pose_vecs:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•poseå‘é‡")
            return None
        
        pose_sequence = torch.stack(pose_vecs, dim=0)  # [total_frames, 8]
        
        # ğŸ”§ åªå¯¹targetéƒ¨åˆ†è¿›è¡Œåˆ†ç±»åˆ†æ
        target_pose_sequence = pose_sequence[segment_info['condition_frames']:, :7]
        
        if target_pose_sequence.numel() == 0:
            print("âŒ Target poseåºåˆ—ä¸ºç©º")
            return None
        
        # ä½¿ç”¨åˆ†ç±»å™¨åˆ†ætargetéƒ¨åˆ†
        pose_analysis = self.pose_classifier.analyze_pose_sequence(target_pose_sequence)
        
        # ğŸ”§ åˆ›å»ºå®Œæ•´çš„ç±»åˆ«embedding
        condition_classes = torch.full((segment_info['condition_frames'],), 0, dtype=torch.long)
        target_classes = pose_analysis['classifications']
        
        full_classes = torch.cat([condition_classes, target_classes], dim=0)
        
        # ğŸ”§ åˆ›å»ºenhanced class embedding
        class_embeddings = self.create_enhanced_class_embedding(
            full_classes, pose_sequence, embed_dim=512
        )
        
        return class_embeddings

    def create_enhanced_class_embedding(self, class_labels: torch.Tensor, pose_sequence: torch.Tensor, embed_dim: int = 512) -> torch.Tensor:
        """åˆ›å»ºå¢å¼ºçš„ç±»åˆ«embedding"""
        num_classes = 4
        num_frames = len(class_labels)
        
        direction_vectors = torch.tensor([
            [1.0, 0.0, 0.0, 0.0],
            [-1.0, 0.0, 0.0, 0.0],
            [0.0, 1.0, 0.0, 0.0],
            [0.0, -1.0, 0.0, 0.0],
        ], dtype=torch.float32)
        
        one_hot = torch.zeros(num_frames, num_classes)
        one_hot.scatter_(1, class_labels.unsqueeze(1), 1)
        
        base_embeddings = one_hot @ direction_vectors
        
        frame_types = pose_sequence[:, -1]
        frame_type_embeddings = torch.zeros(num_frames, 2)
        frame_type_embeddings[:, 0] = (frame_types == 0).float()
        frame_type_embeddings[:, 1] = (frame_types == 1).float()
        
        translations = pose_sequence[:, :3]
        rotations = pose_sequence[:, 3:7]
        
        combined_features = torch.cat([
            base_embeddings,
            frame_type_embeddings,
            translations,
            rotations,
        ], dim=1)
        
        if embed_dim > 13:
            expand_matrix = torch.randn(13, embed_dim) * 0.1
            expand_matrix[:13, :13] = torch.eye(13)
            embeddings = combined_features @ expand_matrix
        else:
            embeddings = combined_features[:, :embed_dim]
        
        return embeddings

    def prepare_framepack_inputs(self, full_latents, segment_info):
        """ğŸ”§ å‡†å¤‡FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥"""
        if len(full_latents.shape) == 4:
            full_latents = full_latents.unsqueeze(0)
            B, C, T, H, W = full_latents.shape
        else:
            B, C, T, H, W = full_latents.shape
        
        # ä¸»è¦latentsï¼ˆç”¨äºå»å™ªé¢„æµ‹ï¼‰
        latent_indices = segment_info['latent_indices']
        main_latents = full_latents[:, :, latent_indices, :, :]
        
        # ğŸ”§ 1xæ¡ä»¶å¸§ï¼ˆèµ·å§‹å¸§ + æœ€å1å¸§ï¼‰
        clean_latent_indices = segment_info['clean_latent_indices']
        clean_latents = full_latents[:, :, clean_latent_indices, :, :]
        
        # ğŸ”§ 4xæ¡ä»¶å¸§ - æ€»æ˜¯16å¸§ï¼Œç›´æ¥ç”¨çœŸå®ç´¢å¼• + 0å¡«å……
        clean_latent_4x_indices = segment_info['clean_latent_4x_indices']
        
        clean_latents_4x = torch.zeros(B, C, 16, H, W, dtype=full_latents.dtype)
        clean_latent_4x_indices_final = torch.full((16,), -1, dtype=torch.long)
        
        if len(clean_latent_4x_indices) > 0:
            actual_4x_frames = len(clean_latent_4x_indices)
            start_pos = max(0, 16 - actual_4x_frames)
            end_pos = 16
            actual_start = max(0, actual_4x_frames - 16)
            
            clean_latents_4x[:, :, start_pos:end_pos, :, :] = full_latents[:, :, clean_latent_4x_indices[actual_start:], :, :]
            clean_latent_4x_indices_final[start_pos:end_pos] = clean_latent_4x_indices[actual_start:]
        
        # ğŸ”§ 2xæ¡ä»¶å¸§ - æ€»æ˜¯2å¸§ï¼Œç›´æ¥ç”¨çœŸå®ç´¢å¼• + 0å¡«å……
        clean_latent_2x_indices = segment_info['clean_latent_2x_indices']
        
        clean_latents_2x = torch.zeros(B, C, 2, H, W, dtype=full_latents.dtype)
        clean_latent_2x_indices_final = torch.full((2,), -1, dtype=torch.long)
        
        if len(clean_latent_2x_indices) > 0:
            actual_2x_frames = len(clean_latent_2x_indices)
            start_pos = max(0, 2 - actual_2x_frames)
            end_pos = 2
            actual_start = max(0, actual_2x_frames - 2)
            
            clean_latents_2x[:, :, start_pos:end_pos, :, :] = full_latents[:, :, clean_latent_2x_indices[actual_start:], :, :]
            clean_latent_2x_indices_final[start_pos:end_pos] = clean_latent_2x_indices[actual_start:]
        
        # ç§»é™¤batchç»´åº¦
        if B == 1:
            main_latents = main_latents.squeeze(0)
            clean_latents = clean_latents.squeeze(0)
            clean_latents_2x = clean_latents_2x.squeeze(0)
            clean_latents_4x = clean_latents_4x.squeeze(0)
        
        return {
            'latents': main_latents,
            'clean_latents': clean_latents,
            'clean_latents_2x': clean_latents_2x,
            'clean_latents_4x': clean_latents_4x,
            'latent_indices': segment_info['latent_indices'],
            'clean_latent_indices': segment_info['clean_latent_indices'],
            'clean_latent_2x_indices': clean_latent_2x_indices_final,
            'clean_latent_4x_indices': clean_latent_4x_indices_final,
        }

    def __getitem__(self, index):
        while True:
            try:
                # éšæœºé€‰æ‹©ä¸€ä¸ªåœºæ™¯
                scene_dir = random.choice(self.scene_dirs)
                
                # åŠ è½½åœºæ™¯ä¿¡æ¯
                with open(os.path.join(scene_dir, "scene_info.json"), 'r') as f:
                    scene_info = json.load(f)
                
                # åŠ è½½ç¼–ç çš„è§†é¢‘æ•°æ®
                encoded_data = torch.load(
                    os.path.join(scene_dir, "encoded_video-480p.pth"),
                    weights_only=True,
                    map_location="cpu"
                )
                
                full_latents = encoded_data['latents']  # [C, T, H, W]
                expected_latent_frames = scene_info['total_frames'] // self.time_compression_ratio
                actual_latent_frames = full_latents.shape[1]
                
                if abs(actual_latent_frames - expected_latent_frames) > 2:
                    print(f"âš ï¸  Latentå¸§æ•°ä¸åŒ¹é…ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                    continue
                
                # ğŸ”§ ä½¿ç”¨FramePacké£æ ¼çš„æ®µè½é€‰æ‹©
                segment_info = self.select_dynamic_segment_framepack(scene_info)
                if segment_info is None:
                    continue
                
                # ğŸ”§ åˆ›å»ºpose embeddings
                pose_embeddings = self.create_pose_embeddings(scene_info, segment_info)
                if pose_embeddings is None:
                    continue
                
                # ğŸ”§ å‡†å¤‡FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥
                framepack_inputs = self.prepare_framepack_inputs(full_latents, segment_info)
                
                n = segment_info["condition_frames"]
                m = segment_info['target_frames']
                
                # ğŸ”§ æ·»åŠ maskåˆ°pose embeddings
                mask = torch.zeros(n+m, dtype=torch.float32)
                mask[:n] = 1.0
                mask = mask.view(-1, 1)
                
                camera_with_mask = torch.cat([pose_embeddings, mask], dim=1)
                
                result = {
                    # ğŸ”§ FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥
                    "latents": framepack_inputs['latents'],
                    "clean_latents": framepack_inputs['clean_latents'],
                    "clean_latents_2x": framepack_inputs['clean_latents_2x'],
                    "clean_latents_4x": framepack_inputs['clean_latents_4x'],
                    "latent_indices": framepack_inputs['latent_indices'],
                    "clean_latent_indices": framepack_inputs['clean_latent_indices'],
                    "clean_latent_2x_indices": framepack_inputs['clean_latent_2x_indices'],
                    "clean_latent_4x_indices": framepack_inputs['clean_latent_4x_indices'],
                    
                    # Cameraæ•°æ®
                    "camera": camera_with_mask,
                    
                    "prompt_emb": encoded_data["prompt_emb"],
                    "image_emb": encoded_data.get("image_emb", {}),
                    "condition_frames": n,
                    "target_frames": m,
                    "scene_name": os.path.basename(scene_dir),
                    "original_condition_frames": segment_info['original_condition_frames'],
                    "original_target_frames": segment_info['original_target_frames'],
                }
                
                return result
                
            except Exception as e:
                print(f"Error loading sample: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    def __len__(self):
        return self.steps_per_epoch

def replace_dit_model_in_manager():
    """åœ¨æ¨¡å‹åŠ è½½å‰æ›¿æ¢DiTæ¨¡å‹ç±»"""
    from diffsynth.models.wan_video_dit_recam_future import WanModelFuture
    from diffsynth.configs.model_config import model_loader_configs
    
    for i, config in enumerate(model_loader_configs):
        keys_hash, keys_hash_with_shape, model_names, model_classes, model_resource = config
        
        if 'wan_video_dit' in model_names:
            new_model_names = []
            new_model_classes = []
            
            for name, cls in zip(model_names, model_classes):
                if name == 'wan_video_dit':
                    new_model_names.append(name)
                    new_model_classes.append(WanModelFuture)
                    print(f"âœ… æ›¿æ¢äº†æ¨¡å‹ç±»: {name} -> WanModelFuture")
                else:
                    new_model_names.append(name)
                    new_model_classes.append(cls)
            
            model_loader_configs[i] = (keys_hash, keys_hash_with_shape, new_model_names, new_model_classes, model_resource)

class DynamicLightningModelForTrain(pl.LightningModule):
    def __init__(
        self,
        dit_path,
        learning_rate=1e-5,
        use_gradient_checkpointing=True,
        use_gradient_checkpointing_offload=False,
        resume_ckpt_path=None
    ):
        super().__init__()
        replace_dit_model_in_manager()  # ğŸ”§ åœ¨è¿™é‡Œè°ƒç”¨
        model_manager = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        if os.path.isfile(dit_path):
            model_manager.load_models([dit_path])
        else:
            dit_path = dit_path.split(",")
            model_manager.load_models([dit_path])
        model_manager.load_models(["models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth"])
        
        self.pipe = WanVideoReCamMasterPipeline.from_model_manager(model_manager)
        self.pipe.scheduler.set_timesteps(1000, training=True)

        # ğŸ”§ æ·»åŠ FramePackçš„clean_x_embedder
        self.add_framepack_components()

        # æ·»åŠ ç›¸æœºç¼–ç å™¨
        dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
        for block in self.pipe.dit.blocks:
            block.cam_encoder = nn.Linear(513, dim)  # 512 + 1 for mask
            block.projector = nn.Linear(dim, dim)
            block.cam_encoder.weight.data.zero_()
            block.cam_encoder.bias.data.zero_()
            block.projector.weight = nn.Parameter(torch.eye(dim))
            block.projector.bias = nn.Parameter(torch.zeros(dim))
        
        if resume_ckpt_path is not None:
            state_dict = torch.load(resume_ckpt_path, map_location="cpu")
            self.pipe.dit.load_state_dict(state_dict, strict=True)
            print('load checkpoint:', resume_ckpt_path)

        self.freeze_parameters()
        
        # åªè®­ç»ƒç›¸æœºç›¸å…³å’Œæ³¨æ„åŠ›æ¨¡å—ä»¥åŠFramePackç›¸å…³ç»„ä»¶
        for name, module in self.pipe.denoising_model().named_modules():
            if any(keyword in name for keyword in ["cam_encoder", "projector", "self_attn", "clean_x_embedder"]):
                for param in module.parameters():
                    param.requires_grad = True
        
        self.learning_rate = learning_rate
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.use_gradient_checkpointing_offload = use_gradient_checkpointing_offload
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        self.vis_dir = "nus/visualizations_dynamic_framepack"
        os.makedirs(self.vis_dir, exist_ok=True)

    def add_framepack_components(self):
        """ğŸ”§ æ·»åŠ FramePackç›¸å…³ç»„ä»¶"""
        if not hasattr(self.pipe.dit, 'clean_x_embedder'):
            inner_dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
            
            class CleanXEmbedder(nn.Module):
                def __init__(self, inner_dim):
                    super().__init__()
                    self.proj = nn.Conv3d(16, inner_dim, kernel_size=(1, 2, 2), stride=(1, 2, 2))
                    self.proj_2x = nn.Conv3d(16, inner_dim, kernel_size=(2, 4, 4), stride=(2, 4, 4))
                    self.proj_4x = nn.Conv3d(16, inner_dim, kernel_size=(4, 8, 8), stride=(4, 8, 8))
                
                def forward(self, x, scale="1x"):
                    if scale == "1x":
                        return self.proj(x)
                    elif scale == "2x":
                        return self.proj_2x(x)
                    elif scale == "4x":
                        return self.proj_4x(x)
                    else:
                        raise ValueError(f"Unsupported scale: {scale}")
            
            self.pipe.dit.clean_x_embedder = CleanXEmbedder(inner_dim)
            print("âœ… æ·»åŠ äº†FramePackçš„clean_x_embedderç»„ä»¶")
        
    def freeze_parameters(self):
        self.pipe.requires_grad_(False)
        self.pipe.eval()
        self.pipe.denoising_model().train()

    def training_step(self, batch, batch_idx):
        """ğŸ”§ ä½¿ç”¨FramePacké£æ ¼çš„è®­ç»ƒæ­¥éª¤"""
        condition_frames = batch["condition_frames"][0].item()
        target_frames = batch["target_frames"][0].item()
        
        original_condition_frames = batch.get("original_condition_frames", [condition_frames * 4])[0]
        original_target_frames = batch.get("original_target_frames", [target_frames * 4])[0]

        scene_name = batch.get("scene_name", ["unknown"])[0]
        
        # ğŸ”§ å‡†å¤‡FramePacké£æ ¼çš„è¾“å…¥
        latents = batch["latents"].to(self.device)
        if len(latents.shape) == 4:
            latents = latents.unsqueeze(0)
        
        clean_latents = batch["clean_latents"].to(self.device) if batch["clean_latents"].numel() > 0 else None
        if clean_latents is not None and len(clean_latents.shape) == 4:
            clean_latents = clean_latents.unsqueeze(0)
        
        clean_latents_2x = batch["clean_latents_2x"].to(self.device) if batch["clean_latents_2x"].numel() > 0 else None
        if clean_latents_2x is not None and len(clean_latents_2x.shape) == 4:
            clean_latents_2x = clean_latents_2x.unsqueeze(0)
        
        clean_latents_4x = batch["clean_latents_4x"].to(self.device) if batch["clean_latents_4x"].numel() > 0 else None
        if clean_latents_4x is not None and len(clean_latents_4x.shape) == 4:
            clean_latents_4x = clean_latents_4x.unsqueeze(0)
        
        # ç´¢å¼•
        latent_indices = batch["latent_indices"].to(self.device)
        clean_latent_indices = batch["clean_latent_indices"].to(self.device) if batch["clean_latent_indices"].numel() > 0 else None
        clean_latent_2x_indices = batch["clean_latent_2x_indices"].to(self.device) if batch["clean_latent_2x_indices"].numel() > 0 else None
        clean_latent_4x_indices = batch["clean_latent_4x_indices"].to(self.device) if batch["clean_latent_4x_indices"].numel() > 0 else None
        
        # Camera embedding
        cam_emb = batch["camera"].to(self.device)
        camera_dropout_prob = 0.1
        if random.random() < camera_dropout_prob:
            cam_emb = torch.zeros_like(cam_emb)
            print("åº”ç”¨camera dropout for CFG training")
        
        prompt_emb = batch["prompt_emb"]
        prompt_emb["context"] = prompt_emb["context"][0].to(self.device)
        image_emb = batch["image_emb"]

        if "clip_feature" in image_emb:
            image_emb["clip_feature"] = image_emb["clip_feature"][0].to(self.device)
        if "y" in image_emb:
            image_emb["y"] = image_emb["y"][0].to(self.device)

        # Lossè®¡ç®—
        self.pipe.device = self.device
        noise = torch.randn_like(latents)
        timestep_id = torch.randint(0, self.pipe.scheduler.num_train_timesteps, (1,))
        timestep = self.pipe.scheduler.timesteps[timestep_id].to(dtype=self.pipe.torch_dtype, device=self.pipe.device)
        
        # ğŸ”§ FramePacké£æ ¼çš„å™ªå£°å¤„ç†
        noisy_condition_latents = None
        if clean_latents is not None:
            noisy_condition_latents = copy.deepcopy(clean_latents)
            is_add_noise = random.random()
            if is_add_noise > 0.2:
                noise_cond = torch.randn_like(clean_latents)
                timestep_id_cond = torch.randint(0, self.pipe.scheduler.num_train_timesteps//4*3, (1,))
                timestep_cond = self.pipe.scheduler.timesteps[timestep_id_cond].to(dtype=self.pipe.torch_dtype, device=self.pipe.device)
                noisy_condition_latents = self.pipe.scheduler.add_noise(clean_latents, noise_cond, timestep_cond)

        extra_input = self.pipe.prepare_extra_input(latents)
        origin_latents = copy.deepcopy(latents)
        noisy_latents = self.pipe.scheduler.add_noise(latents, noise, timestep)
        
        training_target = self.pipe.scheduler.training_target(latents, noise, timestep)
        
        # ğŸ”§ ä½¿ç”¨FramePacké£æ ¼çš„forwardè°ƒç”¨
        noise_pred = self.pipe.denoising_model()(
            noisy_latents, 
            timestep=timestep, 
            cam_emb=cam_emb,
            # FramePacké£æ ¼çš„æ¡ä»¶è¾“å…¥
            latent_indices=latent_indices,
            clean_latents=noisy_condition_latents if noisy_condition_latents is not None else clean_latents,
            clean_latent_indices=clean_latent_indices,
            clean_latents_2x=clean_latents_2x,
            clean_latent_2x_indices=clean_latent_2x_indices,
            clean_latents_4x=clean_latents_4x,
            clean_latent_4x_indices=clean_latent_4x_indices,
            **prompt_emb, 
            **extra_input, 
            **image_emb,
            use_gradient_checkpointing=self.use_gradient_checkpointing,
            use_gradient_checkpointing_offload=self.use_gradient_checkpointing_offload
        )
        
        # è®¡ç®—loss
        loss = torch.nn.functional.mse_loss(noise_pred.float(), training_target.float())
        loss = loss * self.pipe.scheduler.training_weight(timestep)
        print('--------loss------------:', loss)

        # è®°å½•ä¿¡æ¯
        wandb.log({
            "train_loss": loss.item(),
            "timestep": timestep.item(),
            "condition_frames_compressed": condition_frames,
            "target_frames_compressed": target_frames,
            "condition_frames_original": original_condition_frames,
            "target_frames_original": original_target_frames,
            "has_clean_latents": clean_latents is not None,
            "has_clean_latents_2x": clean_latents_2x is not None,
            "has_clean_latents_4x": clean_latents_4x is not None,
            "total_frames_compressed": target_frames,
            "total_frames_original": original_target_frames,
            "scene_name": scene_name,
            "global_step": self.global_step
        })

        return loss

    def configure_optimizers(self):
        trainable_modules = filter(lambda p: p.requires_grad, self.pipe.denoising_model().parameters())
        optimizer = torch.optim.AdamW(trainable_modules, lr=self.learning_rate)
        return optimizer
    
    def on_save_checkpoint(self, checkpoint):
        checkpoint_dir = "/home/zhuyixuan05/ReCamMaster/nus_dynamic_framepack"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        current_step = self.global_step
        checkpoint.clear()
        
        state_dict = self.pipe.denoising_model().state_dict()
        torch.save(state_dict, os.path.join(checkpoint_dir, f"step{current_step}_framepack.ckpt"))
        print(f"Saved FramePack model checkpoint: step{current_step}_framepack.ckpt")

def train_dynamic(args):
    """è®­ç»ƒæ”¯æŒFramePackæœºåˆ¶çš„åŠ¨æ€å†å²é•¿åº¦æ¨¡å‹"""
    dataset = DynamicNuScenesDataset(
        args.dataset_path,
        steps_per_epoch=args.steps_per_epoch,
        min_condition_frames=args.min_condition_frames,
        max_condition_frames=args.max_condition_frames,
        target_frames=args.target_frames,
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle=True,
        batch_size=1,
        num_workers=args.dataloader_num_workers
    )
    
    model = DynamicLightningModelForTrain(
        dit_path=args.dit_path,
        learning_rate=args.learning_rate,
        use_gradient_checkpointing=args.use_gradient_checkpointing,
        use_gradient_checkpointing_offload=args.use_gradient_checkpointing_offload,
        resume_ckpt_path=args.resume_ckpt_path,
    )

    wandb.init(
        project="nuscenes-dynamic-framepack-recam",
        name=f"framepack-{args.min_condition_frames}-{args.max_condition_frames}",
    )

    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        accelerator="gpu",
        devices="auto",
        precision="bf16",
        strategy=args.training_strategy,
        default_root_dir=args.output_path,
        accumulate_grad_batches=args.accumulate_grad_batches,
        callbacks=[],
    )
    trainer.fit(model, dataloader)

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description="Train FramePack Dynamic ReCamMaster for NuScenes")
    parser.add_argument("--dataset_path", type=str, default="/share_zhuyixuan05/zhuyixuan05/nuscenes_video_generation_dynamic")
    parser.add_argument("--dit_path", type=str, default="models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors")
    parser.add_argument("--output_path", type=str, default="./")
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    parser.add_argument("--steps_per_epoch", type=int, default=3000)
    parser.add_argument("--max_epochs", type=int, default=10)
    parser.add_argument("--min_condition_frames", type=int, default=8, help="æœ€å°æ¡ä»¶å¸§æ•°")
    parser.add_argument("--max_condition_frames", type=int, default=120, help="æœ€å¤§æ¡ä»¶å¸§æ•°")
    parser.add_argument("--target_frames", type=int, default=32, help="ç›®æ ‡å¸§æ•°")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    parser.add_argument("--accumulate_grad_batches", type=int, default=1)
    parser.add_argument("--training_strategy", type=str, default="deepspeed_stage_1")
    parser.add_argument("--use_gradient_checkpointing", action="store_true")
    parser.add_argument("--use_gradient_checkpointing_offload", action="store_true")
    parser.add_argument("--resume_ckpt_path", type=str, default=None)
    
    args = parser.parse_args()
    
    print("ğŸ”§ ä½¿ç”¨FramePacké£æ ¼è®­ç»ƒNuScenesæ•°æ®é›†:")
    print(f"  - æ”¯æŒå¤šå°ºåº¦ä¸‹é‡‡æ ·(1x/2x/4x)")
    print(f"  - ä½¿ç”¨WanModelFutureæ¨¡å‹")
    print(f"  - æ•°æ®é›†è·¯å¾„: {args.dataset_path}")
    
    train_dynamic(args)