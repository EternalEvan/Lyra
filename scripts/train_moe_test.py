#èåˆnusceneså’Œsekaiæ•°æ®é›†çš„MoEè®­ç»ƒ
import torch
import torch.nn as nn
import lightning as pl
import wandb
import os
import copy
import json
import numpy as np
import random
import traceback
from diffsynth import WanVideoReCamMasterPipeline, ModelManager
from torchvision.transforms import v2
from einops import rearrange
from pose_classifier import PoseClassifier
import argparse
from scipy.spatial.transform import Rotation as R

def get_traj_position_change(cam_c2w, stride=1):
    positions = cam_c2w[:, :3, 3]
    
    traj_coord = []
    tarj_angle = []
    for i in range(0, len(positions) - 2 * stride):
        v1 = positions[i + stride] - positions[i]
        v2 = positions[i + 2 * stride] - positions[i + stride]

        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        if norm1 < 1e-6 or norm2 < 1e-6:
            continue

        cos_angle = np.dot(v1, v2) / (norm1 * norm2)
        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))

        traj_coord.append(v1)
        tarj_angle.append(angle)
    
    return traj_coord, tarj_angle

def get_traj_rotation_change(cam_c2w, stride=1):
    rotations = cam_c2w[:, :3, :3]
    
    traj_rot_angle = []
    for i in range(0, len(rotations) - stride):
        z1 = rotations[i][:, 2]
        z2 = rotations[i + stride][:, 2]

        norm1 = np.linalg.norm(z1)
        norm2 = np.linalg.norm(z2)
        if norm1 < 1e-6 or norm2 < 1e-6:
            continue

        cos_angle = np.dot(z1, z2) / (norm1 * norm2)
        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))
        traj_rot_angle.append(angle)

    return traj_rot_angle

def compute_relative_pose(pose_a, pose_b, use_torch=False):
    """è®¡ç®—ç›¸æœºBç›¸å¯¹äºç›¸æœºAçš„ç›¸å¯¹ä½å§¿çŸ©é˜µ"""
    assert pose_a.shape == (4, 4), f"ç›¸æœºAå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_a.shape}"
    assert pose_b.shape == (4, 4), f"ç›¸æœºBå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_b.shape}"
    
    if use_torch:
        if not isinstance(pose_a, torch.Tensor):
            pose_a = torch.from_numpy(pose_a).float()
        if not isinstance(pose_b, torch.Tensor):
            pose_b = torch.from_numpy(pose_b).float()
        
        pose_a_inv = torch.inverse(pose_a)
        relative_pose = torch.matmul(pose_b, pose_a_inv)
    else:
        if not isinstance(pose_a, np.ndarray):
            pose_a = np.array(pose_a, dtype=np.float32)
        if not isinstance(pose_b, np.ndarray):
            pose_b = np.array(pose_b, dtype=np.float32)
        
        pose_a_inv = np.linalg.inv(pose_a)
        relative_pose = np.matmul(pose_b, pose_a_inv)
    
    return relative_pose

def compute_relative_pose_matrix(pose1, pose2):
    """
    è®¡ç®—ç›¸é‚»ä¸¤å¸§çš„ç›¸å¯¹ä½å§¿ï¼Œè¿”å›3Ã—4çš„ç›¸æœºçŸ©é˜µ [R_rel | t_rel]
    
    å‚æ•°:
    pose1: ç¬¬iå¸§çš„ç›¸æœºä½å§¿ï¼Œå½¢çŠ¶ä¸º(7,)çš„æ•°ç»„ [tx1, ty1, tz1, qx1, qy1, qz1, qw1]
    pose2: ç¬¬i+1å¸§çš„ç›¸æœºä½å§¿ï¼Œå½¢çŠ¶ä¸º(7,)çš„æ•°ç»„ [tx2, ty2, tz2, qx2, qy2, qz2, qw2]
    
    è¿”å›:
    relative_matrix: 3Ã—4çš„ç›¸å¯¹ä½å§¿çŸ©é˜µï¼Œå‰3åˆ—æ˜¯æ—‹è½¬çŸ©é˜µR_relï¼Œç¬¬4åˆ—æ˜¯å¹³ç§»å‘é‡t_rel
    """
    # åˆ†ç¦»å¹³ç§»å‘é‡å’Œå››å…ƒæ•°
    t1 = pose1[:3]  # ç¬¬iå¸§å¹³ç§» [tx1, ty1, tz1]
    q1 = pose1[3:]  # ç¬¬iå¸§å››å…ƒæ•° [qx1, qy1, qz1, qw1]
    t2 = pose2[:3]  # ç¬¬i+1å¸§å¹³ç§»
    q2 = pose2[3:]  # ç¬¬i+1å¸§å››å…ƒæ•°
    
    # 1. è®¡ç®—ç›¸å¯¹æ—‹è½¬çŸ©é˜µ R_rel
    rot1 = R.from_quat(q1)  # ç¬¬iå¸§æ—‹è½¬
    rot2 = R.from_quat(q2)  # ç¬¬i+1å¸§æ—‹è½¬
    rot_rel = rot2 * rot1.inv()  # ç›¸å¯¹æ—‹è½¬ = åä¸€å¸§æ—‹è½¬ Ã— å‰ä¸€å¸§æ—‹è½¬çš„é€†
    R_rel = rot_rel.as_matrix()  # è½¬æ¢ä¸º3Ã—3çŸ©é˜µ
    
    # 2. è®¡ç®—ç›¸å¯¹å¹³ç§»å‘é‡ t_rel
    R1_T = rot1.as_matrix().T  # å‰ä¸€å¸§æ—‹è½¬çŸ©é˜µçš„è½¬ç½®ï¼ˆç­‰ä»·äºé€†ï¼‰
    t_rel = R1_T @ (t2 - t1)   # ç›¸å¯¹å¹³ç§» = R1^T Ã— (t2 - t1)
    
    # 3. ç»„åˆä¸º3Ã—4çŸ©é˜µ [R_rel | t_rel]
    relative_matrix = np.hstack([R_rel, t_rel.reshape(3, 1)])
    
    return relative_matrix

class MultiDatasetDynamicDataset(torch.utils.data.Dataset):
    """æ”¯æŒFramePackæœºåˆ¶çš„å¤šæ•°æ®é›†åŠ¨æ€å†å²é•¿åº¦æ•°æ®é›† - èåˆnusceneså’Œsekai"""
    
    def __init__(self, dataset_configs, steps_per_epoch, 
                 min_condition_frames=10, max_condition_frames=40,
                 target_frames=10, height=900, width=1600):
        """
        Args:
            dataset_configs: æ•°æ®é›†é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªé…ç½®åŒ…å« {
                'name': æ•°æ®é›†åç§°,
                'paths': æ•°æ®é›†è·¯å¾„åˆ—è¡¨,
                'type': æ•°æ®é›†ç±»å‹ ('sekai' æˆ– 'nuscenes'),
                'weight': é‡‡æ ·æƒé‡
            }
        """
        self.dataset_configs = dataset_configs
        self.min_condition_frames = min_condition_frames
        self.max_condition_frames = max_condition_frames
        self.target_frames = target_frames
        self.height = height
        self.width = width
        self.steps_per_epoch = steps_per_epoch
        self.pose_classifier = PoseClassifier()
        
        # VAEæ—¶é—´å‹ç¼©æ¯”ä¾‹
        self.time_compression_ratio = 4
        
        # ğŸ”§ æ‰«ææ‰€æœ‰æ•°æ®é›†ï¼Œå»ºç«‹ç»Ÿä¸€çš„åœºæ™¯ç´¢å¼•
        self.scene_dirs = []
        self.dataset_info = {}  # è®°å½•æ¯ä¸ªåœºæ™¯çš„æ•°æ®é›†ä¿¡æ¯
        self.dataset_weights = []  # æ¯ä¸ªåœºæ™¯çš„é‡‡æ ·æƒé‡
        
        total_scenes = 0
        
        for config in self.dataset_configs:
            dataset_name = config['name']
            dataset_paths = config['paths'] if isinstance(config['paths'], list) else [config['paths']]
            dataset_type = config['type']
            dataset_weight = config.get('weight', 1.0)
            
            print(f"ğŸ”§ æ‰«ææ•°æ®é›†: {dataset_name} (ç±»å‹: {dataset_type})")
            
            dataset_scenes = []
            for dataset_path in dataset_paths:
                print(f"  ğŸ“ æ£€æŸ¥è·¯å¾„: {dataset_path}")
                if os.path.exists(dataset_path):                    
                    if dataset_type == 'nuscenes':
                        # NuScenesä½¿ç”¨ base_path/scenes ç»“æ„
                        scenes_path = os.path.join(dataset_path, "scenes")
                        print(f"  ğŸ“‚ æ‰«æNuScenes scenesç›®å½•: {scenes_path}")
                        for item in os.listdir(scenes_path):
                            scene_dir = os.path.join(scenes_path, item)
                            if os.path.isdir(scene_dir):
                                self.scene_dirs.append(scene_dir)
                                dataset_scenes.append(scene_dir)
                                self.dataset_info[scene_dir] = {
                                    'name': dataset_name,
                                    'type': dataset_type,
                                    'weight': dataset_weight
                                }
                                self.dataset_weights.append(dataset_weight)

                    elif dataset_type == 'sekai':
                        # Sekaiç­‰å…¶ä»–æ•°æ®é›†ç›´æ¥æ‰«ææ ¹ç›®å½•
                        for item in os.listdir(dataset_path):
                            scene_dir = os.path.join(dataset_path, item)
                            if os.path.isdir(scene_dir):
                                encoded_path = os.path.join(scene_dir, "encoded_video.pth")
                                if os.path.exists(encoded_path):
                                    self.scene_dirs.append(scene_dir)
                                    dataset_scenes.append(scene_dir)
                                    self.dataset_info[scene_dir] = {
                                        'name': dataset_name,
                                        'type': dataset_type,
                                        'weight': dataset_weight
                                    }
                                    self.dataset_weights.append(dataset_weight)

                    elif dataset_type in ['sekai', 'spatialvid', 'openx']:  # ğŸ”§ æ·»åŠ openxç±»å‹
                        # Sekaiã€spatialvidã€OpenXç­‰æ•°æ®é›†ç›´æ¥æ‰«ææ ¹ç›®å½•
                        for item in os.listdir(dataset_path):
                            scene_dir = os.path.join(dataset_path, item)
                            if os.path.isdir(scene_dir):
                                encoded_path = os.path.join(scene_dir, "encoded_video.pth")
                                if os.path.exists(encoded_path):
                                    self.scene_dirs.append(scene_dir)
                                    dataset_scenes.append(scene_dir)
                                    self.dataset_info[scene_dir] = {
                                        'name': dataset_name,
                                        'type': dataset_type,
                                        'weight': dataset_weight
                                    }
                                    self.dataset_weights.append(dataset_weight)
                else:
                    print(f"  âŒ è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
                
                print(f"  âœ… æ‰¾åˆ° {len(dataset_scenes)} ä¸ªåœºæ™¯")
                total_scenes += len(dataset_scenes)
                    
        # ç»Ÿè®¡å„æ•°æ®é›†åœºæ™¯æ•°
        dataset_counts = {}
        for scene_dir in self.scene_dirs:
            dataset_name = self.dataset_info[scene_dir]['name']
            dataset_type = self.dataset_info[scene_dir]['type']
            key = f"{dataset_name} ({dataset_type})"
            dataset_counts[key] = dataset_counts.get(key, 0) + 1
        
        for dataset_key, count in dataset_counts.items():
            print(f"  - {dataset_key}: {count} ä¸ªåœºæ™¯")
        
        assert len(self.scene_dirs) > 0, "No encoded scenes found!"
        
        # ğŸ”§ è®¡ç®—é‡‡æ ·æ¦‚ç‡
        total_weight = sum(self.dataset_weights)
        self.sampling_probs = [w / total_weight for w in self.dataset_weights]

    def select_dynamic_segment_nuscenes(self, scene_info):
        """ğŸ”§ NuScenesä¸“ç”¨çš„FramePacké£æ ¼æ®µè½é€‰æ‹©"""
        keyframe_indices = scene_info['keyframe_indices']  # åŸå§‹å¸§ç´¢å¼•
        total_frames = scene_info['total_frames']  # åŸå§‹æ€»å¸§æ•°
        
        if len(keyframe_indices) < 2:
            return None
        
        # è®¡ç®—å‹ç¼©åçš„å¸§æ•°
        compressed_total_frames = total_frames // self.time_compression_ratio
        compressed_keyframe_indices = [idx // self.time_compression_ratio for idx in keyframe_indices]
        
        min_condition_compressed = self.min_condition_frames // self.time_compression_ratio
        max_condition_compressed = self.max_condition_frames // self.time_compression_ratio
        target_frames_compressed = self.target_frames // self.time_compression_ratio
        
        # FramePacké£æ ¼çš„é‡‡æ ·ç­–ç•¥
        ratio = random.random()
        if ratio < 0.15:
            condition_frames_compressed = 1
        elif 0.15 <= ratio < 0.9:
            condition_frames_compressed = random.randint(min_condition_compressed, max_condition_compressed)
        else:
            condition_frames_compressed = target_frames_compressed
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§æ•°
        min_required_frames = condition_frames_compressed + target_frames_compressed
        if compressed_total_frames < min_required_frames:
            return None
        
        start_frame_compressed = random.randint(0, compressed_total_frames - min_required_frames - 1)
        condition_end_compressed = start_frame_compressed + condition_frames_compressed
        target_end_compressed = condition_end_compressed + target_frames_compressed

        # FramePacké£æ ¼çš„ç´¢å¼•å¤„ç†
        latent_indices = torch.arange(condition_end_compressed, target_end_compressed)
        
        # 1xå¸§ï¼šèµ·å§‹å¸§ + æœ€å1å¸§
        clean_latent_indices_start = torch.tensor([start_frame_compressed])
        clean_latent_1x_indices = torch.tensor([condition_end_compressed - 1])
        clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices])
        
        # ğŸ”§ 2xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®š
        if condition_frames_compressed >= 2:
            # å–æœ€å2å¸§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            clean_latent_2x_start = max(start_frame_compressed, condition_end_compressed - 2)
            clean_latent_2x_indices = torch.arange(clean_latent_2x_start-1, condition_end_compressed-1)
        else:
            # å¦‚æœconditionå¸§æ•°ä¸è¶³2å¸§ï¼Œåˆ›å»ºç©ºç´¢å¼•
            clean_latent_2x_indices = torch.tensor([], dtype=torch.long)
        
        # ğŸ”§ 4xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®šï¼Œæœ€å¤š16å¸§
        if condition_frames_compressed >= 1:
            # å–æœ€å¤š16å¸§çš„å†å²ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            clean_4x_start = max(start_frame_compressed, condition_end_compressed - 16)
            clean_latent_4x_indices = torch.arange(clean_4x_start-3, condition_end_compressed-3)
        else:
            clean_latent_4x_indices = torch.tensor([], dtype=torch.long)
                    
        # ğŸ”§ NuScenesç‰¹æœ‰ï¼šæŸ¥æ‰¾å…³é”®å¸§ç´¢å¼•
        condition_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                        if start_frame_compressed <= idx < condition_end_compressed]
        
        target_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                    if condition_end_compressed <= idx < target_end_compressed]
        
        if not condition_keyframes_compressed:
            return None
        
        # ä½¿ç”¨æ¡ä»¶æ®µçš„æœ€åä¸€ä¸ªå…³é”®å¸§ä½œä¸ºreference
        reference_keyframe_compressed = max(condition_keyframes_compressed)
        
        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•ç”¨äºposeæŸ¥æ‰¾
        reference_keyframe_original_idx = None
        for i, compressed_idx in enumerate(compressed_keyframe_indices):
            if compressed_idx == reference_keyframe_compressed:
                reference_keyframe_original_idx = i
                break
        
        if reference_keyframe_original_idx is None:
            return None
        
        # æ‰¾åˆ°ç›®æ ‡æ®µå¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        target_keyframes_original_indices = []
        for compressed_idx in target_keyframes_compressed:
            for i, comp_idx in enumerate(compressed_keyframe_indices):
                if comp_idx == compressed_idx:
                    target_keyframes_original_indices.append(i)
                    break
        
        # å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        keyframe_original_idx = []
        for compressed_idx in range(start_frame_compressed, target_end_compressed):
            keyframe_original_idx.append(compressed_idx * 4)
        
        return {
            'start_frame': start_frame_compressed,
            'condition_frames': condition_frames_compressed,
            'target_frames': target_frames_compressed,
            'condition_range': (start_frame_compressed, condition_end_compressed),
            'target_range': (condition_end_compressed, target_end_compressed),
            
            # FramePacké£æ ¼çš„ç´¢å¼•
            'latent_indices': latent_indices,
            'clean_latent_indices': clean_latent_indices,
            'clean_latent_2x_indices': clean_latent_2x_indices,
            'clean_latent_4x_indices': clean_latent_4x_indices,
            
            'keyframe_original_idx': keyframe_original_idx,
            'original_condition_frames': condition_frames_compressed * self.time_compression_ratio,
            'original_target_frames': target_frames_compressed * self.time_compression_ratio,
            
            # ğŸ”§ NuScenesç‰¹æœ‰æ•°æ®
            'reference_keyframe_idx': reference_keyframe_original_idx,
            'target_keyframe_indices': target_keyframes_original_indices,
        }

    def calculate_relative_rotation(self, current_rotation, reference_rotation):
        """è®¡ç®—ç›¸å¯¹æ—‹è½¬å››å…ƒæ•° - NuScenesä¸“ç”¨"""
        q_current = torch.tensor(current_rotation, dtype=torch.float32)
        q_ref = torch.tensor(reference_rotation, dtype=torch.float32)

        q_ref_inv = torch.tensor([q_ref[0], -q_ref[1], -q_ref[2], -q_ref[3]])

        w1, x1, y1, z1 = q_ref_inv
        w2, x2, y2, z2 = q_current

        relative_rotation = torch.tensor([
            w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,
            w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,
            w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2,
            w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2
        ])

        return relative_rotation


    def prepare_framepack_inputs(self, full_latents, segment_info):
        """ğŸ”§ å‡†å¤‡FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥ - ä¿®æ­£ç‰ˆï¼Œæ­£ç¡®å¤„ç†ç©ºç´¢å¼•"""
        # ğŸ”§ ä¿®æ­£ï¼šå¤„ç†4ç»´è¾“å…¥ [C, T, H, W]ï¼Œæ·»åŠ batchç»´åº¦
        if len(full_latents.shape) == 4:
            full_latents = full_latents.unsqueeze(0)  # [C, T, H, W] -> [1, C, T, H, W]
            B, C, T, H, W = full_latents.shape
        else:
            B, C, T, H, W = full_latents.shape
        
        # ä¸»è¦latentsï¼ˆç”¨äºå»å™ªé¢„æµ‹ï¼‰
        latent_indices = segment_info['latent_indices']
        main_latents = full_latents[:, :, latent_indices, :, :]  # æ³¨æ„ç»´åº¦é¡ºåº
        
        # ğŸ”§ 1xæ¡ä»¶å¸§ï¼ˆèµ·å§‹å¸§ + æœ€å1å¸§ï¼‰
        clean_latent_indices = segment_info['clean_latent_indices']
        clean_latents = full_latents[:, :, clean_latent_indices, :, :]  # æ³¨æ„ç»´åº¦é¡ºåº
        
        # ğŸ”§ 4xæ¡ä»¶å¸§ - æ€»æ˜¯16å¸§ï¼Œç›´æ¥ç”¨çœŸå®ç´¢å¼• + 0å¡«å……
        clean_latent_4x_indices = segment_info['clean_latent_4x_indices']
        
        # åˆ›å»ºå›ºå®šé•¿åº¦16çš„latentsï¼Œåˆå§‹åŒ–ä¸º0
        clean_latents_4x = torch.zeros(B, C, 16, H, W, dtype=full_latents.dtype)
        clean_latent_4x_indices_final = torch.full((16,), -1, dtype=torch.long)  # -1è¡¨ç¤ºpadding
        
        # ğŸ”§ ä¿®æ­£ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„4xç´¢å¼•
        if len(clean_latent_4x_indices) > 0:
            actual_4x_frames = len(clean_latent_4x_indices)
            # ä»åå¾€å‰å¡«å……ï¼Œç¡®ä¿æœ€æ–°çš„å¸§åœ¨æœ€å
            start_pos = max(0, 16 - actual_4x_frames)
            end_pos = 16
            actual_start = max(0, actual_4x_frames - 16)  # å¦‚æœè¶…è¿‡16å¸§ï¼Œåªå–æœ€å16å¸§
            
            clean_latents_4x[:, :, start_pos:end_pos, :, :] = full_latents[:, :, clean_latent_4x_indices[actual_start:], :, :]
            clean_latent_4x_indices_final[start_pos:end_pos] = clean_latent_4x_indices[actual_start:]
        
        # ğŸ”§ 2xæ¡ä»¶å¸§ - æ€»æ˜¯2å¸§ï¼Œç›´æ¥ç”¨çœŸå®ç´¢å¼• + 0å¡«å……
        clean_latent_2x_indices = segment_info['clean_latent_2x_indices']
        
        # åˆ›å»ºå›ºå®šé•¿åº¦2çš„latentsï¼Œåˆå§‹åŒ–ä¸º0
        clean_latents_2x = torch.zeros(B, C, 2, H, W, dtype=full_latents.dtype)
        clean_latent_2x_indices_final = torch.full((2,), -1, dtype=torch.long)  # -1è¡¨ç¤ºpadding
        
        # ğŸ”§ ä¿®æ­£ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„2xç´¢å¼•
        if len(clean_latent_2x_indices) > 0:
            actual_2x_frames = len(clean_latent_2x_indices)
            # ä»åå¾€å‰å¡«å……ï¼Œç¡®ä¿æœ€æ–°çš„å¸§åœ¨æœ€å
            start_pos = max(0, 2 - actual_2x_frames)
            end_pos = 2
            actual_start = max(0, actual_2x_frames - 2)  # å¦‚æœè¶…è¿‡2å¸§ï¼Œåªå–æœ€å2å¸§
            
            clean_latents_2x[:, :, start_pos:end_pos, :, :] = full_latents[:, :, clean_latent_2x_indices[actual_start:], :, :]
            clean_latent_2x_indices_final[start_pos:end_pos] = clean_latent_2x_indices[actual_start:]
        
        # ğŸ”§ ç§»é™¤æ·»åŠ çš„batchç»´åº¦ï¼Œè¿”å›åŸå§‹æ ¼å¼
        if B == 1:
            main_latents = main_latents.squeeze(0)  # [1, C, T, H, W] -> [C, T, H, W]
            clean_latents = clean_latents.squeeze(0)
            clean_latents_2x = clean_latents_2x.squeeze(0)
            clean_latents_4x = clean_latents_4x.squeeze(0)
        
        return {
            'latents': main_latents,
            'clean_latents': clean_latents,
            'clean_latents_2x': clean_latents_2x,
            'clean_latents_4x': clean_latents_4x,
            'latent_indices': segment_info['latent_indices'],
            'clean_latent_indices': segment_info['clean_latent_indices'],
            'clean_latent_2x_indices': clean_latent_2x_indices_final,  # ğŸ”§ ä½¿ç”¨çœŸå®ç´¢å¼•ï¼ˆå«-1å¡«å……ï¼‰
            'clean_latent_4x_indices': clean_latent_4x_indices_final,  # ğŸ”§ ä½¿ç”¨çœŸå®ç´¢å¼•ï¼ˆå«-1å¡«å……ï¼‰
        }

    def create_sekai_pose_embeddings(self, cam_data, segment_info):
        """åˆ›å»ºSekaié£æ ¼çš„pose embeddings"""
        cam_data_seq = cam_data['extrinsic']
        
        # ä¸ºæ‰€æœ‰å¸§è®¡ç®—ç›¸å¯¹pose
        all_keyframe_indices = []
        for compressed_idx in range(segment_info['start_frame'], segment_info['target_range'][1]):
            all_keyframe_indices.append(compressed_idx * 4)
        
        relative_cams = []
        for idx in all_keyframe_indices:
            cam_prev = cam_data_seq[idx]
            cam_next = cam_data_seq[idx + 4]
            relative_cam = compute_relative_pose(cam_prev, cam_next)
            relative_cams.append(torch.as_tensor(relative_cam[:3, :]))
        
        pose_embedding = torch.stack(relative_cams, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        pose_embedding = pose_embedding.to(torch.bfloat16)

        return pose_embedding

    def create_openx_pose_embeddings(self, cam_data, segment_info):
        """ğŸ”§ åˆ›å»ºOpenXé£æ ¼çš„pose embeddings - ç±»ä¼¼sekaiä½†å¤„ç†æ›´çŸ­çš„åºåˆ—"""
        cam_data_seq = cam_data['extrinsic']
        
        # ä¸ºæ‰€æœ‰å¸§è®¡ç®—ç›¸å¯¹pose - OpenXä½¿ç”¨4å€é—´éš”
        all_keyframe_indices = []
        for compressed_idx in range(segment_info['start_frame'], segment_info['target_range'][1]):
            keyframe_idx = compressed_idx * 4
            if keyframe_idx + 4 < len(cam_data_seq):
                all_keyframe_indices.append(keyframe_idx)
        
        relative_cams = []
        for idx in all_keyframe_indices:
            if idx + 4 < len(cam_data_seq):
                cam_prev = cam_data_seq[idx]
                cam_next = cam_data_seq[idx + 4]
                relative_cam = compute_relative_pose(cam_prev, cam_next)
                relative_cams.append(torch.as_tensor(relative_cam[:3, :]))
            else:
                # å¦‚æœæ²¡æœ‰ä¸‹ä¸€å¸§ï¼Œä½¿ç”¨å•ä½çŸ©é˜µ
                identity_cam = torch.eye(3, 4)
                relative_cams.append(identity_cam)
        
        if len(relative_cams) == 0:
            return None
            
        pose_embedding = torch.stack(relative_cams, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        pose_embedding = pose_embedding.to(torch.bfloat16)

        return pose_embedding
    
    def create_spatialvid_pose_embeddings(self, cam_data, segment_info):
        """ğŸ”§ åˆ›å»ºSpatialvidé£æ ¼çš„pose embeddings - cameraé—´éš”ä¸º1å¸§è€Œé4å¸§"""
        cam_data_seq = cam_data['extrinsic']
        
        # ä¸ºæ‰€æœ‰å¸§è®¡ç®—ç›¸å¯¹pose - spatialvidç‰¹æœ‰ï¼šæ¯éš”1å¸§è€Œä¸æ˜¯4å¸§
        all_keyframe_indices = []
        for compressed_idx in range(segment_info['start_frame'], segment_info['target_range'][1]):
            # ğŸ”§ spatialvidå…³é”®å·®å¼‚ï¼šcameraæ¯éš”4å¸§æœ‰ä¸€ä¸ªï¼Œä½†ç´¢å¼•é€’å¢1
            all_keyframe_indices.append(compressed_idx)
        
        relative_cams = []
        for idx in all_keyframe_indices:
            # ğŸ”§ spatialvidå…³é”®å·®å¼‚ï¼šcurrentå’Œnextæ˜¯+1è€Œä¸æ˜¯+4
            cam_prev = cam_data_seq[idx]
            cam_next = cam_data_seq[idx + 1]  # è¿™é‡Œæ˜¯+1ï¼Œä¸æ˜¯+4
            relative_cam = compute_relative_pose_matrix(cam_prev, cam_next)
            relative_cams.append(torch.as_tensor(relative_cam[:3, :]))
        
        pose_embedding = torch.stack(relative_cams, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        pose_embedding = pose_embedding.to(torch.bfloat16)

        return pose_embedding
               
    def create_nuscenes_pose_embeddings_framepack(self, scene_info, segment_info):
        """åˆ›å»ºNuScenesé£æ ¼çš„pose embeddings - FramePackç‰ˆæœ¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥7ç»´ï¼‰"""
        keyframe_poses = scene_info['keyframe_poses']
        reference_keyframe_idx = segment_info['reference_keyframe_idx']
        target_keyframe_indices = segment_info['target_keyframe_indices']
        
        if reference_keyframe_idx >= len(keyframe_poses):
            return None
        
        reference_pose = keyframe_poses[reference_keyframe_idx]
        
        # ä¸ºæ‰€æœ‰å¸§ï¼ˆcondition + targetï¼‰åˆ›å»ºpose embeddings
        start_frame = segment_info['start_frame']
        condition_end_compressed = start_frame + segment_info['condition_frames']
        target_end_compressed = condition_end_compressed + segment_info['target_frames']
        
        # å‹ç¼©åçš„å…³é”®å¸§ç´¢å¼•
        compressed_keyframe_indices = [idx // self.time_compression_ratio for idx in scene_info['keyframe_indices']]
        
        # æ‰¾åˆ°conditionæ®µçš„å…³é”®å¸§
        condition_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                        if start_frame <= idx < condition_end_compressed]
        
        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        condition_keyframes_original_indices = []
        for compressed_idx in condition_keyframes_compressed:
            for i, comp_idx in enumerate(compressed_keyframe_indices):
                if comp_idx == compressed_idx:
                    condition_keyframes_original_indices.append(i)
                    break
        
        pose_vecs = []
        
        # ä¸ºconditionå¸§è®¡ç®—pose
        for i in range(segment_info['condition_frames']):
            if not condition_keyframes_original_indices:
                translation = torch.zeros(3, dtype=torch.float32)
                rotation = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)
            else:
                # ä¸ºconditionå¸§åˆ†é…pose
                if len(condition_keyframes_original_indices) == 1:
                    keyframe_idx = condition_keyframes_original_indices[0]
                else:
                    if segment_info['condition_frames'] == 1:
                        keyframe_idx = condition_keyframes_original_indices[0]
                    else:
                        interp_ratio = i / (segment_info['condition_frames'] - 1)
                        interp_idx = int(interp_ratio * (len(condition_keyframes_original_indices) - 1))
                        keyframe_idx = condition_keyframes_original_indices[interp_idx]
                
                if keyframe_idx >= len(keyframe_poses):
                    translation = torch.zeros(3, dtype=torch.float32)
                    rotation = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)
                else:
                    condition_pose = keyframe_poses[keyframe_idx]
                    
                    translation = torch.tensor(
                        np.array(condition_pose['translation']) - np.array(reference_pose['translation']),
                        dtype=torch.float32
                    )
                    
                    relative_rotation = self.calculate_relative_rotation(
                        condition_pose['rotation'],
                        reference_pose['rotation']
                    )
                    
                    rotation = relative_rotation
            
            # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥7ç»´ [translation(3) + rotation(4)]
            pose_vec = torch.cat([translation, rotation], dim=0)  # [7D]
            pose_vecs.append(pose_vec)
        
        # ä¸ºtargetå¸§è®¡ç®—pose
        if not target_keyframe_indices:
            for i in range(segment_info['target_frames']):
                pose_vec = torch.cat([
                    torch.zeros(3, dtype=torch.float32),
                    torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32),
                ], dim=0)  # [7D]
                pose_vecs.append(pose_vec)
        else:
            for i in range(segment_info['target_frames']):
                if len(target_keyframe_indices) == 1:
                    target_keyframe_idx = target_keyframe_indices[0]
                else:
                    if segment_info['target_frames'] == 1:
                        target_keyframe_idx = target_keyframe_indices[0]
                    else:
                        interp_ratio = i / (segment_info['target_frames'] - 1)
                        interp_idx = int(interp_ratio * (len(target_keyframe_indices) - 1))
                        target_keyframe_idx = target_keyframe_indices[interp_idx]
                
                if target_keyframe_idx >= len(keyframe_poses):
                    pose_vec = torch.cat([
                        torch.zeros(3, dtype=torch.float32),
                        torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32),
                    ], dim=0)  # [7D]
                else:
                    target_pose = keyframe_poses[target_keyframe_idx]
                    
                    relative_translation = torch.tensor(
                        np.array(target_pose['translation']) - np.array(reference_pose['translation']),
                        dtype=torch.float32
                    )
                    
                    relative_rotation = self.calculate_relative_rotation(
                        target_pose['rotation'],
                        reference_pose['rotation']
                    )
                    
                    # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥7ç»´ [translation(3) + rotation(4)]
                    pose_vec = torch.cat([relative_translation, relative_rotation], dim=0)  # [7D]
                
                pose_vecs.append(pose_vec)
        
        if not pose_vecs:
            return None
        
        pose_sequence = torch.stack(pose_vecs, dim=0)  # [total_frames, 7]
        
        return pose_sequence

    # ä¿®æ”¹select_dynamic_segmentæ–¹æ³•
    def select_dynamic_segment(self, full_latents, dataset_type, scene_info=None):
        """ğŸ”§ æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ä¸åŒçš„æ®µè½é€‰æ‹©ç­–ç•¥"""
        if dataset_type == 'nuscenes' and scene_info is not None:
            return self.select_dynamic_segment_nuscenes(scene_info)
        else:
            # åŸæœ‰çš„sekaiæ–¹å¼
            total_lens = full_latents.shape[1]
            
            min_condition_compressed = self.min_condition_frames // self.time_compression_ratio
            max_condition_compressed = self.max_condition_frames // self.time_compression_ratio
            target_frames_compressed = self.target_frames // self.time_compression_ratio
            max_condition_compressed = min(total_lens-target_frames_compressed-1, max_condition_compressed)

            ratio = random.random()
            if ratio < 0.15:
                condition_frames_compressed = 1
            elif 0.15 <= ratio < 0.9 or total_lens <= 2*target_frames_compressed + 1:
                condition_frames_compressed = random.randint(min_condition_compressed, max_condition_compressed)
            else:
                condition_frames_compressed = target_frames_compressed
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§æ•°
            min_required_frames = condition_frames_compressed + target_frames_compressed
            if total_lens < min_required_frames:
                return None
            
            start_frame_compressed = random.randint(0, total_lens - min_required_frames - 1)
            condition_end_compressed = start_frame_compressed + condition_frames_compressed
            target_end_compressed = condition_end_compressed + target_frames_compressed

            # FramePacké£æ ¼çš„ç´¢å¼•å¤„ç†
            latent_indices = torch.arange(condition_end_compressed, target_end_compressed)
            
            # 1xå¸§ï¼šèµ·å§‹å¸§ + æœ€å1å¸§
            clean_latent_indices_start = torch.tensor([start_frame_compressed])
            clean_latent_1x_indices = torch.tensor([condition_end_compressed - 1])
            clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices])
            
            # ğŸ”§ 2xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®š
            if condition_frames_compressed >= 2:
                # å–æœ€å2å¸§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                clean_latent_2x_start = max(start_frame_compressed, condition_end_compressed - 2-1)
                clean_latent_2x_indices = torch.arange(clean_latent_2x_start, condition_end_compressed-1)
            else:
                # å¦‚æœconditionå¸§æ•°ä¸è¶³2å¸§ï¼Œåˆ›å»ºç©ºç´¢å¼•
                clean_latent_2x_indices = torch.tensor([], dtype=torch.long)
            
            # ğŸ”§ 4xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®šï¼Œæœ€å¤š16å¸§
            if condition_frames_compressed > 3:
                # å–æœ€å¤š16å¸§çš„å†å²ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                clean_4x_start = max(start_frame_compressed, condition_end_compressed - 16-3)
                clean_latent_4x_indices = torch.arange(clean_4x_start, condition_end_compressed-3)
            else:
                clean_latent_4x_indices = torch.tensor([], dtype=torch.long)
            
            # å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
            keyframe_original_idx = []
            for compressed_idx in range(start_frame_compressed, target_end_compressed):
                if dataset_type == 'spatialvid':
                    keyframe_original_idx.append(compressed_idx)  # spatialvidç›´æ¥ä½¿ç”¨compressed_idx
                elif dataset_type == 'openx' or 'sekai':  # ğŸ”§ æ–°å¢openxå¤„ç†
                    keyframe_original_idx.append(compressed_idx * 4)  # openxä½¿ç”¨4å€é—´éš”

            return {
                'start_frame': start_frame_compressed,
                'condition_frames': condition_frames_compressed,
                'target_frames': target_frames_compressed,
                'condition_range': (start_frame_compressed, condition_end_compressed),
                'target_range': (condition_end_compressed, target_end_compressed),
                
                # FramePacké£æ ¼çš„ç´¢å¼•
                'latent_indices': latent_indices,
                'clean_latent_indices': clean_latent_indices,
                'clean_latent_2x_indices': clean_latent_2x_indices,
                'clean_latent_4x_indices': clean_latent_4x_indices,
                
                'keyframe_original_idx': keyframe_original_idx,
                'original_condition_frames': condition_frames_compressed * self.time_compression_ratio,
                'original_target_frames': target_frames_compressed * self.time_compression_ratio,
            }

    # ä¿®æ”¹create_pose_embeddingsæ–¹æ³•
    def create_pose_embeddings(self, cam_data, segment_info, dataset_type, scene_info=None):
        """ğŸ”§ æ ¹æ®æ•°æ®é›†ç±»å‹åˆ›å»ºpose embeddings"""
        if dataset_type == 'nuscenes' and scene_info is not None:
            return self.create_nuscenes_pose_embeddings_framepack(scene_info, segment_info)
        elif dataset_type == 'spatialvid':  # ğŸ”§ æ–°å¢spatialvidå¤„ç†
            return self.create_spatialvid_pose_embeddings(cam_data, segment_info)
        elif dataset_type == 'sekai':
            return self.create_sekai_pose_embeddings(cam_data, segment_info)
        elif dataset_type == 'openx':  # ğŸ”§ æ–°å¢openxå¤„ç†
            return self.create_openx_pose_embeddings(cam_data, segment_info)
        
    def __getitem__(self, index):
        while True:
            try:
                # æ ¹æ®æƒé‡éšæœºé€‰æ‹©åœºæ™¯
                scene_idx = np.random.choice(len(self.scene_dirs), p=self.sampling_probs)
                scene_dir = self.scene_dirs[scene_idx]
                dataset_info = self.dataset_info[scene_dir]
                
                dataset_name = dataset_info['name']
                dataset_type = dataset_info['type']
                
                # ğŸ”§ æ ¹æ®æ•°æ®é›†ç±»å‹åŠ è½½æ•°æ®
                scene_info = None
                if dataset_type == 'nuscenes':
                    # NuSceneséœ€è¦åŠ è½½scene_info.json
                    scene_info_path = os.path.join(scene_dir, "scene_info.json")
                    if os.path.exists(scene_info_path):
                        with open(scene_info_path, 'r') as f:
                            scene_info = json.load(f)
                    
                    # NuScenesä½¿ç”¨ä¸åŒçš„ç¼–ç æ–‡ä»¶å
                    encoded_path = os.path.join(scene_dir, "encoded_video-480p.pth")
                    if not os.path.exists(encoded_path):
                        encoded_path = os.path.join(scene_dir, "encoded_video.pth")  # fallback
                    
                    encoded_data = torch.load(encoded_path, weights_only=True, map_location="cpu")
                else:
                    # Sekaiæ•°æ®é›†
                    encoded_path = os.path.join(scene_dir, "encoded_video.pth")
                    encoded_data = torch.load(encoded_path, weights_only=False, map_location="cpu")
                
                full_latents = encoded_data['latents']
                if full_latents.shape[1] <= 10:
                    continue
                cam_data = encoded_data.get('cam_emb', encoded_data)
                
                # ğŸ”§ éªŒè¯NuScenesçš„latentå¸§æ•°
                if dataset_type == 'nuscenes' and scene_info is not None:
                    expected_latent_frames = scene_info['total_frames'] // self.time_compression_ratio
                    actual_latent_frames = full_latents.shape[1]
                    
                    if abs(actual_latent_frames - expected_latent_frames) > 2:
                        print(f"âš ï¸  NuScenes Latentå¸§æ•°ä¸åŒ¹é…ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                        continue
                
                # ä½¿ç”¨æ•°æ®é›†ç‰¹å®šçš„æ®µè½é€‰æ‹©ç­–ç•¥
                segment_info = self.select_dynamic_segment(full_latents, dataset_type, scene_info)
                if segment_info is None:
                    continue
                
                # åˆ›å»ºæ•°æ®é›†ç‰¹å®šçš„pose embeddings
                all_camera_embeddings = self.create_pose_embeddings(cam_data, segment_info, dataset_type, scene_info)
                if all_camera_embeddings is None:
                    continue
                
                # å‡†å¤‡FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥
                framepack_inputs = self.prepare_framepack_inputs(full_latents, segment_info)
                
                n = segment_info["condition_frames"]
                m = segment_info['target_frames']
                
                # å¤„ç†camera embedding with mask
                mask = torch.zeros(n+m, dtype=torch.float32)
                mask[:n] = 1.0
                mask = mask.view(-1, 1)
                
                # ğŸ”§ NuScenesè¿”å›çš„æ˜¯ç›´æ¥çš„embeddingï¼ŒSekaiè¿”å›çš„æ˜¯tensor
                if isinstance(all_camera_embeddings, torch.Tensor):
                    camera_with_mask = torch.cat([all_camera_embeddings, mask], dim=1)
                else:
                    # NuScenesé£æ ¼ï¼Œç›´æ¥å°±æ˜¯æœ€ç»ˆçš„embedding
                    camera_with_mask = torch.cat([all_camera_embeddings, mask], dim=1)
                
                result = {
                    # FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥
                    "latents": framepack_inputs['latents'],
                    "clean_latents": framepack_inputs['clean_latents'],
                    "clean_latents_2x": framepack_inputs['clean_latents_2x'],
                    "clean_latents_4x": framepack_inputs['clean_latents_4x'],
                    "latent_indices": framepack_inputs['latent_indices'],
                    "clean_latent_indices": framepack_inputs['clean_latent_indices'],
                    "clean_latent_2x_indices": framepack_inputs['clean_latent_2x_indices'],
                    "clean_latent_4x_indices": framepack_inputs['clean_latent_4x_indices'],
                    
                    # Cameraæ•°æ®
                    "camera": camera_with_mask,
                    
                    # å…¶ä»–æ•°æ®
                    "prompt_emb": encoded_data["prompt_emb"],
                    "image_emb": encoded_data.get("image_emb", {}),
                    
                    # å…ƒä¿¡æ¯
                    "condition_frames": n,
                    "target_frames": m,
                    "scene_name": os.path.basename(scene_dir),
                    "dataset_name": dataset_name,
                    "dataset_type": dataset_type,
                    "original_condition_frames": segment_info['original_condition_frames'],
                    "original_target_frames": segment_info['original_target_frames'],
                }
                
                return result
                
            except Exception as e:
                print(f"Error loading sample: {e}")
                traceback.print_exc()
                continue

    def __len__(self):
        return self.steps_per_epoch

def replace_dit_model_in_manager():
    """åœ¨æ¨¡å‹åŠ è½½å‰æ›¿æ¢DiTæ¨¡å‹ç±»ä¸ºMoEç‰ˆæœ¬"""
    from diffsynth.models.wan_video_dit_moe import WanModelMoe
    from diffsynth.configs.model_config import model_loader_configs
    
    # ä¿®æ”¹model_loader_configsä¸­çš„é…ç½®
    for i, config in enumerate(model_loader_configs):
        keys_hash, keys_hash_with_shape, model_names, model_classes, model_resource = config
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«wan_video_ditæ¨¡å‹
        if 'wan_video_dit' in model_names:
            new_model_names = []
            new_model_classes = []
            
            for name, cls in zip(model_names, model_classes):
                if name == 'wan_video_dit':
                    new_model_names.append(name)
                    new_model_classes.append(WanModelMoe)  # ğŸ”§ ä½¿ç”¨MoEç‰ˆæœ¬
                    print(f"âœ… æ›¿æ¢äº†æ¨¡å‹ç±»: {name} -> WanModelMoe")
                else:
                    new_model_names.append(name)
                    new_model_classes.append(cls)
            
            # æ›´æ–°é…ç½®
            model_loader_configs[i] = (keys_hash, keys_hash_with_shape, new_model_names, new_model_classes, model_resource)

class MultiDatasetLightningModelForTrain(pl.LightningModule):
    def __init__(
        self,
        dit_path,
        learning_rate=1e-5,
        use_gradient_checkpointing=True,
        use_gradient_checkpointing_offload=False,
        resume_ckpt_path=None,
        # ğŸ”§ MoEå‚æ•°
        use_moe=False,
        moe_config=None
    ):
        super().__init__()
        self.use_moe = use_moe
        self.moe_config = moe_config or {}
        
        replace_dit_model_in_manager()
        model_manager = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        if os.path.isfile(dit_path):
            model_manager.load_models([dit_path])
        else:
            dit_path = dit_path.split(",")
            model_manager.load_models([dit_path])
        model_manager.load_models(["models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth"])
        
        self.pipe = WanVideoReCamMasterPipeline.from_model_manager(model_manager)
        self.pipe.scheduler.set_timesteps(1000, training=True)

        # æ·»åŠ FramePackçš„clean_x_embedder
        self.add_framepack_components()
        if self.use_moe:
            self.add_moe_components()

        # ğŸ”§ æ·»åŠ cameraç¼–ç å™¨ï¼ˆwan_video_dit_moe.pyå·²ç»åŒ…å«MoEé€»è¾‘ï¼‰
        dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
        for block in self.pipe.dit.blocks:
            # ğŸ”§ ç®€åŒ–ï¼šåªæ·»åŠ ä¼ ç»Ÿcameraç¼–ç å™¨ï¼ŒMoEé€»è¾‘åœ¨wan_video_dit_moe.pyä¸­
            block.cam_encoder = nn.Linear(13, dim)
            block.projector = nn.Linear(dim, dim)
            block.cam_encoder.weight.data.zero_()
            block.cam_encoder.bias.data.zero_()
            block.projector.weight = nn.Parameter(torch.eye(dim))
            block.projector.bias = nn.Parameter(torch.zeros(dim))
        
        if resume_ckpt_path is not None:
            state_dict = torch.load(resume_ckpt_path, map_location="cpu")
            self.pipe.dit.load_state_dict(state_dict, strict=False)
            print('load checkpoint:', resume_ckpt_path)

        self.freeze_parameters()
        
        # ğŸ”§ è®­ç»ƒå‚æ•°è®¾ç½®
        for name, module in self.pipe.denoising_model().named_modules():
            if any(keyword in name for keyword in [
                                                "moe", "sekai_processor"]):
                for param in module.parameters():
                    param.requires_grad = True
        
        self.learning_rate = learning_rate
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.use_gradient_checkpointing_offload = use_gradient_checkpointing_offload
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        self.vis_dir = "multi_dataset_dynamic/visualizations"
        os.makedirs(self.vis_dir, exist_ok=True)

    def add_moe_components(self):
        """ğŸ”§ æ·»åŠ MoEç›¸å…³ç»„ä»¶ - ç±»ä¼¼add_framepack_componentsçš„æ–¹å¼"""
        if not hasattr(self.pipe.dit, 'moe_config'):
            self.pipe.dit.moe_config = self.moe_config
            print("âœ… æ·»åŠ äº†MoEé…ç½®åˆ°æ¨¡å‹")
        
        # ä¸ºæ¯ä¸ªblockåŠ¨æ€æ·»åŠ MoEç»„ä»¶
        dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
        unified_dim = self.moe_config.get("unified_dim", 30)
        
        for i, block in enumerate(self.pipe.dit.blocks):
            from diffsynth.models.wan_video_dit_moe import ModalityProcessor, MultiModalMoE
            
            # Sekaiæ¨¡æ€å¤„ç†å™¨ - è¾“å‡ºunified_dim
            block.sekai_processor = ModalityProcessor("sekai", 13, unified_dim)
            
            # NuScenesæ¨¡æ€å¤„ç†å™¨ - è¾“å‡ºunified_dim
            # block.nuscenes_processor = ModalityProcessor("nuscenes", 8, unified_dim)

            # block.openx_processor = ModalityProcessor("openx", 13, unified_dim)  # OpenXä½¿ç”¨13ç»´è¾“å…¥ï¼Œç±»ä¼¼sekaiä½†ç‹¬ç«‹å¤„ç†

            
            # MoEç½‘ç»œ - è¾“å…¥unified_dimï¼Œè¾“å‡ºdim
            block.moe = MultiModalMoE(
                unified_dim=unified_dim,
                output_dim=dim,  # è¾“å‡ºç»´åº¦åŒ¹é…transformer blockçš„dim
                num_experts=self.moe_config.get("num_experts", 4),
                top_k=self.moe_config.get("top_k", 2)
            )
            
            print(f"âœ… Block {i} æ·»åŠ äº†MoEç»„ä»¶ (unified_dim: {unified_dim}, experts: {self.moe_config.get('num_experts', 4)})")


    def add_framepack_components(self):
        """ğŸ”§ æ·»åŠ FramePackç›¸å…³ç»„ä»¶"""
        if not hasattr(self.pipe.dit, 'clean_x_embedder'):
            inner_dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
            
            class CleanXEmbedder(nn.Module):
                def __init__(self, inner_dim):
                    super().__init__()
                    self.proj = nn.Conv3d(16, inner_dim, kernel_size=(1, 2, 2), stride=(1, 2, 2))
                    self.proj_2x = nn.Conv3d(16, inner_dim, kernel_size=(2, 4, 4), stride=(2, 4, 4))
                    self.proj_4x = nn.Conv3d(16, inner_dim, kernel_size=(4, 8, 8), stride=(4, 8, 8))
                
                def forward(self, x, scale="1x"):
                    if scale == "1x":
                        return self.proj(x)
                    elif scale == "2x":
                        return self.proj_2x(x)
                    elif scale == "4x":
                        return self.proj_4x(x)
                    else:
                        raise ValueError(f"Unsupported scale: {scale}")
            
            self.pipe.dit.clean_x_embedder = CleanXEmbedder(inner_dim)
            print("âœ… æ·»åŠ äº†FramePackçš„clean_x_embedderç»„ä»¶")
        
    def freeze_parameters(self):
        self.pipe.requires_grad_(False)
        self.pipe.eval()
        self.pipe.denoising_model().train()

    def training_step(self, batch, batch_idx):
        """ğŸ”§ å¤šæ•°æ®é›†è®­ç»ƒæ­¥éª¤"""
        condition_frames = batch["condition_frames"][0].item()
        target_frames = batch["target_frames"][0].item()
        
        original_condition_frames = batch.get("original_condition_frames", [condition_frames * 4])[0]
        original_target_frames = batch.get("original_target_frames", [target_frames * 4])[0]

        dataset_name = batch.get("dataset_name", ["unknown"])[0]
        dataset_type = batch.get("dataset_type", ["sekai"])[0]
        scene_name = batch.get("scene_name", ["unknown"])[0]
        
        # å‡†å¤‡è¾“å…¥æ•°æ®
        latents = batch["latents"].to(self.device)
        if len(latents.shape) == 4:
            latents = latents.unsqueeze(0)
        
        clean_latents = batch["clean_latents"].to(self.device) if batch["clean_latents"].numel() > 0 else None
        if clean_latents is not None and len(clean_latents.shape) == 4:
            clean_latents = clean_latents.unsqueeze(0)
        
        clean_latents_2x = batch["clean_latents_2x"].to(self.device) if batch["clean_latents_2x"].numel() > 0 else None
        if clean_latents_2x is not None and len(clean_latents_2x.shape) == 4:
            clean_latents_2x = clean_latents_2x.unsqueeze(0)
        
        clean_latents_4x = batch["clean_latents_4x"].to(self.device) if batch["clean_latents_4x"].numel() > 0 else None
        if clean_latents_4x is not None and len(clean_latents_4x.shape) == 4:
            clean_latents_4x = clean_latents_4x.unsqueeze(0)
        
        # ç´¢å¼•å¤„ç†
        latent_indices = batch["latent_indices"].to(self.device)
        clean_latent_indices = batch["clean_latent_indices"].to(self.device) if batch["clean_latent_indices"].numel() > 0 else None
        clean_latent_2x_indices = batch["clean_latent_2x_indices"].to(self.device) if batch["clean_latent_2x_indices"].numel() > 0 else None
        clean_latent_4x_indices = batch["clean_latent_4x_indices"].to(self.device) if batch["clean_latent_4x_indices"].numel() > 0 else None
        
        # Camera embeddingå¤„ç†
        cam_emb = batch["camera"].to(self.device)
        
        # ğŸ”§ æ ¹æ®æ•°æ®é›†ç±»å‹è®¾ç½®modality_inputs
        if dataset_type == "sekai":
            modality_inputs = {"sekai": cam_emb}
        elif dataset_type == "spatialvid":  # ğŸ”§ spatialvidä½¿ç”¨sekai processor
            modality_inputs = {"sekai": cam_emb}  # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨"sekai"é”®
        elif dataset_type == "nuscenes":
            modality_inputs = {"nuscenes": cam_emb}
        elif dataset_type == "openx":  # ğŸ”§ æ–°å¢ï¼šopenxä½¿ç”¨ç‹¬ç«‹çš„processor
            modality_inputs = {"openx": cam_emb}
        else:
            modality_inputs = {"sekai": cam_emb}  # é»˜è®¤
        
        camera_dropout_prob = 0.05
        if random.random() < camera_dropout_prob:
            cam_emb = torch.zeros_like(cam_emb)
            # åŒæ—¶æ¸…ç©ºmodality_inputs
            for key in modality_inputs:
                modality_inputs[key] = torch.zeros_like(modality_inputs[key])
            print(f"åº”ç”¨camera dropout for CFG training (dataset: {dataset_name}, type: {dataset_type})")
        
        prompt_emb = batch["prompt_emb"]
        prompt_emb["context"] = prompt_emb["context"][0].to(self.device)
        image_emb = batch["image_emb"]

        if "clip_feature" in image_emb:
            image_emb["clip_feature"] = image_emb["clip_feature"][0].to(self.device)
        if "y" in image_emb:
            image_emb["y"] = image_emb["y"][0].to(self.device)

        # Lossè®¡ç®—
        self.pipe.device = self.device
        noise = torch.randn_like(latents)
        timestep_id = torch.randint(0, self.pipe.scheduler.num_train_timesteps, (1,))
        timestep = self.pipe.scheduler.timesteps[timestep_id].to(dtype=self.pipe.torch_dtype, device=self.pipe.device)
        
        # FramePacké£æ ¼çš„å™ªå£°å¤„ç†
        noisy_condition_latents = None
        if clean_latents is not None:
            noisy_condition_latents = copy.deepcopy(clean_latents)
            is_add_noise = random.random()
            if is_add_noise > 0.2:
                noise_cond = torch.randn_like(clean_latents)
                timestep_id_cond = torch.randint(0, self.pipe.scheduler.num_train_timesteps//4*3, (1,))
                timestep_cond = self.pipe.scheduler.timesteps[timestep_id_cond].to(dtype=self.pipe.torch_dtype, device=self.pipe.device)
                noisy_condition_latents = self.pipe.scheduler.add_noise(clean_latents, noise_cond, timestep_cond)

        extra_input = self.pipe.prepare_extra_input(latents)
        origin_latents = copy.deepcopy(latents)
        noisy_latents = self.pipe.scheduler.add_noise(latents, noise, timestep)
        
        training_target = self.pipe.scheduler.training_target(latents, noise, timestep)
        
        # ğŸ”§ Forwardè°ƒç”¨ - ä¼ é€’modality_inputs
        noise_pred, moe_loss = self.pipe.denoising_model()(
            noisy_latents, 
            timestep=timestep, 
            cam_emb=cam_emb,
            modality_inputs=modality_inputs,  # ğŸ”§ ä¼ é€’å¤šæ¨¡æ€è¾“å…¥
            latent_indices=latent_indices,
            clean_latents=noisy_condition_latents if noisy_condition_latents is not None else clean_latents,
            clean_latent_indices=clean_latent_indices,
            clean_latents_2x=clean_latents_2x,
            clean_latent_2x_indices=clean_latent_2x_indices,
            clean_latents_4x=clean_latents_4x,
            clean_latent_4x_indices=clean_latent_4x_indices,
            **prompt_emb, 
            **extra_input, 
            **image_emb,
            use_gradient_checkpointing=self.use_gradient_checkpointing,
            use_gradient_checkpointing_offload=self.use_gradient_checkpointing_offload
        )
        
        # è®¡ç®—loss
        loss = torch.nn.functional.mse_loss(noise_pred.float(), training_target.float())
        loss = loss * self.pipe.scheduler.training_weight(timestep)
        
        print(f'--------loss ({dataset_name}-{dataset_type})------------:', loss)

        return loss

    def configure_optimizers(self):
        trainable_modules = filter(lambda p: p.requires_grad, self.pipe.denoising_model().parameters())
        optimizer = torch.optim.AdamW(trainable_modules, lr=self.learning_rate)
        return optimizer
    
    def on_save_checkpoint(self, checkpoint):
        checkpoint_dir = "/share_zhuyixuan05/zhuyixuan05/ICLR2026/framepack_moe_test"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        current_step = self.global_step
        checkpoint.clear()
        
        state_dict = self.pipe.denoising_model().state_dict()
        torch.save(state_dict, os.path.join(checkpoint_dir, f"step{current_step}_moe.ckpt"))
        print(f"Saved MoE model checkpoint: step{current_step}_moe.ckpt")

def train_multi_dataset(args):
    """è®­ç»ƒæ”¯æŒå¤šæ•°æ®é›†MoEçš„æ¨¡å‹"""
    
    # ğŸ”§ æ•°æ®é›†é…ç½®
    dataset_configs = [
        {
            'name': 'sekai-drone',
            'paths': ['/share_zhuyixuan05/zhuyixuan05/sekai-game-drone'],
            'type': 'sekai',
            'weight': 1.0
        },
        {
            'name': 'sekai-walking',
            'paths': ['/share_zhuyixuan05/zhuyixuan05/sekai-game-walking'],
            'type': 'sekai',
            'weight': 1.0
        },
        # {
        #     'name': 'spatialvid',
        #     'paths': ['/share_zhuyixuan05/zhuyixuan05/spatialvid'],
        #     'type': 'spatialvid',
        #     'weight': 1.0
        # },
        # {
        #     'name': 'nuscenes',
        #     'paths': ['/share_zhuyixuan05/zhuyixuan05/nuscenes_video_generation_dynamic'],
        #     'type': 'nuscenes',
        #     'weight': 4.0
        # },
        # {
        #     'name': 'openx-fractal',
        #     'paths': ['/share_zhuyixuan05/zhuyixuan05/openx-fractal-encoded'],
        #     'type': 'openx',
        #     'weight': 1.0
        # }
    ]
    
    dataset = MultiDatasetDynamicDataset(
        dataset_configs,
        steps_per_epoch=args.steps_per_epoch,
        min_condition_frames=args.min_condition_frames,
        max_condition_frames=args.max_condition_frames,
        target_frames=args.target_frames,
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle=True,
        batch_size=1,
        num_workers=args.dataloader_num_workers
    )
    
    # ğŸ”§ MoEé…ç½®
    moe_config = {
        "unified_dim": args.unified_dim,  # æ–°å¢
        "num_experts": args.moe_num_experts,
        "top_k": args.moe_top_k,
        "moe_loss_weight": args.moe_loss_weight,
        "sekai_input_dim": 13,
        "nuscenes_input_dim": 8,
        "openx_input_dim": 13  
    }
    
    model = MultiDatasetLightningModelForTrain(
        dit_path=args.dit_path,
        learning_rate=args.learning_rate,
        use_gradient_checkpointing=args.use_gradient_checkpointing,
        use_gradient_checkpointing_offload=args.use_gradient_checkpointing_offload,
        resume_ckpt_path=args.resume_ckpt_path,
        use_moe=True,  # æ€»æ˜¯ä½¿ç”¨MoE
        moe_config=moe_config
    )

    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        accelerator="gpu",
        devices="auto",
        precision="bf16",
        strategy=args.training_strategy,
        default_root_dir=args.output_path,
        accumulate_grad_batches=args.accumulate_grad_batches,
        callbacks=[],
        logger=False
    )
    trainer.fit(model, dataloader)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Train Multi-Dataset FramePack with MoE")
    parser.add_argument("--dit_path", type=str, default="models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors")
    parser.add_argument("--output_path", type=str, default="./")
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    parser.add_argument("--steps_per_epoch", type=int, default=2000)
    parser.add_argument("--max_epochs", type=int, default=100000)
    parser.add_argument("--min_condition_frames", type=int, default=8, help="æœ€å°æ¡ä»¶å¸§æ•°")
    parser.add_argument("--max_condition_frames", type=int, default=120, help="æœ€å¤§æ¡ä»¶å¸§æ•°")
    parser.add_argument("--target_frames", type=int, default=32, help="ç›®æ ‡å¸§æ•°")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    parser.add_argument("--accumulate_grad_batches", type=int, default=1)
    parser.add_argument("--training_strategy", type=str, default="deepspeed_stage_1")
    parser.add_argument("--use_gradient_checkpointing", default=False)
    parser.add_argument("--use_gradient_checkpointing_offload", action="store_true")
    parser.add_argument("--resume_ckpt_path", type=str, default="/share_zhuyixuan05/zhuyixuan05/ICLR2026/framepack_moe_test/step1500_moe.ckpt")
    
    # ğŸ”§ MoEå‚æ•°
    parser.add_argument("--unified_dim", type=int, default=25, help="ç»Ÿä¸€çš„ä¸­é—´ç»´åº¦")
    parser.add_argument("--moe_num_experts", type=int, default=1, help="ä¸“å®¶æ•°é‡")
    parser.add_argument("--moe_top_k", type=int, default=1, help="Top-Kä¸“å®¶")
    parser.add_argument("--moe_loss_weight", type=float, default=0.00, help="MoEæŸå¤±æƒé‡")
    
    args = parser.parse_args()
    
    print("ğŸ”§ å¤šæ•°æ®é›†MoEè®­ç»ƒé…ç½®:")
    print(f"  - ä½¿ç”¨wan_video_dit_moe.pyä½œä¸ºæ¨¡å‹")
    print(f"  - ç»Ÿä¸€ç»´åº¦: {args.unified_dim}")
    print(f"  - ä¸“å®¶æ•°é‡: {args.moe_num_experts}")
    print(f"  - Top-K: {args.moe_top_k}")
    print(f"  - MoEæŸå¤±æƒé‡: {args.moe_loss_weight}")
    print("  - æ•°æ®é›†:")
    print("    - sekai-game-drone (sekaiæ¨¡æ€)")
    print("    - sekai-game-walking (sekaiæ¨¡æ€)")
    print("    - spatialvid (ä½¿ç”¨sekaiæ¨¡æ€å¤„ç†å™¨)") 
    print("    - openx-fractal (ä½¿ç”¨sekaiæ¨¡æ€å¤„ç†å™¨)")
    print(f"   - nuscenes (nuscenesæ¨¡æ€)")
    
    train_multi_dataset(args)