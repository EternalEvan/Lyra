import os
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import imageio
import json
from diffsynth import WanVideoReCamMasterPipeline, ModelManager
import argparse
from torchvision.transforms import v2
from einops import rearrange
import copy


def load_encoded_video_from_pth(pth_path, start_frame=0, num_frames=10):
    """ä»pthæ–‡ä»¶åŠ è½½é¢„ç¼–ç çš„è§†é¢‘æ•°æ®"""
    print(f"Loading encoded video from {pth_path}")
    
    encoded_data = torch.load(pth_path, weights_only=False, map_location="cpu")
    full_latents = encoded_data['latents']  # [C, T, H, W]
    
    print(f"Full latents shape: {full_latents.shape}")
    print(f"Extracting frames {start_frame} to {start_frame + num_frames}")
    
    if start_frame + num_frames > full_latents.shape[1]:
        raise ValueError(f"Not enough frames: requested {start_frame + num_frames}, available {full_latents.shape[1]}")
    
    condition_latents = full_latents[:, start_frame:start_frame + num_frames, :, :]
    print(f"Extracted condition latents shape: {condition_latents.shape}")
    
    return condition_latents, encoded_data


def compute_relative_pose(pose_a, pose_b, use_torch=False):
    """è®¡ç®—ç›¸æœºBç›¸å¯¹äºç›¸æœºAçš„ç›¸å¯¹ä½å§¿çŸ©é˜µ"""
    assert pose_a.shape == (4, 4), f"ç›¸æœºAå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_a.shape}"
    assert pose_b.shape == (4, 4), f"ç›¸æœºBå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_b.shape}"
    
    if use_torch:
        if not isinstance(pose_a, torch.Tensor):
            pose_a = torch.from_numpy(pose_a).float()
        if not isinstance(pose_b, torch.Tensor):
            pose_b = torch.from_numpy(pose_b).float()
        
        pose_a_inv = torch.inverse(pose_a)
        relative_pose = torch.matmul(pose_b, pose_a_inv)
    else:
        if not isinstance(pose_a, np.ndarray):
            pose_a = np.array(pose_a, dtype=np.float32)
        if not isinstance(pose_b, np.ndarray):
            pose_b = np.array(pose_b, dtype=np.float32)
        
        pose_a_inv = np.linalg.inv(pose_a)
        relative_pose = np.matmul(pose_b, pose_a_inv)
    
    return relative_pose


def replace_dit_model_in_manager():
    """æ›¿æ¢DiTæ¨¡å‹ç±»ä¸ºMoEç‰ˆæœ¬"""
    from diffsynth.models.wan_video_dit_moe import WanModelMoe
    from diffsynth.configs.model_config import model_loader_configs
    
    for i, config in enumerate(model_loader_configs):
        keys_hash, keys_hash_with_shape, model_names, model_classes, model_resource = config
        
        if 'wan_video_dit' in model_names:
            new_model_names = []
            new_model_classes = []
            
            for name, cls in zip(model_names, model_classes):
                if name == 'wan_video_dit':
                    new_model_names.append(name)
                    new_model_classes.append(WanModelMoe)
                    print(f"âœ… æ›¿æ¢äº†æ¨¡å‹ç±»: {name} -> WanModelMoe")
                else:
                    new_model_names.append(name)
                    new_model_classes.append(cls)
            
            model_loader_configs[i] = (keys_hash, keys_hash_with_shape, new_model_names, new_model_classes, model_resource)


def add_framepack_components(dit_model):
    """æ·»åŠ FramePackç›¸å…³ç»„ä»¶"""
    if not hasattr(dit_model, 'clean_x_embedder'):
        inner_dim = dit_model.blocks[0].self_attn.q.weight.shape[0]
        
        class CleanXEmbedder(nn.Module):
            def __init__(self, inner_dim):
                super().__init__()
                self.proj = nn.Conv3d(16, inner_dim, kernel_size=(1, 2, 2), stride=(1, 2, 2))
                self.proj_2x = nn.Conv3d(16, inner_dim, kernel_size=(2, 4, 4), stride=(2, 4, 4))
                self.proj_4x = nn.Conv3d(16, inner_dim, kernel_size=(4, 8, 8), stride=(4, 8, 8))
            
            def forward(self, x, scale="1x"):
                if scale == "1x":
                    x = x.to(self.proj.weight.dtype)
                    return self.proj(x)
                elif scale == "2x":
                    x = x.to(self.proj_2x.weight.dtype)
                    return self.proj_2x(x)
                elif scale == "4x":
                    x = x.to(self.proj_4x.weight.dtype)
                    return self.proj_4x(x)
                else:
                    raise ValueError(f"Unsupported scale: {scale}")
        
        dit_model.clean_x_embedder = CleanXEmbedder(inner_dim)
        model_dtype = next(dit_model.parameters()).dtype
        dit_model.clean_x_embedder = dit_model.clean_x_embedder.to(dtype=model_dtype)
        print("âœ… æ·»åŠ äº†FramePackçš„clean_x_embedderç»„ä»¶")


def add_moe_components(dit_model, moe_config):
    """ğŸ”§ æ·»åŠ MoEç›¸å…³ç»„ä»¶ - ä¿®æ­£ç‰ˆæœ¬"""
    if not hasattr(dit_model, 'moe_config'):
        dit_model.moe_config = moe_config
        print("âœ… æ·»åŠ äº†MoEé…ç½®åˆ°æ¨¡å‹")
    
    # ä¸ºæ¯ä¸ªblockåŠ¨æ€æ·»åŠ MoEç»„ä»¶
    dim = dit_model.blocks[0].self_attn.q.weight.shape[0]
    unified_dim = moe_config.get("unified_dim", 25)
    
    for i, block in enumerate(dit_model.blocks):
        from diffsynth.models.wan_video_dit_moe import ModalityProcessor, MultiModalMoE
        
        # Sekaiæ¨¡æ€å¤„ç†å™¨ - è¾“å‡ºunified_dim
        block.sekai_processor = ModalityProcessor("sekai", 13, unified_dim)
        
        # # NuScenesæ¨¡æ€å¤„ç†å™¨ - è¾“å‡ºunified_dim  
        # block.nuscenes_processor = ModalityProcessor("nuscenes", 8, unified_dim)
        
        # MoEç½‘ç»œ - è¾“å…¥unified_dimï¼Œè¾“å‡ºdim
        block.moe = MultiModalMoE(
            unified_dim=unified_dim,
            output_dim=dim,  # è¾“å‡ºç»´åº¦åŒ¹é…transformer blockçš„dim
            num_experts=moe_config.get("num_experts", 4),
            top_k=moe_config.get("top_k", 2)
        )
        
        print(f"âœ… Block {i} æ·»åŠ äº†MoEç»„ä»¶ (unified_dim: {unified_dim}, experts: {moe_config.get('num_experts', 4)})")


def generate_sekai_camera_embeddings_sliding(cam_data, start_frame, current_history_length, new_frames, total_generated, use_real_poses=True):
    """ä¸ºSekaiæ•°æ®é›†ç”Ÿæˆcamera embeddings - æ»‘åŠ¨çª—å£ç‰ˆæœ¬"""
    time_compression_ratio = 4
    
    # è®¡ç®—FramePackå®é™…éœ€è¦çš„cameraå¸§æ•°
    framepack_needed_frames = 1 + 16 + 2 + 1 + new_frames
    
    if use_real_poses and cam_data is not None and 'extrinsic' in cam_data:
        print("ğŸ”§ ä½¿ç”¨çœŸå®Sekai cameraæ•°æ®")
        cam_extrinsic = cam_data['extrinsic']
        
        # ç¡®ä¿ç”Ÿæˆè¶³å¤Ÿé•¿çš„cameraåºåˆ—
        max_needed_frames = max(
            start_frame + current_history_length + new_frames,
            framepack_needed_frames,
            30
        )
        
        print(f"ğŸ”§ è®¡ç®—Sekai cameraåºåˆ—é•¿åº¦:")
        print(f"  - åŸºç¡€éœ€æ±‚: {start_frame + current_history_length + new_frames}")
        print(f"  - FramePackéœ€æ±‚: {framepack_needed_frames}")
        print(f"  - æœ€ç»ˆç”Ÿæˆ: {max_needed_frames}")
        
        relative_poses = []
        for i in range(max_needed_frames):
            # è®¡ç®—å½“å‰å¸§åœ¨åŸå§‹åºåˆ—ä¸­çš„ä½ç½®
            frame_idx = i * time_compression_ratio
            next_frame_idx = frame_idx + time_compression_ratio
            
            if next_frame_idx < len(cam_extrinsic):
                cam_prev = cam_extrinsic[frame_idx]
                cam_next = cam_extrinsic[next_frame_idx]
                relative_pose = compute_relative_pose(cam_prev, cam_next)
                relative_poses.append(torch.as_tensor(relative_pose[:3, :]))
            else:
                # è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é›¶è¿åŠ¨
                print(f"âš ï¸ å¸§{frame_idx}è¶…å‡ºcameraæ•°æ®èŒƒå›´ï¼Œä½¿ç”¨é›¶è¿åŠ¨")
                relative_poses.append(torch.zeros(3, 4))
        
        pose_embedding = torch.stack(relative_poses, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        
        # åˆ›å»ºå¯¹åº”é•¿åº¦çš„maskåºåˆ—
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        # ä»start_frameåˆ°current_history_lengthæ ‡è®°ä¸ºcondition
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_embedding, mask], dim=1)
        print(f"ğŸ”§ SekaiçœŸå®camera embedding shape: {camera_embedding.shape}")
        return camera_embedding.to(torch.bfloat16)
        
    else:
        print("ğŸ”§ ä½¿ç”¨Sekaiåˆæˆcameraæ•°æ®")
        
        max_needed_frames = max(
            start_frame + current_history_length + new_frames,
            framepack_needed_frames,
            30
        )
        
        print(f"ğŸ”§ ç”ŸæˆSekaiåˆæˆcameraå¸§æ•°: {max_needed_frames}")
        relative_poses = []
        for i in range(max_needed_frames):
            # æŒç»­å·¦è½¬è¿åŠ¨æ¨¡å¼
            yaw_per_frame = 0.05  # æ¯å¸§å·¦è½¬ï¼ˆæ­£è§’åº¦è¡¨ç¤ºå·¦è½¬ï¼‰
            forward_speed = 0.005  # æ¯å¸§å‰è¿›è·ç¦»
            
            pose = np.eye(4, dtype=np.float32)
            
            # æ—‹è½¬çŸ©é˜µï¼ˆç»•Yè½´å·¦è½¬ï¼‰
            cos_yaw = np.cos(yaw_per_frame)
            sin_yaw = np.sin(yaw_per_frame)
            
            pose[0, 0] = cos_yaw
            pose[0, 2] = sin_yaw
            pose[2, 0] = -sin_yaw
            pose[2, 2] = cos_yaw
            
            # å¹³ç§»ï¼ˆåœ¨æ—‹è½¬åçš„å±€éƒ¨åæ ‡ç³»ä¸­å‰è¿›ï¼‰
            pose[2, 3] = -forward_speed  # å±€éƒ¨Zè½´è´Ÿæ–¹å‘ï¼ˆå‰è¿›ï¼‰
            
            # æ·»åŠ è½»å¾®çš„å‘å¿ƒè¿åŠ¨ï¼Œæ¨¡æ‹Ÿåœ†å½¢è½¨è¿¹
            radius_drift = 0.002  # å‘åœ†å¿ƒçš„è½»å¾®æ¼‚ç§»
            pose[0, 3] = -radius_drift  # å±€éƒ¨Xè½´è´Ÿæ–¹å‘ï¼ˆå‘å·¦ï¼‰
            
            relative_pose = pose[:3, :]
            relative_poses.append(torch.as_tensor(relative_pose))
        
        pose_embedding = torch.stack(relative_poses, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        
        # åˆ›å»ºå¯¹åº”é•¿åº¦çš„maskåºåˆ—
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_embedding, mask], dim=1)
        print(f"ğŸ”§ Sekaiåˆæˆcamera embedding shape: {camera_embedding.shape}")
        return camera_embedding.to(torch.bfloat16)

def generate_openx_camera_embeddings_sliding(encoded_data, start_frame, current_history_length, new_frames, use_real_poses):
    """ä¸ºOpenXæ•°æ®é›†ç”Ÿæˆcamera embeddings - æ»‘åŠ¨çª—å£ç‰ˆæœ¬"""
    time_compression_ratio = 4
    
    # è®¡ç®—FramePackå®é™…éœ€è¦çš„cameraå¸§æ•°
    framepack_needed_frames = 1 + 16 + 2 + 1 + new_frames
    
    if use_real_poses and encoded_data is not None and 'cam_emb' in encoded_data and 'extrinsic' in encoded_data['cam_emb']:
        print("ğŸ”§ ä½¿ç”¨OpenXçœŸå®cameraæ•°æ®")
        cam_extrinsic = encoded_data['cam_emb']['extrinsic']
        
        # ç¡®ä¿ç”Ÿæˆè¶³å¤Ÿé•¿çš„cameraåºåˆ—
        max_needed_frames = max(
            start_frame + current_history_length + new_frames,
            framepack_needed_frames,
            30
        )
        
        print(f"ğŸ”§ è®¡ç®—OpenX cameraåºåˆ—é•¿åº¦:")
        print(f"  - åŸºç¡€éœ€æ±‚: {start_frame + current_history_length + new_frames}")
        print(f"  - FramePackéœ€æ±‚: {framepack_needed_frames}")
        print(f"  - æœ€ç»ˆç”Ÿæˆ: {max_needed_frames}")
        
        relative_poses = []
        for i in range(max_needed_frames):
            # OpenXä½¿ç”¨4å€é—´éš”ï¼Œç±»ä¼¼sekaiä½†å¤„ç†æ›´çŸ­çš„åºåˆ—
            frame_idx = i * time_compression_ratio
            next_frame_idx = frame_idx + time_compression_ratio
            
            if next_frame_idx < len(cam_extrinsic):
                cam_prev = cam_extrinsic[frame_idx]
                cam_next = cam_extrinsic[next_frame_idx]
                relative_pose = compute_relative_pose(cam_prev, cam_next)
                relative_poses.append(torch.as_tensor(relative_pose[:3, :]))
            else:
                # è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é›¶è¿åŠ¨
                print(f"âš ï¸ å¸§{frame_idx}è¶…å‡ºOpenX cameraæ•°æ®èŒƒå›´ï¼Œä½¿ç”¨é›¶è¿åŠ¨")
                relative_poses.append(torch.zeros(3, 4))
        
        pose_embedding = torch.stack(relative_poses, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        
        # åˆ›å»ºå¯¹åº”é•¿åº¦çš„maskåºåˆ—
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        # ä»start_frameåˆ°current_history_lengthæ ‡è®°ä¸ºcondition
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_embedding, mask], dim=1)
        print(f"ğŸ”§ OpenXçœŸå®camera embedding shape: {camera_embedding.shape}")
        return camera_embedding.to(torch.bfloat16)
        
    else:
        print("ğŸ”§ ä½¿ç”¨OpenXåˆæˆcameraæ•°æ®")
        
        max_needed_frames = max(
            start_frame + current_history_length + new_frames,
            framepack_needed_frames,
            30
        )
        
        print(f"ğŸ”§ ç”ŸæˆOpenXåˆæˆcameraå¸§æ•°: {max_needed_frames}")
        relative_poses = []
        for i in range(max_needed_frames):
            # OpenXæœºå™¨äººæ“ä½œè¿åŠ¨æ¨¡å¼ - è¾ƒå°çš„è¿åŠ¨å¹…åº¦
            # æ¨¡æ‹Ÿæœºå™¨äººæ‰‹è‡‚çš„ç²¾ç»†æ“ä½œè¿åŠ¨
            roll_per_frame = 0.02   # è½»å¾®ç¿»æ»š
            pitch_per_frame = 0.01  # è½»å¾®ä¿¯ä»°
            yaw_per_frame = 0.015   # è½»å¾®åèˆª
            forward_speed = 0.003   # è¾ƒæ…¢çš„å‰è¿›é€Ÿåº¦
            
            pose = np.eye(4, dtype=np.float32)
            
            # å¤åˆæ—‹è½¬ - æ¨¡æ‹Ÿæœºå™¨äººæ‰‹è‡‚çš„å¤æ‚è¿åŠ¨
            # ç»•Xè½´æ—‹è½¬ï¼ˆrollï¼‰
            cos_roll = np.cos(roll_per_frame)
            sin_roll = np.sin(roll_per_frame)
            # ç»•Yè½´æ—‹è½¬ï¼ˆpitchï¼‰
            cos_pitch = np.cos(pitch_per_frame)
            sin_pitch = np.sin(pitch_per_frame)
            # ç»•Zè½´æ—‹è½¬ï¼ˆyawï¼‰
            cos_yaw = np.cos(yaw_per_frame)
            sin_yaw = np.sin(yaw_per_frame)
            
            # ç®€åŒ–çš„å¤åˆæ—‹è½¬çŸ©é˜µï¼ˆZYXé¡ºåºï¼‰
            pose[0, 0] = cos_yaw * cos_pitch
            pose[0, 1] = cos_yaw * sin_pitch * sin_roll - sin_yaw * cos_roll
            pose[0, 2] = cos_yaw * sin_pitch * cos_roll + sin_yaw * sin_roll
            pose[1, 0] = sin_yaw * cos_pitch
            pose[1, 1] = sin_yaw * sin_pitch * sin_roll + cos_yaw * cos_roll
            pose[1, 2] = sin_yaw * sin_pitch * cos_roll - cos_yaw * sin_roll
            pose[2, 0] = -sin_pitch
            pose[2, 1] = cos_pitch * sin_roll
            pose[2, 2] = cos_pitch * cos_roll
            
            # å¹³ç§» - æ¨¡æ‹Ÿæœºå™¨äººæ“ä½œçš„ç²¾ç»†ç§»åŠ¨
            pose[0, 3] = forward_speed * 0.5   # Xæ–¹å‘è½»å¾®ç§»åŠ¨
            pose[1, 3] = forward_speed * 0.3   # Yæ–¹å‘è½»å¾®ç§»åŠ¨
            pose[2, 3] = -forward_speed        # Zæ–¹å‘ï¼ˆæ·±åº¦ï¼‰ä¸»è¦ç§»åŠ¨
            
            relative_pose = pose[:3, :]
            relative_poses.append(torch.as_tensor(relative_pose))
        
        pose_embedding = torch.stack(relative_poses, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        
        # åˆ›å»ºå¯¹åº”é•¿åº¦çš„maskåºåˆ—
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_embedding, mask], dim=1)
        print(f"ğŸ”§ OpenXåˆæˆcamera embedding shape: {camera_embedding.shape}")
        return camera_embedding.to(torch.bfloat16)


def generate_nuscenes_camera_embeddings_sliding(scene_info, start_frame, current_history_length, new_frames):
    """ä¸ºNuScenesæ•°æ®é›†ç”Ÿæˆcamera embeddings - æ»‘åŠ¨çª—å£ç‰ˆæœ¬ - ä¿®æ­£ç‰ˆï¼Œä¸train_moe.pyä¿æŒä¸€è‡´"""
    time_compression_ratio = 4
    
    # è®¡ç®—FramePackå®é™…éœ€è¦çš„cameraå¸§æ•°
    framepack_needed_frames = 1 + 16 + 2 + 1 + new_frames
    
    if scene_info is not None and 'keyframe_poses' in scene_info:
        print("ğŸ”§ ä½¿ç”¨NuScenesçœŸå®poseæ•°æ®")
        keyframe_poses = scene_info['keyframe_poses']
        
        if len(keyframe_poses) == 0:
            print("âš ï¸ NuScenes keyframe_posesä¸ºç©ºï¼Œä½¿ç”¨é›¶pose")
            max_needed_frames = max(framepack_needed_frames, 30)
            
            pose_sequence = torch.zeros(max_needed_frames, 7, dtype=torch.float32)
            
            mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
            condition_end = min(start_frame + current_history_length, max_needed_frames)
            mask[start_frame:condition_end] = 1.0
            
            camera_embedding = torch.cat([pose_sequence, mask], dim=1)  # [max_needed_frames, 8]
            print(f"ğŸ”§ NuScenesé›¶pose embedding shape: {camera_embedding.shape}")
            return camera_embedding.to(torch.bfloat16)
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªposeä½œä¸ºå‚è€ƒ
        reference_pose = keyframe_poses[0]
        
        max_needed_frames = max(framepack_needed_frames, 30)
        
        pose_vecs = []
        for i in range(max_needed_frames):
            if i < len(keyframe_poses):
                current_pose = keyframe_poses[i]
                
                # è®¡ç®—ç›¸å¯¹ä½ç§»
                translation = torch.tensor(
                    np.array(current_pose['translation']) - np.array(reference_pose['translation']),
                    dtype=torch.float32
                )
                
                # è®¡ç®—ç›¸å¯¹æ—‹è½¬ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                rotation = torch.tensor(current_pose['rotation'], dtype=torch.float32)
                
                pose_vec = torch.cat([translation, rotation], dim=0)  # [7D]
            else:
                # è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é›¶pose
                pose_vec = torch.cat([
                    torch.zeros(3, dtype=torch.float32),
                    torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)
                ], dim=0)  # [7D]
            
            pose_vecs.append(pose_vec)
        
        pose_sequence = torch.stack(pose_vecs, dim=0)  # [max_needed_frames, 7]
        
        # åˆ›å»ºmask
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_sequence, mask], dim=1)  # [max_needed_frames, 8]
        print(f"ğŸ”§ NuScenesçœŸå®pose embedding shape: {camera_embedding.shape}")
        return camera_embedding.to(torch.bfloat16)
    
    else:
        print("ğŸ”§ ä½¿ç”¨NuScenesåˆæˆposeæ•°æ®")
        max_needed_frames = max(framepack_needed_frames, 30)
        
        # åˆ›å»ºåˆæˆè¿åŠ¨åºåˆ—
        pose_vecs = []
        for i in range(max_needed_frames):
            # ç®€å•çš„å‰è¿›è¿åŠ¨
            translation = torch.tensor([0.0, 0.0, i * 0.1], dtype=torch.float32)  # æ²¿Zè½´å‰è¿›
            rotation = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)  # æ— æ—‹è½¬
            
            pose_vec = torch.cat([translation, rotation], dim=0)  # [7D]
            pose_vecs.append(pose_vec)
        
        pose_sequence = torch.stack(pose_vecs, dim=0)
        
        # åˆ›å»ºmask
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_sequence, mask], dim=1)  # [max_needed_frames, 8]
        print(f"ğŸ”§ NuScenesåˆæˆpose embedding shape: {camera_embedding.shape}")
        return camera_embedding.to(torch.bfloat16)

def prepare_framepack_sliding_window_with_camera_moe(history_latents, target_frames_to_generate, camera_embedding_full, start_frame, modality_type, max_history_frames=49):
    """FramePackæ»‘åŠ¨çª—å£æœºåˆ¶ - MoEç‰ˆæœ¬"""
    # history_latents: [C, T, H, W] å½“å‰çš„å†å²latents
    C, T, H, W = history_latents.shape
    
    # å›ºå®šç´¢å¼•ç»“æ„ï¼ˆè¿™å†³å®šäº†éœ€è¦çš„cameraå¸§æ•°ï¼‰
    total_indices_length = 1 + 16 + 2 + 1 + target_frames_to_generate
    indices = torch.arange(0, total_indices_length)
    split_sizes = [1, 16, 2, 1, target_frames_to_generate]
    clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = \
        indices.split(split_sizes, dim=0)
    clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=0)
    
    # æ£€æŸ¥cameraé•¿åº¦æ˜¯å¦è¶³å¤Ÿ
    if camera_embedding_full.shape[0] < total_indices_length:
        shortage = total_indices_length - camera_embedding_full.shape[0]
        padding = torch.zeros(shortage, camera_embedding_full.shape[1], 
                            dtype=camera_embedding_full.dtype, device=camera_embedding_full.device)
        camera_embedding_full = torch.cat([camera_embedding_full, padding], dim=0)
    
    # ä»å®Œæ•´cameraåºåˆ—ä¸­é€‰å–å¯¹åº”éƒ¨åˆ†
    combined_camera = camera_embedding_full[:total_indices_length, :].clone()
    
    # æ ¹æ®å½“å‰history lengthé‡æ–°è®¾ç½®mask
    combined_camera[:, -1] = 0.0  # å…ˆå…¨éƒ¨è®¾ä¸ºtarget (0)
    
    # è®¾ç½®condition maskï¼šå‰19å¸§æ ¹æ®å®é™…å†å²é•¿åº¦å†³å®š
    if T > 0:
        available_frames = min(T, 19)
        start_pos = 19 - available_frames
        combined_camera[start_pos:19, -1] = 1.0  # å°†æœ‰æ•ˆçš„clean latentså¯¹åº”çš„cameraæ ‡è®°ä¸ºcondition
    
    print(f"ğŸ”§ MoE Camera maskæ›´æ–°:")
    print(f"  - å†å²å¸§æ•°: {T}")
    print(f"  - æœ‰æ•ˆconditionå¸§æ•°: {available_frames if T > 0 else 0}")
    print(f"  - æ¨¡æ€ç±»å‹: {modality_type}")
    
    # å¤„ç†latents
    clean_latents_combined = torch.zeros(C, 19, H, W, dtype=history_latents.dtype, device=history_latents.device)
    
    if T > 0:
        available_frames = min(T, 19)
        start_pos = 19 - available_frames
        clean_latents_combined[:, start_pos:, :, :] = history_latents[:, -available_frames:, :, :]
    
    clean_latents_4x = clean_latents_combined[:, 0:16, :, :]
    clean_latents_2x = clean_latents_combined[:, 16:18, :, :]
    clean_latents_1x = clean_latents_combined[:, 18:19, :, :]
    
    if T > 0:
        start_latent = history_latents[:, 0:1, :, :]
    else:
        start_latent = torch.zeros(C, 1, H, W, dtype=history_latents.dtype, device=history_latents.device)
    
    clean_latents = torch.cat([start_latent, clean_latents_1x], dim=1)
    
    return {
        'latent_indices': latent_indices,
        'clean_latents': clean_latents,
        'clean_latents_2x': clean_latents_2x,
        'clean_latents_4x': clean_latents_4x,
        'clean_latent_indices': clean_latent_indices,
        'clean_latent_2x_indices': clean_latent_2x_indices,
        'clean_latent_4x_indices': clean_latent_4x_indices,
        'camera_embedding': combined_camera,
        'modality_type': modality_type,  # æ–°å¢æ¨¡æ€ç±»å‹ä¿¡æ¯
        'current_length': T,
        'next_length': T + target_frames_to_generate
    }


def inference_moe_framepack_sliding_window(
    condition_pth_path,
    dit_path,
    output_path="moe/infer_results/output_moe_framepack_sliding.mp4",
    start_frame=0,
    initial_condition_frames=8,
    frames_per_generation=4,
    total_frames_to_generate=32,
    max_history_frames=49,
    device="cuda",
    prompt="A video of a scene shot using a pedestrian's front camera while walking",
    modality_type="sekai",  # "sekai" æˆ– "nuscenes"
    use_real_poses=True,
    scene_info_path=None,  # å¯¹äºNuScenesæ•°æ®é›†
    # CFGå‚æ•°
    use_camera_cfg=True,
    camera_guidance_scale=2.0,
    text_guidance_scale=1.0,
    # MoEå‚æ•°
    moe_num_experts=4,
    moe_top_k=2,
    moe_hidden_dim=None
):
    """
    MoE FramePackæ»‘åŠ¨çª—å£è§†é¢‘ç”Ÿæˆ - æ”¯æŒå¤šæ¨¡æ€
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    print(f"ğŸ”§ MoE FramePackæ»‘åŠ¨çª—å£ç”Ÿæˆå¼€å§‹...")
    print(f"æ¨¡æ€ç±»å‹: {modality_type}")
    print(f"Camera CFG: {use_camera_cfg}, Camera guidance scale: {camera_guidance_scale}")
    print(f"Text guidance scale: {text_guidance_scale}")
    print(f"MoEé…ç½®: experts={moe_num_experts}, top_k={moe_top_k}")
    
    # 1. æ¨¡å‹åˆå§‹åŒ–
    replace_dit_model_in_manager()
    
    model_manager = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
    model_manager.load_models([
        "models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors",
        "models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth",
        "models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth",
    ])
    pipe = WanVideoReCamMasterPipeline.from_model_manager(model_manager, device="cuda")

    # 2. æ·»åŠ ä¼ ç»Ÿcameraç¼–ç å™¨ï¼ˆå…¼å®¹æ€§ï¼‰
    dim = pipe.dit.blocks[0].self_attn.q.weight.shape[0]
    for block in pipe.dit.blocks:
        block.cam_encoder = nn.Linear(13, dim)
        block.projector = nn.Linear(dim, dim)
        block.cam_encoder.weight.data.zero_()
        block.cam_encoder.bias.data.zero_()
        block.projector.weight = nn.Parameter(torch.eye(dim))
        block.projector.bias = nn.Parameter(torch.zeros(dim))
    
    # 3. æ·»åŠ FramePackç»„ä»¶
    add_framepack_components(pipe.dit)
    
    # 4. æ·»åŠ MoEç»„ä»¶
    moe_config = {
        "num_experts": moe_num_experts,
        "top_k": moe_top_k,
        "hidden_dim": moe_hidden_dim or dim * 2,
        "sekai_input_dim": 13,    # Sekai: 12ç»´pose + 1ç»´mask
        "nuscenes_input_dim": 8,   # NuScenes: 7ç»´pose + 1ç»´mask
        "openx_input_dim": 13       # OpenX: 12ç»´pose + 1ç»´mask (ç±»ä¼¼sekai)
    }
    add_moe_components(pipe.dit, moe_config)
    
    # 5. åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    dit_state_dict = torch.load(dit_path, map_location="cpu")
    pipe.dit.load_state_dict(dit_state_dict, strict=False)  # ä½¿ç”¨strict=Falseä»¥å…¼å®¹æ–°å¢çš„MoEç»„ä»¶
    pipe = pipe.to(device)
    model_dtype = next(pipe.dit.parameters()).dtype
    
    if hasattr(pipe.dit, 'clean_x_embedder'):
        pipe.dit.clean_x_embedder = pipe.dit.clean_x_embedder.to(dtype=model_dtype)
    
    pipe.scheduler.set_timesteps(50)
    
    # 6. åŠ è½½åˆå§‹æ¡ä»¶
    print("Loading initial condition frames...")
    initial_latents, encoded_data = load_encoded_video_from_pth(
        condition_pth_path, 
        start_frame=start_frame,
        num_frames=initial_condition_frames
    )
    
    # ç©ºé—´è£å‰ª
    target_height, target_width = 60, 104
    C, T, H, W = initial_latents.shape
    
    if H > target_height or W > target_width:
        h_start = (H - target_height) // 2
        w_start = (W - target_width) // 2
        initial_latents = initial_latents[:, :, h_start:h_start+target_height, w_start:w_start+target_width]
        H, W = target_height, target_width
    
    history_latents = initial_latents.to(device, dtype=model_dtype)
    
    print(f"åˆå§‹history_latents shape: {history_latents.shape}")
    
    # 7. ç¼–ç prompt - æ”¯æŒCFG
    if text_guidance_scale > 1.0:
        prompt_emb_pos = pipe.encode_prompt(prompt)
        prompt_emb_neg = pipe.encode_prompt("")
        print(f"ä½¿ç”¨Text CFGï¼Œguidance scale: {text_guidance_scale}")
    else:
        prompt_emb_pos = pipe.encode_prompt(prompt)
        prompt_emb_neg = None
        print("ä¸ä½¿ç”¨Text CFG")
    
    # 8. åŠ è½½åœºæ™¯ä¿¡æ¯ï¼ˆå¯¹äºNuScenesï¼‰
    scene_info = None
    if modality_type == "nuscenes" and scene_info_path and os.path.exists(scene_info_path):
        with open(scene_info_path, 'r') as f:
            scene_info = json.load(f)
        print(f"åŠ è½½NuScenesåœºæ™¯ä¿¡æ¯: {scene_info_path}")
    
    # 9. é¢„ç”Ÿæˆå®Œæ•´çš„camera embeddingåºåˆ—
    if modality_type == "sekai":
        camera_embedding_full = generate_sekai_camera_embeddings_sliding(
            encoded_data.get('cam_emb', None),
            0,
            max_history_frames,
            0,
            0,
            use_real_poses=use_real_poses
        ).to(device, dtype=model_dtype)
    elif modality_type == "nuscenes":
        camera_embedding_full = generate_nuscenes_camera_embeddings_sliding(
            scene_info,
            0,
            max_history_frames,
            0
        ).to(device, dtype=model_dtype)
    elif modality_type == "openx":
        camera_embedding_full = generate_openx_camera_embeddings_sliding(
            encoded_data,
            0,
            max_history_frames,
            0,
            use_real_poses=use_real_poses
        ).to(device, dtype=model_dtype)        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡æ€ç±»å‹: {modality_type}")
    
    print(f"å®Œæ•´cameraåºåˆ—shape: {camera_embedding_full.shape}")
    
    # 10. ä¸ºCamera CFGåˆ›å»ºæ— æ¡ä»¶çš„camera embedding
    if use_camera_cfg:
        camera_embedding_uncond = torch.zeros_like(camera_embedding_full)
        print(f"åˆ›å»ºæ— æ¡ä»¶camera embeddingç”¨äºCFG")
    
    # 11. æ»‘åŠ¨çª—å£ç”Ÿæˆå¾ªç¯
    total_generated = 0
    all_generated_frames = []
    
    while total_generated < total_frames_to_generate:
        current_generation = min(frames_per_generation, total_frames_to_generate - total_generated)
        print(f"\nğŸ”§ ç”Ÿæˆæ­¥éª¤ {total_generated // frames_per_generation + 1}")
        print(f"å½“å‰å†å²é•¿åº¦: {history_latents.shape[1]}, æœ¬æ¬¡ç”Ÿæˆ: {current_generation}")
        
        # FramePackæ•°æ®å‡†å¤‡ - MoEç‰ˆæœ¬
        framepack_data = prepare_framepack_sliding_window_with_camera_moe(
            history_latents,
            current_generation,
            camera_embedding_full,
            start_frame,
            modality_type,
            max_history_frames
        )
        
        # å‡†å¤‡è¾“å…¥
        clean_latents = framepack_data['clean_latents'].unsqueeze(0)
        clean_latents_2x = framepack_data['clean_latents_2x'].unsqueeze(0)
        clean_latents_4x = framepack_data['clean_latents_4x'].unsqueeze(0)
        camera_embedding = framepack_data['camera_embedding'].unsqueeze(0)
        
        # å‡†å¤‡modality_inputs
        modality_inputs = {modality_type: camera_embedding}
        
        # ä¸ºCFGå‡†å¤‡æ— æ¡ä»¶camera embedding
        if use_camera_cfg:
            camera_embedding_uncond_batch = camera_embedding_uncond[:camera_embedding.shape[1], :].unsqueeze(0)
            modality_inputs_uncond = {modality_type: camera_embedding_uncond_batch}
        
        # ç´¢å¼•å¤„ç†
        latent_indices = framepack_data['latent_indices'].unsqueeze(0).cpu()
        clean_latent_indices = framepack_data['clean_latent_indices'].unsqueeze(0).cpu()
        clean_latent_2x_indices = framepack_data['clean_latent_2x_indices'].unsqueeze(0).cpu()
        clean_latent_4x_indices = framepack_data['clean_latent_4x_indices'].unsqueeze(0).cpu()
        
        # åˆå§‹åŒ–è¦ç”Ÿæˆçš„latents
        new_latents = torch.randn(
            1, C, current_generation, H, W,
            device=device, dtype=model_dtype
        )
        
        extra_input = pipe.prepare_extra_input(new_latents)
        
        print(f"Camera embedding shape: {camera_embedding.shape}")
        print(f"Camera maskåˆ†å¸ƒ - condition: {torch.sum(camera_embedding[0, :, -1] == 1.0).item()}, target: {torch.sum(camera_embedding[0, :, -1] == 0.0).item()}")
        
        # å»å™ªå¾ªç¯ - æ”¯æŒCFG
        timesteps = pipe.scheduler.timesteps
        
        for i, timestep in enumerate(timesteps):
            if i % 10 == 0:
                print(f"  å»å™ªæ­¥éª¤ {i+1}/{len(timesteps)}")
            
            timestep_tensor = timestep.unsqueeze(0).to(device, dtype=model_dtype)
            
            with torch.no_grad():
                # CFGæ¨ç†
                if use_camera_cfg and camera_guidance_scale > 1.0:
                    # æ¡ä»¶é¢„æµ‹ï¼ˆæœ‰cameraï¼‰
                    noise_pred_cond, moe_loss = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        modality_inputs=modality_inputs,  # MoEæ¨¡æ€è¾“å…¥
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_pos,
                        **extra_input
                    )
                    
                    # æ— æ¡ä»¶é¢„æµ‹ï¼ˆæ— cameraï¼‰
                    noise_pred_uncond, moe_loss = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding_uncond_batch,
                        modality_inputs=modality_inputs_uncond,  # MoEæ— æ¡ä»¶æ¨¡æ€è¾“å…¥
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **(prompt_emb_neg if prompt_emb_neg else prompt_emb_pos),
                        **extra_input
                    )
                    
                    # Camera CFG
                    noise_pred = noise_pred_uncond + camera_guidance_scale * (noise_pred_cond - noise_pred_uncond)
                    
                    # å¦‚æœåŒæ—¶ä½¿ç”¨Text CFG
                    if text_guidance_scale > 1.0 and prompt_emb_neg:
                        noise_pred_text_uncond, moe_loss = pipe.dit(
                            new_latents,
                            timestep=timestep_tensor,
                            cam_emb=camera_embedding,
                            modality_inputs=modality_inputs,
                            latent_indices=latent_indices,
                            clean_latents=clean_latents,
                            clean_latent_indices=clean_latent_indices,
                            clean_latents_2x=clean_latents_2x,
                            clean_latent_2x_indices=clean_latent_2x_indices,
                            clean_latents_4x=clean_latents_4x,
                            clean_latent_4x_indices=clean_latent_4x_indices,
                            **prompt_emb_neg,
                            **extra_input
                        )
                        
                        # åº”ç”¨Text CFGåˆ°å·²ç»åº”ç”¨Camera CFGçš„ç»“æœ
                        noise_pred = noise_pred_text_uncond + text_guidance_scale * (noise_pred - noise_pred_text_uncond)
                
                elif text_guidance_scale > 1.0 and prompt_emb_neg:
                    # åªä½¿ç”¨Text CFG
                    noise_pred_cond, moe_loss = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        modality_inputs=modality_inputs,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_pos,
                        **extra_input
                    )
                    
                    noise_pred_uncond, moe_loss = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        modality_inputs=modality_inputs,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_neg,
                        **extra_input
                    )
                    
                    noise_pred = noise_pred_uncond + text_guidance_scale * (noise_pred_cond - noise_pred_uncond)
                
                else:
                    # æ ‡å‡†æ¨ç†ï¼ˆæ— CFGï¼‰
                    noise_pred, moe_loss = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        modality_inputs=modality_inputs,  # MoEæ¨¡æ€è¾“å…¥
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_pos,
                        **extra_input
                    )
            
            new_latents = pipe.scheduler.step(noise_pred, timestep, new_latents)
        
        # æ›´æ–°å†å²
        new_latents_squeezed = new_latents.squeeze(0)
        history_latents = torch.cat([history_latents, new_latents_squeezed], dim=1)
        
        # ç»´æŠ¤æ»‘åŠ¨çª—å£
        if history_latents.shape[1] > max_history_frames:
            first_frame = history_latents[:, 0:1, :, :]
            recent_frames = history_latents[:, -(max_history_frames-1):, :, :]
            history_latents = torch.cat([first_frame, recent_frames], dim=1)
            print(f"å†å²çª—å£å·²æ»¡ï¼Œä¿ç•™ç¬¬ä¸€å¸§+æœ€æ–°{max_history_frames-1}å¸§")
        
        print(f"æ›´æ–°åhistory_latents shape: {history_latents.shape}")
        
        all_generated_frames.append(new_latents_squeezed)
        total_generated += current_generation
        
        print(f"âœ… å·²ç”Ÿæˆ {total_generated}/{total_frames_to_generate} å¸§")
    
    # 12. è§£ç å’Œä¿å­˜
    print("\nğŸ”§ è§£ç ç”Ÿæˆçš„è§†é¢‘...")
    
    all_generated = torch.cat(all_generated_frames, dim=1)
    final_video = torch.cat([initial_latents.to(all_generated.device), all_generated], dim=1).unsqueeze(0)
    
    print(f"æœ€ç»ˆè§†é¢‘shape: {final_video.shape}")
    
    decoded_video = pipe.decode_video(final_video, tiled=True, tile_size=(34, 34), tile_stride=(18, 16))
    
    print(f"Saving video to {output_path}")
    
    video_np = decoded_video[0].to(torch.float32).permute(1, 2, 3, 0).cpu().numpy()
    video_np = (video_np * 0.5 + 0.5).clip(0, 1)
    video_np = (video_np * 255).astype(np.uint8)

    with imageio.get_writer(output_path, fps=20) as writer:
        for frame in video_np:
            writer.append_data(frame)

    print(f"ğŸ”§ MoE FramePackæ»‘åŠ¨çª—å£ç”Ÿæˆå®Œæˆ! ä¿å­˜åˆ°: {output_path}")
    print(f"æ€»å…±ç”Ÿæˆäº† {total_generated} å¸§ (å‹ç¼©å), å¯¹åº”åŸå§‹ {total_generated * 4} å¸§")
    print(f"ä½¿ç”¨æ¨¡æ€: {modality_type}")
    

def main():
    parser = argparse.ArgumentParser(description="MoE FramePackæ»‘åŠ¨çª—å£è§†é¢‘ç”Ÿæˆ - æ”¯æŒå¤šæ¨¡æ€")
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--condition_pth", type=str,
                       default="/share_zhuyixuan05/zhuyixuan05/sekai-game-walking/00100100001_0004650_0004950/encoded_video.pth")
                       #default="/share_zhuyixuan05/zhuyixuan05/nuscenes_video_generation_dynamic/scenes/scene-0001_CAM_FRONT/encoded_video-480p.pth")
                       #default="/share_zhuyixuan05/zhuyixuan05/spatialvid/a9a6d37f-0a6c-548a-a494-7d902469f3f2_0000000_0000300/encoded_video.pth")
                       #default="/share_zhuyixuan05/zhuyixuan05/openx-fractal-encoded/episode_000001/encoded_video.pth")
    parser.add_argument("--start_frame", type=int, default=0)
    parser.add_argument("--initial_condition_frames", type=int, default=16)
    parser.add_argument("--frames_per_generation", type=int, default=8)
    parser.add_argument("--total_frames_to_generate", type=int, default=40)
    parser.add_argument("--max_history_frames", type=int, default=100)
    parser.add_argument("--use_real_poses", action="store_true", default=False)
    parser.add_argument("--dit_path", type=str, 
                       default="/share_zhuyixuan05/zhuyixuan05/ICLR2026/framepack_moe_test/step1000_moe.ckpt")
    parser.add_argument("--output_path", type=str, 
                       default='/home/zhuyixuan05/ReCamMaster/moe/infer_results/output_moe_framepack_sliding.mp4')
    parser.add_argument("--prompt", type=str, 
                       default="A drone flying scene in a game world")
    parser.add_argument("--device", type=str, default="cuda")
    
    # æ¨¡æ€ç±»å‹å‚æ•°
    parser.add_argument("--modality_type", type=str, choices=["sekai", "nuscenes", "openx"], default="sekai",
                       help="æ¨¡æ€ç±»å‹ï¼šsekai æˆ– nuscenes æˆ– openx")
    parser.add_argument("--scene_info_path", type=str, default=None,
                       help="NuScenesåœºæ™¯ä¿¡æ¯æ–‡ä»¶è·¯å¾„ï¼ˆä»…ç”¨äºnuscenesæ¨¡æ€ï¼‰")
    
    # CFGå‚æ•°
    parser.add_argument("--use_camera_cfg", default=True,
                       help="ä½¿ç”¨Camera CFG")
    parser.add_argument("--camera_guidance_scale", type=float, default=2.0,
                       help="Camera guidance scale for CFG")
    parser.add_argument("--text_guidance_scale", type=float, default=1.0,
                       help="Text guidance scale for CFG")
    
    # MoEå‚æ•°
    parser.add_argument("--moe_num_experts", type=int, default=1, help="ä¸“å®¶æ•°é‡")
    parser.add_argument("--moe_top_k", type=int, default=1, help="Top-Kä¸“å®¶")
    parser.add_argument("--moe_hidden_dim", type=int, default=None, help="MoEéšè—å±‚ç»´åº¦")
    
    args = parser.parse_args()

    print(f"ğŸ”§ MoE FramePack CFGç”Ÿæˆè®¾ç½®:")
    print(f"æ¨¡æ€ç±»å‹: {args.modality_type}")
    print(f"Camera CFG: {args.use_camera_cfg}")
    if args.use_camera_cfg:
        print(f"Camera guidance scale: {args.camera_guidance_scale}")
    print(f"Text guidance scale: {args.text_guidance_scale}")
    print(f"MoEé…ç½®: experts={args.moe_num_experts}, top_k={args.moe_top_k}")
    
    # éªŒè¯NuSceneså‚æ•°
    if args.modality_type == "nuscenes" and not args.scene_info_path:
        print("âš ï¸ ä½¿ç”¨NuScenesæ¨¡æ€ä½†æœªæä¾›scene_info_pathï¼Œå°†ä½¿ç”¨åˆæˆposeæ•°æ®")
    
    inference_moe_framepack_sliding_window(
        condition_pth_path=args.condition_pth,
        dit_path=args.dit_path,
        output_path=args.output_path,
        start_frame=args.start_frame,
        initial_condition_frames=args.initial_condition_frames,
        frames_per_generation=args.frames_per_generation,
        total_frames_to_generate=args.total_frames_to_generate,
        max_history_frames=args.max_history_frames,
        device=args.device,
        prompt=args.prompt,
        modality_type=args.modality_type,
        use_real_poses=args.use_real_poses,
        scene_info_path=args.scene_info_path,
        # CFGå‚æ•°
        use_camera_cfg=args.use_camera_cfg,
        camera_guidance_scale=args.camera_guidance_scale,
        text_guidance_scale=args.text_guidance_scale,
        # MoEå‚æ•°
        moe_num_experts=args.moe_num_experts,
        moe_top_k=args.moe_top_k,
        moe_hidden_dim=args.moe_hidden_dim
    )


if __name__ == "__main__":
    main()