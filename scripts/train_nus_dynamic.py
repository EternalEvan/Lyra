import torch
import torch.nn as nn
import lightning as pl
import wandb
import os
import copy
from diffsynth import WanVideoReCamMasterPipeline, ModelManager
import os
import json
import torch
import numpy as np
from PIL import Image
import imageio
import random
from torchvision.transforms import v2
from einops import rearrange
from pose_classifier import PoseClassifier

class DynamicNuScenesDataset(torch.utils.data.Dataset):
    """æ”¯æŒåŠ¨æ€å†å²é•¿åº¦çš„NuScenesæ•°æ®é›†"""
    
    def __init__(self, base_path, steps_per_epoch, 
                 min_condition_frames=10, max_condition_frames=40,
                 target_frames=10, height=900, width=1600):
        self.base_path = base_path
        self.scenes_path = os.path.join(base_path, "scenes")
        self.min_condition_frames = min_condition_frames
        self.max_condition_frames = max_condition_frames
        self.target_frames = target_frames
        self.height = height
        self.width = width
        self.steps_per_epoch = steps_per_epoch
        self.pose_classifier = PoseClassifier()
        
        # ğŸ”§ æ–°å¢ï¼šVAEæ—¶é—´å‹ç¼©æ¯”ä¾‹
        self.time_compression_ratio = 4  # VAEå°†æ—¶é—´ç»´åº¦å‹ç¼©4å€
        
        # æŸ¥æ‰¾æ‰€æœ‰å¤„ç†å¥½çš„åœºæ™¯
        self.scene_dirs = []
        if os.path.exists(self.scenes_path):
            for item in os.listdir(self.scenes_path):
                scene_dir = os.path.join(self.scenes_path, item)
                if os.path.isdir(scene_dir):
                    scene_info_path = os.path.join(scene_dir, "scene_info.json")
                    if os.path.exists(scene_info_path):
                        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼–ç çš„tensoræ–‡ä»¶
                        encoded_path = os.path.join(scene_dir, "encoded_video-480p.pth")
                        if os.path.exists(encoded_path):
                            self.scene_dirs.append(scene_dir)
        
        # print(f"Found {len(self.scene_dirs)} scenes with encoded data")
        assert len(self.scene_dirs) > 0, "No encoded scenes found!"
        
        # é¢„å¤„ç†è®¾ç½®
        self.frame_process = v2.Compose([
            v2.CenterCrop(size=(height, width)),
            v2.Resize(size=(height, width), antialias=True),
            v2.ToTensor(),
            v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        ])

    def calculate_relative_rotation(self, current_rotation, reference_rotation):
        """è®¡ç®—ç›¸å¯¹æ—‹è½¬å››å…ƒæ•°"""
        q_current = torch.tensor(current_rotation, dtype=torch.float32)
        q_ref = torch.tensor(reference_rotation, dtype=torch.float32)

        q_ref_inv = torch.tensor([q_ref[0], -q_ref[1], -q_ref[2], -q_ref[3]])

        w1, x1, y1, z1 = q_ref_inv
        w2, x2, y2, z2 = q_current

        relative_rotation = torch.tensor([
            w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,
            w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,
            w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2,
            w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2
        ])

        return relative_rotation
    
    def select_dynamic_segment(self, scene_info):
        """åŠ¨æ€é€‰æ‹©æ¡ä»¶å¸§å’Œç›®æ ‡å¸§ - ä¿®æ­£ç‰ˆæœ¬å¤„ç†VAEæ—¶é—´å‹ç¼©"""
        keyframe_indices = scene_info['keyframe_indices']  # åŸå§‹å¸§ç´¢å¼•
        total_frames = scene_info['total_frames']  # åŸå§‹æ€»å¸§æ•°
        
        if len(keyframe_indices) < 2:
            print('error1____________')
            return None
        
        # ğŸ”§ è®¡ç®—å‹ç¼©åçš„å¸§æ•°
        compressed_total_frames = total_frames // self.time_compression_ratio
        compressed_keyframe_indices = [idx // self.time_compression_ratio for idx in keyframe_indices]
        
        # print(f"åŸå§‹æ€»å¸§æ•°: {total_frames}, å‹ç¼©å: {compressed_total_frames}")
        # print(f"åŸå§‹å…³é”®å¸§: {keyframe_indices[:5]}..., å‹ç¼©å: {compressed_keyframe_indices[:5]}...")
        
        # éšæœºé€‰æ‹©æ¡ä»¶å¸§é•¿åº¦ï¼ˆåŸºäºå‹ç¼©åçš„å¸§æ•°ï¼‰

        min_condition_compressed = self.min_condition_frames // self.time_compression_ratio
        max_condition_compressed = self.max_condition_frames // self.time_compression_ratio
        target_frames_compressed = self.target_frames // self.time_compression_ratio
        
        ratio = random.random()
        print('ratio:',ratio)
        if ratio<0.15:
            condition_frames_compressed = 1
        elif 0.15<=ratio<0.3:
            condition_frames_compressed = random.randint(min_condition_compressed, max_condition_compressed)
        else:
            condition_frames_compressed = target_frames_compressed
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§æ•°
        min_required_frames = condition_frames_compressed + target_frames_compressed
        if compressed_total_frames < min_required_frames:
            print(f"å‹ç¼©åå¸§æ•°ä¸è¶³: {compressed_total_frames} < {min_required_frames}")
            return None
        
        # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®ï¼ˆåŸºäºå‹ç¼©åçš„å¸§æ•°ï¼‰
        max_start = compressed_total_frames - min_required_frames
        start_frame_compressed = random.randint(0, max_start)
        
        condition_end_compressed = start_frame_compressed + condition_frames_compressed
        target_end_compressed = condition_end_compressed + target_frames_compressed
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨å‹ç¼©ç©ºé—´ä¸­æŸ¥æ‰¾å…³é”®å¸§
        condition_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                        if start_frame_compressed <= idx < condition_end_compressed]
        
        target_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                     if condition_end_compressed <= idx < target_end_compressed]
        
        if not condition_keyframes_compressed:
            print(f"æ¡ä»¶æ®µå†…æ— å…³é”®å¸§: {start_frame_compressed}-{condition_end_compressed}")
            return None
        
        # ä½¿ç”¨æ¡ä»¶æ®µçš„æœ€åä¸€ä¸ªå…³é”®å¸§ä½œä¸ºreference
        reference_keyframe_compressed = max(condition_keyframes_compressed)
        
        # ğŸ”§ æ‰¾åˆ°å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•ç”¨äºposeæŸ¥æ‰¾
        reference_keyframe_original_idx = None
        for i, compressed_idx in enumerate(compressed_keyframe_indices):
            if compressed_idx == reference_keyframe_compressed:
                reference_keyframe_original_idx = i
                break
        
        if reference_keyframe_original_idx is None:
            print(f"æ— æ³•æ‰¾åˆ°referenceå…³é”®å¸§çš„åŸå§‹ç´¢å¼•")
            return None
        
        # æ‰¾åˆ°ç›®æ ‡æ®µå¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        target_keyframes_original_indices = []
        for compressed_idx in target_keyframes_compressed:
            for i, comp_idx in enumerate(compressed_keyframe_indices):
                if comp_idx == compressed_idx:
                    target_keyframes_original_indices.append(i)
                    break
        
        return {
            'start_frame': start_frame_compressed,  # å‹ç¼©åçš„èµ·å§‹å¸§
            'condition_frames': condition_frames_compressed,  # å‹ç¼©åçš„æ¡ä»¶å¸§æ•°
            'target_frames': target_frames_compressed,  # å‹ç¼©åçš„ç›®æ ‡å¸§æ•°
            'condition_range': (start_frame_compressed, condition_end_compressed),
            'target_range': (condition_end_compressed, target_end_compressed),
            'reference_keyframe_idx': reference_keyframe_original_idx,  # åŸå§‹å…³é”®å¸§ç´¢å¼•
            'target_keyframe_indices': target_keyframes_original_indices,  # åŸå§‹å…³é”®å¸§ç´¢å¼•åˆ—è¡¨
            'original_condition_frames': condition_frames_compressed * self.time_compression_ratio,  # ç”¨äºè®°å½•
            'original_target_frames': target_frames_compressed * self.time_compression_ratio,
        }
    

    def create_pose_embeddings(self, scene_info, segment_info):
        """åˆ›å»ºpose embeddings - ä¿®æ­£ç‰ˆæœ¬ï¼ŒåŒ…å«conditionå’Œtargetçš„å®é™…pose"""
        keyframe_poses = scene_info['keyframe_poses']
        reference_keyframe_idx = segment_info['reference_keyframe_idx']
        target_keyframe_indices = segment_info['target_keyframe_indices']
        
        if reference_keyframe_idx >= len(keyframe_poses):
            return None
        
        reference_pose = keyframe_poses[reference_keyframe_idx]
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šposeå‘é‡åº”è¯¥åŒ…å«conditionå¸§å’Œtargetå¸§çš„å®é™…poseæ•°æ®
        condition_frames = segment_info['condition_frames']  # å‹ç¼©åçš„conditionå¸§æ•°
        target_frames = segment_info['target_frames']        # å‹ç¼©åçš„targetå¸§æ•°
        total_frames = condition_frames + target_frames      # æ€»å¸§æ•°ï¼Œä¸latentå¯¹é½
        
        print(f"åˆ›å»ºpose embedding: condition_frames={condition_frames}, target_frames={target_frames}, total_frames={total_frames}")
        
        # ğŸ”§ è·å–conditionæ®µçš„å…³é”®å¸§ç´¢å¼•
        start_frame = segment_info['start_frame']
        condition_end_compressed = start_frame + condition_frames
        
        # å‹ç¼©åçš„å…³é”®å¸§ç´¢å¼•
        compressed_keyframe_indices = [idx // self.time_compression_ratio for idx in scene_info['keyframe_indices']]
        
        # æ‰¾åˆ°conditionæ®µçš„å…³é”®å¸§
        condition_keyframes_compressed = [idx for idx in compressed_keyframe_indices 
                                        if start_frame <= idx < condition_end_compressed]
        
        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        condition_keyframes_original_indices = []
        for compressed_idx in condition_keyframes_compressed:
            for i, comp_idx in enumerate(compressed_keyframe_indices):
                if comp_idx == compressed_idx:
                    condition_keyframes_original_indices.append(i)
                    break
        
        pose_vecs = []
        frame_types = []  # æ–°å¢ï¼šè®°å½•æ¯å¸§æ˜¯conditionè¿˜æ˜¯target
        
        # ğŸ”§ å‰é¢çš„conditionå¸§ä½¿ç”¨å®é™…çš„poseæ•°æ®
        for i in range(condition_frames):
            if not condition_keyframes_original_indices:
                # å¦‚æœconditionæ®µæ²¡æœ‰å…³é”®å¸§ï¼Œä½¿ç”¨reference pose
                translation = torch.zeros(3, dtype=torch.float32)
                rotation = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)  # å•ä½å››å…ƒæ•°
            else:
                # ä¸ºconditionå¸§åˆ†é…pose
                if len(condition_keyframes_original_indices) == 1:
                    keyframe_idx = condition_keyframes_original_indices[0]
                else:
                    # çº¿æ€§æ’å€¼é€‰æ‹©å…³é”®å¸§
                    if condition_frames == 1:
                        keyframe_idx = condition_keyframes_original_indices[0]
                    else:
                        interp_ratio = i / (condition_frames - 1)
                        interp_idx = int(interp_ratio * (len(condition_keyframes_original_indices) - 1))
                        keyframe_idx = condition_keyframes_original_indices[interp_idx]
                
                if keyframe_idx >= len(keyframe_poses):
                    translation = torch.zeros(3, dtype=torch.float32)
                    rotation = torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32)
                else:
                    condition_pose = keyframe_poses[keyframe_idx]
                    
                    # è®¡ç®—ç›¸å¯¹äºreferenceçš„pose
                    translation = torch.tensor(
                        np.array(condition_pose['translation']) - np.array(reference_pose['translation']),
                        dtype=torch.float32
                    )
                    
                    relative_rotation = self.calculate_relative_rotation(
                        condition_pose['rotation'],
                        reference_pose['rotation']
                    )
                    
                    rotation = relative_rotation
            
            # ğŸ”§ æ·»åŠ frame type embeddingï¼š0è¡¨ç¤ºcondition
            pose_vec = torch.cat([translation, rotation, torch.tensor([0.0], dtype=torch.float32)], dim=0)  # [3+4+1=8D]
            pose_vecs.append(pose_vec)
            frame_types.append('condition')
        
        # ğŸ”§ åé¢çš„targetå¸§ä½¿ç”¨å®é™…çš„poseæ•°æ®
        if not target_keyframe_indices:
            # å¦‚æœç›®æ ‡æ®µæ²¡æœ‰å…³é”®å¸§ï¼Œtargetå¸§ä½¿ç”¨é›¶å‘é‡
            for i in range(target_frames):
                pose_vec = torch.cat([
                    torch.zeros(3, dtype=torch.float32),  # translation
                    torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32),  # rotation
                    torch.tensor([1.0], dtype=torch.float32)  # frame type: 1è¡¨ç¤ºtarget
                ], dim=0)
                pose_vecs.append(pose_vec)
                frame_types.append('target')
        else:
            # ä¸ºæ¯ä¸ªtargetå¸§åˆ†é…pose
            for i in range(target_frames):
                if len(target_keyframe_indices) == 1:
                    # åªæœ‰ä¸€ä¸ªå…³é”®å¸§ï¼Œæ‰€æœ‰targetå¸§ä½¿ç”¨ç›¸åŒçš„pose
                    target_keyframe_idx = target_keyframe_indices[0]
                else:
                    # å¤šä¸ªå…³é”®å¸§ï¼Œçº¿æ€§æ’å€¼é€‰æ‹©
                    if target_frames == 1:
                        # åªæœ‰ä¸€å¸§ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®å¸§
                        target_keyframe_idx = target_keyframe_indices[0]
                    else:
                        # çº¿æ€§æ’å€¼
                        interp_ratio = i / (target_frames - 1)
                        interp_idx = int(interp_ratio * (len(target_keyframe_indices) - 1))
                        target_keyframe_idx = target_keyframe_indices[interp_idx]
                
                if target_keyframe_idx >= len(keyframe_poses):
                    pose_vec = torch.cat([
                        torch.zeros(3, dtype=torch.float32),
                        torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32),
                        torch.tensor([1.0], dtype=torch.float32)  # target
                    ], dim=0)
                else:
                    target_pose = keyframe_poses[target_keyframe_idx]
                    
                    # è®¡ç®—ç›¸å¯¹pose
                    relative_translation = torch.tensor(
                        np.array(target_pose['translation']) - np.array(reference_pose['translation']),
                        dtype=torch.float32
                    )
                    
                    relative_rotation = self.calculate_relative_rotation(
                        target_pose['rotation'],
                        reference_pose['rotation']
                    )
                    
                    # ğŸ”§ æ·»åŠ frame type embeddingï¼š1è¡¨ç¤ºtarget
                    pose_vec = torch.cat([
                        relative_translation, 
                        relative_rotation, 
                        torch.tensor([1.0], dtype=torch.float32)
                    ], dim=0)  # [8D]
                
                pose_vecs.append(pose_vec)
                frame_types.append('target')
        
        if not pose_vecs:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•poseå‘é‡")
            return None
        
        pose_sequence = torch.stack(pose_vecs, dim=0)  # [total_frames, 8]
        print(f"ç”Ÿæˆposeåºåˆ—å½¢çŠ¶: {pose_sequence.shape}")
        print(f"æœŸæœ›å½¢çŠ¶: [{total_frames}, 8]")
        print(f"å¸§ç±»å‹åˆ†å¸ƒ: {frame_types}")
        
        # ğŸ”§ åªå¯¹targetéƒ¨åˆ†è¿›è¡Œåˆ†ç±»åˆ†æï¼ˆconditionéƒ¨åˆ†ä¸éœ€è¦åˆ†ç±»ï¼‰
        target_pose_sequence = pose_sequence[condition_frames:, :7]  # åªå–targetéƒ¨åˆ†çš„å‰7ç»´
        
        if target_pose_sequence.numel() == 0:
            print("âŒ Target poseåºåˆ—ä¸ºç©º")
            return None
        
        # ä½¿ç”¨åˆ†ç±»å™¨åˆ†ætargetéƒ¨åˆ†
        pose_analysis = self.pose_classifier.analyze_pose_sequence(target_pose_sequence)
        
        # è¿‡æ»¤æ‰backwardæ ·æœ¬
        class_distribution = pose_analysis['class_distribution']
        # if 'backward' in class_distribution and class_distribution['backward'] > 0:
        #     print(f"âš ï¸  æ£€æµ‹åˆ°backwardè¿åŠ¨ï¼Œè·³è¿‡æ ·æœ¬")
        #     return None
        
        # ğŸ”§ åˆ›å»ºå®Œæ•´çš„ç±»åˆ«embeddingï¼ˆåŒ…å«conditionå’Œtargetï¼‰
        # conditionå¸§çš„ç±»åˆ«æ ‡ç­¾è®¾ä¸ºforwardï¼ˆæˆ–è€…å¯ä»¥è®¾ä¸ºç‰¹æ®Šçš„"condition"ç±»åˆ«ï¼‰
        condition_classes = torch.full((condition_frames,), 0, dtype=torch.long)  # 0è¡¨ç¤ºforward/condition
        target_classes = pose_analysis['classifications']
        
        # æ‹¼æ¥conditionå’Œtargetçš„ç±»åˆ«
        full_classes = torch.cat([condition_classes, target_classes], dim=0)
        
        # ğŸ”§ åˆ›å»ºenhanced class embeddingï¼ŒåŒ…å«frame typeä¿¡æ¯
        class_embeddings = self.create_enhanced_class_embedding(
            full_classes, pose_sequence, embed_dim=512
        )
        
        print(f"æœ€ç»ˆclass embeddingå½¢çŠ¶: {class_embeddings.shape}")
        print(f"æœŸæœ›å½¢çŠ¶: [{total_frames}, 512]")
        
        # ğŸ”§ éªŒè¯embeddingå½¢çŠ¶æ˜¯å¦æ­£ç¡®
        if class_embeddings.shape[0] != total_frames:
            print(f"âŒ Embeddingå¸§æ•°ä¸åŒ¹é…: {class_embeddings.shape[0]} != {total_frames}")
            return None
        
        return {
            'raw_poses': pose_sequence,           # [total_frames, 8] åŒ…å«conditionå’Œtargetçš„å®é™…pose + frame type
            'pose_classes': full_classes,         # [total_frames] åŒ…å«conditionå’Œtargetçš„ç±»åˆ«
            'class_embeddings': class_embeddings, # [total_frames, 512] å¢å¼ºçš„embedding
            'pose_analysis': pose_analysis,       # åªåŒ…å«targetéƒ¨åˆ†çš„åˆ†æ
            'condition_frames': condition_frames,
            'target_frames': target_frames,
            'frame_types': frame_types
        }

    def create_enhanced_class_embedding(self, class_labels: torch.Tensor, pose_sequence: torch.Tensor, embed_dim: int = 512) -> torch.Tensor:
        """
        åˆ›å»ºå¢å¼ºçš„ç±»åˆ«embeddingï¼ŒåŒ…å«frame typeå’Œposeä¿¡æ¯
        Args:
            class_labels: [num_frames] ç±»åˆ«æ ‡ç­¾
            pose_sequence: [num_frames, 8] poseåºåˆ—ï¼Œæœ€åä¸€ç»´æ˜¯frame type
            embed_dim: embeddingç»´åº¦
        Returns:
            embeddings: [num_frames, embed_dim]
        """
        num_classes = 4
        num_frames = len(class_labels)
        
        # åŸºç¡€çš„æ–¹å‘embedding
        direction_vectors = torch.tensor([
            [1.0, 0.0, 0.0, 0.0],  # forward: ä¸»è¦xåˆ†é‡
            [-1.0, 0.0, 0.0, 0.0], # backward: è´Ÿxåˆ†é‡  
            [0.0, 1.0, 0.0, 0.0],  # left_turn: ä¸»è¦yåˆ†é‡
            [0.0, -1.0, 0.0, 0.0], # right_turn: è´Ÿyåˆ†é‡
        ], dtype=torch.float32)
        
        # One-hotç¼–ç 
        one_hot = torch.zeros(num_frames, num_classes)
        one_hot.scatter_(1, class_labels.unsqueeze(1), 1)
        
        # åŸºäºæ–¹å‘å‘é‡çš„åŸºç¡€embedding
        base_embeddings = one_hot @ direction_vectors  # [num_frames, 4]
        
        # ğŸ”§ æ·»åŠ frame typeä¿¡æ¯
        frame_types = pose_sequence[:, -1]  # æœ€åä¸€ç»´æ˜¯frame type
        frame_type_embeddings = torch.zeros(num_frames, 2)
        frame_type_embeddings[:, 0] = (frame_types == 0).float()  # condition
        frame_type_embeddings[:, 1] = (frame_types == 1).float()  # target
        
        # ğŸ”§ æ·»åŠ poseçš„å‡ ä½•ä¿¡æ¯
        translations = pose_sequence[:, :3]  # [num_frames, 3]
        rotations = pose_sequence[:, 3:7]    # [num_frames, 4]
        
        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        combined_features = torch.cat([
            base_embeddings,         # [num_frames, 4] æ–¹å‘ç‰¹å¾
            frame_type_embeddings,   # [num_frames, 2] å¸§ç±»å‹ç‰¹å¾
            translations,            # [num_frames, 3] ä½ç§»ç‰¹å¾
            rotations,               # [num_frames, 4] æ—‹è½¬ç‰¹å¾
        ], dim=1)  # [num_frames, 13]
        
        # æ‰©å±•åˆ°ç›®æ ‡ç»´åº¦
        if embed_dim > 13:
            # ä½¿ç”¨çº¿æ€§å˜æ¢æ‰©å±•
            expand_matrix = torch.randn(13, embed_dim) * 0.1
            # ä¿æŒé‡è¦ç‰¹å¾
            expand_matrix[:13, :13] = torch.eye(13)
            embeddings = combined_features @ expand_matrix
        else:
            embeddings = combined_features[:, :embed_dim]
        
        return embeddings

    def __getitem__(self, index):
        while True:
            try:
                # éšæœºé€‰æ‹©ä¸€ä¸ªåœºæ™¯
                scene_dir = random.choice(self.scene_dirs)
                
                # åŠ è½½åœºæ™¯ä¿¡æ¯
                with open(os.path.join(scene_dir, "scene_info.json"), 'r') as f:
                    scene_info = json.load(f)
                
                # åŠ è½½ç¼–ç çš„è§†é¢‘æ•°æ®
                encoded_data = torch.load(
                    os.path.join(scene_dir, "encoded_video-480p.pth"),
                    weights_only=True,
                    map_location="cpu"
                )
                
                # ğŸ”§ éªŒè¯latentå¸§æ•°æ˜¯å¦ç¬¦åˆé¢„æœŸ
                full_latents = encoded_data['latents']  # [C, T, H, W]
                expected_latent_frames = scene_info['total_frames'] // self.time_compression_ratio
                actual_latent_frames = full_latents.shape[1]
                
                # print(f"åœºæ™¯ {os.path.basename(scene_dir)}: åŸå§‹å¸§æ•°={scene_info['total_frames']}, "
                #       f"é¢„æœŸlatentå¸§æ•°={expected_latent_frames}, å®é™…latentå¸§æ•°={actual_latent_frames}")
                
                if abs(actual_latent_frames - expected_latent_frames) > 2:  # å…è®¸å°çš„èˆå…¥è¯¯å·®
                    print(f"âš ï¸  Latentå¸§æ•°ä¸åŒ¹é…ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                    continue
                
                # åŠ¨æ€é€‰æ‹©æ®µè½
                segment_info = self.select_dynamic_segment(scene_info)
                if segment_info is None:
                    continue
                
                # åˆ›å»ºpose embeddings
                pose_data = self.create_pose_embeddings(scene_info, segment_info)
                if pose_data is None:
                    continue
                
                # ğŸ”§ ä½¿ç”¨å‹ç¼©åçš„ç´¢å¼•æå–latentæ®µè½
                start_frame = segment_info['start_frame']  # å·²ç»æ˜¯å‹ç¼©åçš„ç´¢å¼•
                condition_frames = segment_info['condition_frames']  # å·²ç»æ˜¯å‹ç¼©åçš„å¸§æ•°
                target_frames = segment_info['target_frames']  # å·²ç»æ˜¯å‹ç¼©åçš„å¸§æ•°
                
                # print(f"æå–latentæ®µè½: start={start_frame}, condition={condition_frames}, target={target_frames}")
                # print(f"Full latents shape: {full_latents.shape}")
                
                # ç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
                if start_frame + condition_frames + target_frames > full_latents.shape[1]:
                    print(f"ç´¢å¼•è¶Šç•Œï¼Œè·³è¿‡: {start_frame + condition_frames + target_frames} > {full_latents.shape[1]}")
                    continue
                
                condition_latents = full_latents[:, start_frame:start_frame+condition_frames, :, :]
                
                target_latents = full_latents[:, start_frame+condition_frames:start_frame+condition_frames+target_frames, :, :]
                
                # print(f"Condition latents shape: {condition_latents.shape}")
                # print(f"Target latents shape: {target_latents.shape}")
                
                # æ‹¼æ¥latents [condition, target]
                combined_latents = torch.cat([condition_latents, target_latents], dim=1)
                
                result = {
                    "latents": combined_latents,
                    "prompt_emb": encoded_data["prompt_emb"],
                    "image_emb": encoded_data.get("image_emb", {}),
                    "camera": pose_data['class_embeddings'].to(torch.bfloat16),
                    "pose_classes": pose_data['pose_classes'],
                    "raw_poses": pose_data['raw_poses'],
                    "pose_analysis": pose_data['pose_analysis'],
                    "condition_frames": condition_frames,  # å‹ç¼©åçš„å¸§æ•°
                    "target_frames": target_frames,  # å‹ç¼©åçš„å¸§æ•°
                    "scene_name": os.path.basename(scene_dir),
                    # ğŸ”§ æ–°å¢ï¼šè®°å½•åŸå§‹å¸§æ•°ç”¨äºè°ƒè¯•
                    "original_condition_frames": segment_info['original_condition_frames'],
                    "original_target_frames": segment_info['original_target_frames'],
                }
                
                return result
                
            except Exception as e:
                print(f"Error loading sample: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    def __len__(self):
        return self.steps_per_epoch
    
class DynamicLightningModelForTrain(pl.LightningModule):
    def __init__(
        self,
        dit_path,
        learning_rate=1e-5,
        use_gradient_checkpointing=True,
        use_gradient_checkpointing_offload=False,
        resume_ckpt_path=None
    ):
        super().__init__()
        model_manager = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        if os.path.isfile(dit_path):
            model_manager.load_models([dit_path])
        else:
            dit_path = dit_path.split(",")
            model_manager.load_models([dit_path])
        model_manager.load_models(["models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth"])
        
        self.pipe = WanVideoReCamMasterPipeline.from_model_manager(model_manager)
        self.pipe.scheduler.set_timesteps(1000, training=True)

        # æ·»åŠ ç›¸æœºç¼–ç å™¨
        dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
        for block in self.pipe.dit.blocks:
            block.cam_encoder = nn.Linear(512, dim)
            block.projector = nn.Linear(dim, dim)
            block.cam_encoder.weight.data.zero_()
            block.cam_encoder.bias.data.zero_()
            block.projector.weight = nn.Parameter(torch.eye(dim))
            block.projector.bias = nn.Parameter(torch.zeros(dim))
        
        if resume_ckpt_path is not None:
            state_dict = torch.load(resume_ckpt_path, map_location="cpu")
            self.pipe.dit.load_state_dict(state_dict, strict=True)

        self.freeze_parameters()
        
        # åªè®­ç»ƒç›¸æœºç›¸å…³å’Œæ³¨æ„åŠ›æ¨¡å—
        for name, module in self.pipe.denoising_model().named_modules():
            if any(keyword in name for keyword in ["cam_encoder", "projector", "self_attn"]):
                for param in module.parameters():
                    param.requires_grad = True
        
        self.learning_rate = learning_rate
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.use_gradient_checkpointing_offload = use_gradient_checkpointing_offload
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        self.vis_dir = "nus/visualizations_dynamic"
        os.makedirs(self.vis_dir, exist_ok=True)
        
    def freeze_parameters(self):
        self.pipe.requires_grad_(False)
        self.pipe.eval()
        self.pipe.denoising_model().train()

    def training_step(self, batch, batch_idx):
        # è·å–åŠ¨æ€é•¿åº¦ä¿¡æ¯ï¼ˆè¿™äº›å·²ç»æ˜¯å‹ç¼©åçš„å¸§æ•°ï¼‰
        condition_frames = batch["condition_frames"][0].item()  # å‹ç¼©åçš„conditioné•¿åº¦
        target_frames = batch["target_frames"][0].item()       # å‹ç¼©åçš„targeté•¿åº¦
        
        # ğŸ”§ è·å–åŸå§‹å¸§æ•°ç”¨äºæ—¥å¿—è®°å½•
        original_condition_frames = batch.get("original_condition_frames", [condition_frames * 4])[0]
        original_target_frames = batch.get("original_target_frames", [target_frames * 4])[0]
        
        # Data
        latents = batch["latents"].to(self.device)
        # print(f"å‹ç¼©åconditionå¸§æ•°: {condition_frames}, targetå¸§æ•°: {target_frames}")
        # print(f"åŸå§‹conditionå¸§æ•°: {original_condition_frames}, targetå¸§æ•°: {original_target_frames}")
        # print(f"Latents shape: {latents.shape}")
        
        # è£å‰ªç©ºé—´å°ºå¯¸ä»¥èŠ‚çœå†…å­˜
        # target_height, target_width = 50, 70
        # current_height, current_width = latents.shape[3], latents.shape[4]
        
        # if current_height > target_height or current_width > target_width:
        #     h_start = (current_height - target_height) // 2
        #     w_start = (current_width - target_width) // 2
        #     latents = latents[:, :, :, 
        #                     h_start:h_start+target_height, 
        #                     w_start:w_start+target_width]
        
        prompt_emb = batch["prompt_emb"]
        prompt_emb["context"] = prompt_emb["context"][0].to(self.device)
        image_emb = batch["image_emb"]
        # print(f"è£å‰ªålatents shape: {latents.shape}")

        if "clip_feature" in image_emb:
            image_emb["clip_feature"] = image_emb["clip_feature"][0].to(self.device)
        if "y" in image_emb:
            image_emb["y"] = image_emb["y"][0].to(self.device)
        
        cam_emb = batch["camera"].to(self.device)

        # Lossè®¡ç®—
        self.pipe.device = self.device
        noise = torch.randn_like(latents)
        timestep_id = torch.randint(0, self.pipe.scheduler.num_train_timesteps, (1,))
        timestep = self.pipe.scheduler.timesteps[timestep_id].to(dtype=self.pipe.torch_dtype, device=self.pipe.device)
        
        extra_input = self.pipe.prepare_extra_input(latents)
        origin_latents = copy.deepcopy(latents)
        noisy_latents = self.pipe.scheduler.add_noise(latents, noise, timestep)
        
        # ğŸ”§ å…³é”®ï¼šä½¿ç”¨å‹ç¼©åçš„conditioné•¿åº¦
        # conditionéƒ¨åˆ†ä¿æŒcleanï¼Œåªå¯¹targetéƒ¨åˆ†åŠ å™ª
        noisy_latents[:, :, :condition_frames, ...] = origin_latents[:, :, :condition_frames, ...]
        training_target = self.pipe.scheduler.training_target(latents, noise, timestep)
        # print(f"targeå°ºå¯¸: {training_target.shape}")
        # é¢„æµ‹å™ªå£°
        noise_pred = self.pipe.denoising_model()(
            noisy_latents, timestep=timestep, cam_emb=cam_emb, **prompt_emb, **extra_input, **image_emb,
            use_gradient_checkpointing=self.use_gradient_checkpointing,
            use_gradient_checkpointing_offload=self.use_gradient_checkpointing_offload
        )
        # print(f"predå°ºå¯¸: {training_target.shape}")
        # ğŸ”§ åªå¯¹targetéƒ¨åˆ†è®¡ç®—lossï¼ˆä½¿ç”¨å‹ç¼©åçš„ç´¢å¼•ï¼‰
        target_noise_pred = noise_pred[:, :, condition_frames:condition_frames+target_frames, ...]
        target_training_target = training_target[:, :, condition_frames:condition_frames+target_frames, ...]
        
        loss = torch.nn.functional.mse_loss(target_noise_pred.float(), target_training_target.float())
        loss = loss * self.pipe.scheduler.training_weight(timestep)
        print('--------loss------------:',loss)

        # è®°å½•é¢å¤–ä¿¡æ¯
        wandb.log({
            "train_loss": loss.item(),
            "timestep": timestep.item(),
            "condition_frames_compressed": condition_frames,  # å‹ç¼©åçš„å¸§æ•°000
            "target_frames_compressed": target_frames,
            "condition_frames_original": original_condition_frames,  # åŸå§‹å¸§æ•°
            "target_frames_original": original_target_frames,
            "total_frames_compressed": condition_frames + target_frames,
            "total_frames_original": original_condition_frames + original_target_frames,
            "global_step": self.global_step
        })

        return loss

    def configure_optimizers(self):
        trainable_modules = filter(lambda p: p.requires_grad, self.pipe.denoising_model().parameters())
        optimizer = torch.optim.AdamW(trainable_modules, lr=self.learning_rate)
        return optimizer
    
    def on_save_checkpoint(self, checkpoint):
        checkpoint_dir = "/home/zhuyixuan05/ReCamMaster/nus_dynamic"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        current_step = self.global_step
        checkpoint.clear()
        
        state_dict = self.pipe.denoising_model().state_dict()
        torch.save(state_dict, os.path.join(checkpoint_dir, f"step{current_step}_dynamic.ckpt"))
        print(f"Saved dynamic model checkpoint: step{current_step}_dynamic.ckpt")

def train_dynamic(args):
    """è®­ç»ƒæ”¯æŒåŠ¨æ€å†å²é•¿åº¦çš„æ¨¡å‹"""
    dataset = DynamicNuScenesDataset(
        args.dataset_path,
        steps_per_epoch=args.steps_per_epoch,
        min_condition_frames=args.min_condition_frames,
        max_condition_frames=args.max_condition_frames,
        target_frames=args.target_frames,
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle=True,
        batch_size=1,
        num_workers=args.dataloader_num_workers
    )
    
    model = DynamicLightningModelForTrain(
        dit_path=args.dit_path,
        learning_rate=args.learning_rate,
        use_gradient_checkpointing=args.use_gradient_checkpointing,
        use_gradient_checkpointing_offload=args.use_gradient_checkpointing_offload,
        resume_ckpt_path=args.resume_ckpt_path,
    )

    wandb.init(
        project="nuscenes-dynamic-recam",
        name=f"dynamic-{args.min_condition_frames}-{args.max_condition_frames}",
    )

    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        accelerator="gpu",
        devices="auto",
        precision="bf16",
        strategy=args.training_strategy,
        default_root_dir=args.output_path,
        accumulate_grad_batches=args.accumulate_grad_batches,
        callbacks=[],
    )
    trainer.fit(model, dataloader)

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description="Train Dynamic ReCamMaster")
    parser.add_argument("--dataset_path", type=str, default="/share_zhuyixuan05/zhuyixuan05/nuscenes_video_generation_dynamic")
    parser.add_argument("--dit_path", type=str, default="models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors")
    parser.add_argument("--output_path", type=str, default="./")
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    parser.add_argument("--steps_per_epoch", type=int, default=3000)
    parser.add_argument("--max_epochs", type=int, default=10)
    parser.add_argument("--min_condition_frames", type=int, default=10, help="æœ€å°æ¡ä»¶å¸§æ•°")
    parser.add_argument("--max_condition_frames", type=int, default=40, help="æœ€å¤§æ¡ä»¶å¸§æ•°")
    parser.add_argument("--target_frames", type=int, default=32, help="ç›®æ ‡å¸§æ•°")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    parser.add_argument("--accumulate_grad_batches", type=int, default=1)
    parser.add_argument("--training_strategy", type=str, default="deepspeed_stage_1")
    parser.add_argument("--use_gradient_checkpointing", action="store_true")
    parser.add_argument("--use_gradient_checkpointing_offload", action="store_true")
    parser.add_argument("--resume_ckpt_path", type=str, default=None)
    
    args = parser.parse_args()
    
    train_dynamic(args)