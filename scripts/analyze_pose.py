import os
import json
import matplotlib.pyplot as plt
import numpy as np
from pose_classifier import PoseClassifier
import torch
from collections import defaultdict

def analyze_turning_patterns_detailed(dataset_path, num_samples=50):
    """è¯¦ç»†åˆ†æè½¬å¼¯æ¨¡å¼ï¼ŒåŸºäºç›¸å¯¹äºreferenceçš„poseå˜åŒ–"""
    classifier = PoseClassifier()
    samples_path = os.path.join(dataset_path, "samples")
    
    all_analyses = []
    sample_count = 0
    
    # ç”¨äºç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬
    class_samples = defaultdict(list)
    
    print("=== å¼€å§‹åˆ†ææ ·æœ¬ï¼ˆåŸºäºç›¸å¯¹äºreferenceçš„å˜åŒ–ï¼‰===")
    
    for item in sorted(os.listdir(samples_path)):  # æ’åºä»¥ä¾¿æœ‰åºè¾“å‡º
        if sample_count >= num_samples:
            break
            
        sample_dir = os.path.join(samples_path, item)
        if os.path.isdir(sample_dir):
            poses_path = os.path.join(sample_dir, "poses.json")
            if os.path.exists(poses_path):
                try:
                    with open(poses_path, 'r') as f:
                        poses_data = json.load(f)
                    
                    target_relative_poses = poses_data['target_relative_poses']
                    
                    if len(target_relative_poses) > 0:
                        # ğŸ”§ åˆ›å»ºç›¸å¯¹poseå‘é‡ï¼ˆå·²ç»æ˜¯ç›¸å¯¹äºreferenceçš„ï¼‰
                        pose_vecs = []
                        for pose_data in target_relative_poses:
                            # ç›¸å¯¹ä½ç§»ï¼ˆå·²ç»æ˜¯ç›¸å¯¹äºreferenceè®¡ç®—çš„ï¼‰
                            translation = torch.tensor(pose_data['relative_translation'], dtype=torch.float32)
                            
                            # ğŸ”§ ç›¸å¯¹æ—‹è½¬ï¼ˆéœ€è¦ä»currentå’Œreferenceè®¡ç®—ï¼‰
                            current_rotation = torch.tensor(pose_data['current_rotation'], dtype=torch.float32)
                            reference_rotation = torch.tensor(pose_data['reference_rotation'], dtype=torch.float32)
                            
                            # è®¡ç®—ç›¸å¯¹æ—‹è½¬ï¼šq_relative = q_ref^-1 * q_current
                            relative_rotation = calculate_relative_rotation(current_rotation, reference_rotation)
                            
                            # ç»„åˆä¸º7Då‘é‡ï¼š[relative_translation, relative_rotation]
                            pose_vec = torch.cat([translation, relative_rotation], dim=0)
                            pose_vecs.append(pose_vec)
                        
                        if pose_vecs:
                            pose_sequence = torch.stack(pose_vecs, dim=0)
                            
                            # ğŸ”§ ä½¿ç”¨æ–°çš„åˆ†ææ–¹æ³•
                            analysis = classifier.analyze_pose_sequence(pose_sequence)
                            analysis['sample_name'] = item
                            all_analyses.append(analysis)
                            
                            # ğŸ”§ è¯¦ç»†è¾“å‡ºæ¯ä¸ªæ ·æœ¬çš„åˆ†ç±»ä¿¡æ¯
                            print(f"\n--- æ ·æœ¬ {sample_count + 1}: {item} ---")
                            print(f"æ€»å¸§æ•°: {analysis['total_frames']}")
                            print(f"æ€»è·ç¦»: {analysis['total_distance']:.4f}")
                            
                            # åˆ†ç±»åˆ†å¸ƒ
                            class_dist = analysis['class_distribution']
                            print(f"åˆ†ç±»åˆ†å¸ƒ:")
                            for class_name, count in class_dist.items():
                                percentage = count / analysis['total_frames'] * 100
                                print(f"  {class_name}: {count} å¸§ ({percentage:.1f}%)")
                            
                            # ğŸ”§ è°ƒè¯•å‰å‡ ä¸ªposeçš„åˆ†ç±»è¿‡ç¨‹
                            print(f"å‰3å¸§çš„è¯¦ç»†åˆ†ç±»è¿‡ç¨‹:")
                            for i in range(min(3, len(pose_vecs))):
                                debug_info = classifier.debug_single_pose(
                                    pose_vecs[i][:3], pose_vecs[i][3:7]
                                )
                                print(f"  å¸§{i}: {debug_info['classification']} "
                                      f"(yaw: {debug_info['yaw_angle_deg']:.2f}Â°, "
                                      f"forward: {debug_info['forward_movement']:.3f})")
                            
                            # è¿åŠ¨æ®µè½
                            print(f"è¿åŠ¨æ®µè½:")
                            for i, segment in enumerate(analysis['motion_segments']):
                                print(f"  æ®µè½{i+1}: {segment['class']} (å¸§ {segment['start_frame']}-{segment['end_frame']}, æŒç»­ {segment['duration']} å¸§)")
                            
                            # ğŸ”§ ç¡®å®šä¸»è¦è¿åŠ¨ç±»å‹
                            dominant_class = max(class_dist.items(), key=lambda x: x[1])
                            dominant_class_name = dominant_class[0]
                            dominant_percentage = dominant_class[1] / analysis['total_frames'] * 100
                            
                            print(f"ä¸»è¦è¿åŠ¨ç±»å‹: {dominant_class_name} ({dominant_percentage:.1f}%)")
                            
                            # å°†æ ·æœ¬æ·»åŠ åˆ°å¯¹åº”ç±»åˆ«
                            class_samples[dominant_class_name].append({
                                'name': item,
                                'percentage': dominant_percentage,
                                'analysis': analysis
                            })
                            
                            sample_count += 1
                            
                except Exception as e:
                    print(f"âŒ å¤„ç†æ ·æœ¬ {item} æ—¶å‡ºé”™: {e}")
    
    print("\n" + "="*60)
    print("=== æŒ‰ç±»åˆ«åˆ†ç»„çš„æ ·æœ¬ç»Ÿè®¡ï¼ˆåŸºäºç›¸å¯¹äºreferenceçš„å˜åŒ–ï¼‰===")
    
    # ğŸ”§ æŒ‰ç±»åˆ«è¾“å‡ºæ ·æœ¬åˆ—è¡¨
    for class_name in ['forward', 'backward', 'left_turn', 'right_turn']:
        samples = class_samples[class_name]
        print(f"\nğŸ”¸ {class_name.upper()} ç±»æ ·æœ¬ (å…± {len(samples)} ä¸ª):")
        
        if samples:
            # æŒ‰ä¸»è¦ç±»åˆ«å æ¯”æ’åº
            samples.sort(key=lambda x: x['percentage'], reverse=True)
            
            for i, sample_info in enumerate(samples, 1):
                print(f"  {i:2d}. {sample_info['name']} ({sample_info['percentage']:.1f}%)")
                
                # æ˜¾ç¤ºè¯¦ç»†çš„æ®µè½ä¿¡æ¯
                segments = sample_info['analysis']['motion_segments']
                segment_summary = []
                for seg in segments:
                    if seg['duration'] >= 2:  # åªæ˜¾ç¤ºæŒç»­æ—¶é—´>=2å¸§çš„æ®µè½
                        segment_summary.append(f"{seg['class']}({seg['duration']})")
                
                if segment_summary:
                    print(f"      æ®µè½: {' -> '.join(segment_summary)}")
        else:
            print("  (æ— æ ·æœ¬)")
    
    # ğŸ”§ ç»Ÿè®¡æ€»ä½“æ¨¡å¼
    print(f"\n" + "="*60)
    print("=== æ€»ä½“ç»Ÿè®¡ ===")
    
    total_forward = sum(a['class_distribution']['forward'] for a in all_analyses)
    total_backward = sum(a['class_distribution']['backward'] for a in all_analyses)
    total_left_turn = sum(a['class_distribution']['left_turn'] for a in all_analyses)
    total_right_turn = sum(a['class_distribution']['right_turn'] for a in all_analyses)
    total_frames = total_forward + total_backward + total_left_turn + total_right_turn
    
    print(f"æ€»æ ·æœ¬æ•°: {len(all_analyses)}")
    print(f"æ€»å¸§æ•°: {total_frames}")
    print(f"Forward: {total_forward} å¸§ ({total_forward/total_frames*100:.1f}%)")
    print(f"Backward: {total_backward} å¸§ ({total_backward/total_frames*100:.1f}%)")
    print(f"Left Turn: {total_left_turn} å¸§ ({total_left_turn/total_frames*100:.1f}%)")
    print(f"Right Turn: {total_right_turn} å¸§ ({total_right_turn/total_frames*100:.1f}%)")
    
    # ğŸ”§ æ ·æœ¬åˆ†å¸ƒç»Ÿè®¡
    print(f"\næŒ‰ä¸»è¦ç±»å‹çš„æ ·æœ¬åˆ†å¸ƒ:")
    for class_name in ['forward', 'backward', 'left_turn', 'right_turn']:
        count = len(class_samples[class_name])
        percentage = count / len(all_analyses) * 100 if all_analyses else 0
        print(f"  {class_name}: {count} æ ·æœ¬ ({percentage:.1f}%)")
    
    return all_analyses, class_samples

def calculate_relative_rotation(current_rotation, reference_rotation):
    """è®¡ç®—ç›¸å¯¹æ—‹è½¬å››å…ƒæ•°"""
    q_current = torch.tensor(current_rotation, dtype=torch.float32)
    q_ref = torch.tensor(reference_rotation, dtype=torch.float32)

    # è®¡ç®—å‚è€ƒæ—‹è½¬çš„é€† (q_ref^-1)
    q_ref_inv = torch.tensor([q_ref[0], -q_ref[1], -q_ref[2], -q_ref[3]])

    # å››å…ƒæ•°ä¹˜æ³•è®¡ç®—ç›¸å¯¹æ—‹è½¬: q_relative = q_ref^-1 * q_current
    w1, x1, y1, z1 = q_ref_inv
    w2, x2, y2, z2 = q_current

    relative_rotation = torch.tensor([
        w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2,
        w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2,
        w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2,
        w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2
    ])

    return relative_rotation

if __name__ == "__main__":
    dataset_path = "/share_zhuyixuan05/zhuyixuan05/nuscenes_video_generation_2"
    
    print("å¼€å§‹è¯¦ç»†åˆ†æposeåˆ†ç±»ï¼ˆåŸºäºç›¸å¯¹äºreferenceçš„å˜åŒ–ï¼‰...")
    all_analyses, class_samples = analyze_turning_patterns_detailed(dataset_path, num_samples=4000)
    
    print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±å¤„ç† {len(all_analyses)} ä¸ªæ ·æœ¬")