import os
import torch
from tqdm import tqdm

def analyze_openx_dataset_frame_counts(dataset_path):
    """åˆ†æOpenXæ•°æ®é›†ä¸­çš„å¸§æ•°åˆ†å¸ƒ"""
    
    print(f"ğŸ”§ åˆ†æOpenXæ•°æ®é›†: {dataset_path}")
    
    if not os.path.exists(dataset_path):
        print(f"  âš ï¸ è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        return
    
    episode_dirs = []
    total_episodes = 0
    valid_episodes = 0
    
    # æ”¶é›†æ‰€æœ‰episodeç›®å½•
    for item in os.listdir(dataset_path):
        episode_dir = os.path.join(dataset_path, item)
        if os.path.isdir(episode_dir):
            total_episodes += 1
            encoded_path = os.path.join(episode_dir, "encoded_video.pth")
            if os.path.exists(encoded_path):
                episode_dirs.append(episode_dir)
                valid_episodes += 1
    
    print(f"ğŸ“Š æ€»episodeæ•°: {total_episodes}")
    print(f"ğŸ“Š æœ‰æ•ˆepisodeæ•°: {valid_episodes}")
    
    if len(episode_dirs) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„episode")
        return
    
    # ç»Ÿè®¡å¸§æ•°åˆ†å¸ƒ
    frame_counts = []
    less_than_10 = 0
    less_than_8 = 0
    less_than_5 = 0
    error_count = 0
    
    print("ğŸ”§ å¼€å§‹åˆ†æå¸§æ•°åˆ†å¸ƒ...")
    
    for episode_dir in tqdm(episode_dirs, desc="åˆ†æepisodes"):
        try:
            encoded_data = torch.load(
                os.path.join(episode_dir, "encoded_video.pth"),
                weights_only=False,
                map_location="cpu"
            )
            
            latents = encoded_data['latents']  # [C, T, H, W]
            frame_count = latents.shape[1]  # Tç»´åº¦
            frame_counts.append(frame_count)
            
            if frame_count < 10:
                less_than_10 += 1
            if frame_count < 8:
                less_than_8 += 1
            if frame_count < 5:
                less_than_5 += 1
                
        except Exception as e:
            error_count += 1
            if error_count <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                print(f"âŒ åŠ è½½episode {os.path.basename(episode_dir)} æ—¶å‡ºé”™: {e}")
    
    # ç»Ÿè®¡ç»“æœ
    total_valid = len(frame_counts)
    print(f"\nğŸ“ˆ å¸§æ•°åˆ†å¸ƒç»Ÿè®¡:")
    print(f"  æ€»æœ‰æ•ˆepisodes: {total_valid}")
    print(f"  é”™è¯¯episodes: {error_count}")
    print(f"  æœ€å°å¸§æ•°: {min(frame_counts) if frame_counts else 0}")
    print(f"  æœ€å¤§å¸§æ•°: {max(frame_counts) if frame_counts else 0}")
    print(f"  å¹³å‡å¸§æ•°: {sum(frame_counts) / len(frame_counts):.2f}" if frame_counts else 0)
    
    print(f"\nğŸ¯ å…³é”®ç»Ÿè®¡:")
    print(f"  å¸§æ•° < 5:  {less_than_5:6d} episodes ({less_than_5/total_valid*100:.2f}%)")
    print(f"  å¸§æ•° < 8:  {less_than_8:6d} episodes ({less_than_8/total_valid*100:.2f}%)")
    print(f"  å¸§æ•° < 10: {less_than_10:6d} episodes ({less_than_10/total_valid*100:.2f}%)")
    print(f"  å¸§æ•° >= 10: {total_valid-less_than_10:6d} episodes ({(total_valid-less_than_10)/total_valid*100:.2f}%)")
    
    # è¯¦ç»†åˆ†å¸ƒ
    frame_counts.sort()
    print(f"\nğŸ“Š è¯¦ç»†å¸§æ•°åˆ†å¸ƒ:")
    
    # æŒ‰èŒƒå›´ç»Ÿè®¡
    ranges = [
        (1, 4, "1-4å¸§"),
        (5, 7, "5-7å¸§"),
        (8, 9, "8-9å¸§"),
        (10, 19, "10-19å¸§"),
        (20, 49, "20-49å¸§"),
        (50, 99, "50-99å¸§"),
        (100, float('inf'), "100+å¸§")
    ]
    
    for min_f, max_f, label in ranges:
        count = sum(1 for f in frame_counts if min_f <= f <= max_f)
        percentage = count / total_valid * 100
        print(f"  {label:8s}: {count:6d} episodes ({percentage:5.2f}%)")
    
    # å»ºè®®çš„è®­ç»ƒé…ç½®
    print(f"\nğŸ’¡ è®­ç»ƒé…ç½®å»ºè®®:")
    time_compression_ratio = 4
    min_condition_compressed = 4 // time_compression_ratio  # 1å¸§
    target_frames_compressed = 32 // time_compression_ratio  # 8å¸§
    min_required_compressed = min_condition_compressed + target_frames_compressed  # 9å¸§
    
    usable_episodes = sum(1 for f in frame_counts if f >= min_required_compressed)
    usable_percentage = usable_episodes / total_valid * 100
    
    print(f"  æœ€å°æ¡ä»¶å¸§æ•°(å‹ç¼©å): {min_condition_compressed}")
    print(f"  ç›®æ ‡å¸§æ•°(å‹ç¼©å): {target_frames_compressed}")
    print(f"  æœ€å°æ‰€éœ€å¸§æ•°(å‹ç¼©å): {min_required_compressed}")
    print(f"  å¯ç”¨äºè®­ç»ƒçš„episodes: {usable_episodes} ({usable_percentage:.2f}%)")
    
    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡åˆ°æ–‡ä»¶
    output_file = os.path.join(dataset_path, "frame_count_analysis.txt")
    with open(output_file, 'w') as f:
        f.write(f"OpenX Dataset Frame Count Analysis\n")
        f.write(f"Dataset Path: {dataset_path}\n")
        f.write(f"Analysis Date: {__import__('datetime').datetime.now()}\n\n")
        
        f.write(f"Total Episodes: {total_episodes}\n")
        f.write(f"Valid Episodes: {total_valid}\n")
        f.write(f"Error Episodes: {error_count}\n\n")
        
        f.write(f"Frame Count Statistics:\n")
        f.write(f"  Min Frames: {min(frame_counts) if frame_counts else 0}\n")
        f.write(f"  Max Frames: {max(frame_counts) if frame_counts else 0}\n")
        f.write(f"  Avg Frames: {sum(frame_counts) / len(frame_counts):.2f}\n\n" if frame_counts else "  Avg Frames: 0\n\n")
        
        f.write(f"Key Statistics:\n")
        f.write(f"  < 5 frames:  {less_than_5} ({less_than_5/total_valid*100:.2f}%)\n")
        f.write(f"  < 8 frames:  {less_than_8} ({less_than_8/total_valid*100:.2f}%)\n")
        f.write(f"  < 10 frames: {less_than_10} ({less_than_10/total_valid*100:.2f}%)\n")
        f.write(f"  >= 10 frames: {total_valid-less_than_10} ({(total_valid-less_than_10)/total_valid*100:.2f}%)\n\n")
        
        f.write(f"Detailed Distribution:\n")
        for min_f, max_f, label in ranges:
            count = sum(1 for f in frame_counts if min_f <= f <= max_f)
            percentage = count / total_valid * 100
            f.write(f"  {label}: {count} ({percentage:.2f}%)\n")
        
        f.write(f"\nTraining Configuration Recommendation:\n")
        f.write(f"  Usable Episodes (>= {min_required_compressed} compressed frames): {usable_episodes} ({usable_percentage:.2f}%)\n")
        
        # å†™å…¥æ‰€æœ‰å¸§æ•°
        f.write(f"\nAll Frame Counts:\n")
        for i, count in enumerate(frame_counts):
            f.write(f"{count}")
            if (i + 1) % 20 == 0:
                f.write("\n")
            else:
                f.write(", ")
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜åˆ°: {output_file}")
    
    return {
        'total_valid': total_valid,
        'less_than_10': less_than_10,
        'less_than_8': less_than_8,
        'less_than_5': less_than_5,
        'frame_counts': frame_counts,
        'usable_episodes': usable_episodes
    }

def quick_sample_analysis(dataset_path, sample_size=1000):
    """å¿«é€Ÿé‡‡æ ·åˆ†æï¼Œç”¨äºå¤§æ•°æ®é›†çš„åˆæ­¥ä¼°è®¡"""
    
    print(f"ğŸš€ å¿«é€Ÿé‡‡æ ·åˆ†æ (æ ·æœ¬æ•°: {sample_size})")
    
    episode_dirs = []
    for item in os.listdir(dataset_path):
        episode_dir = os.path.join(dataset_path, item)
        if os.path.isdir(episode_dir):
            encoded_path = os.path.join(episode_dir, "encoded_video.pth")
            if os.path.exists(encoded_path):
                episode_dirs.append(episode_dir)
    
    if len(episode_dirs) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„episode")
        return
    
    # éšæœºé‡‡æ ·
    import random
    sample_dirs = random.sample(episode_dirs, min(sample_size, len(episode_dirs)))
    
    frame_counts = []
    less_than_10 = 0
    
    for episode_dir in tqdm(sample_dirs, desc="é‡‡æ ·åˆ†æ"):
        try:
            encoded_data = torch.load(
                os.path.join(episode_dir, "encoded_video.pth"),
                weights_only=False,
                map_location="cpu"
            )
            
            frame_count = encoded_data['latents'].shape[1]
            frame_counts.append(frame_count)
            
            if frame_count < 10:
                less_than_10 += 1
                
        except Exception as e:
            continue
    
    total_sample = len(frame_counts)
    percentage_less_than_10 = less_than_10 / total_sample * 100
    
    print(f"ğŸ“Š é‡‡æ ·ç»“æœ:")
    print(f"  é‡‡æ ·æ•°é‡: {total_sample}")
    print(f"  < 10å¸§: {less_than_10} ({percentage_less_than_10:.2f}%)")
    print(f"  >= 10å¸§: {total_sample - less_than_10} ({100 - percentage_less_than_10:.2f}%)")
    print(f"  å¹³å‡å¸§æ•°: {sum(frame_counts) / len(frame_counts):.2f}")
    
    # ä¼°ç®—å…¨æ•°æ®é›†
    total_episodes = len(episode_dirs)
    estimated_less_than_10 = int(total_episodes * percentage_less_than_10 / 100)
    
    print(f"\nğŸ”® å…¨æ•°æ®é›†ä¼°ç®—:")
    print(f"  æ€»episodes: {total_episodes}")
    print(f"  ä¼°ç®— < 10å¸§: {estimated_less_than_10} ({percentage_less_than_10:.2f}%)")
    print(f"  ä¼°ç®— >= 10å¸§: {total_episodes - estimated_less_than_10} ({100 - percentage_less_than_10:.2f}%)")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ†æOpenXæ•°æ®é›†çš„å¸§æ•°åˆ†å¸ƒ")
    parser.add_argument("--dataset_path", type=str, 
                       default="/share_zhuyixuan05/zhuyixuan05/openx-fractal-encoded",
                       help="OpenXç¼–ç æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿé‡‡æ ·åˆ†ææ¨¡å¼")
    parser.add_argument("--sample_size", type=int, default=1000, help="å¿«é€Ÿæ¨¡å¼çš„é‡‡æ ·æ•°é‡")
    
    args = parser.parse_args()
    
    if args.quick:
        quick_sample_analysis(args.dataset_path, args.sample_size)
    else:
        analyze_openx_dataset_frame_counts(args.dataset_path)