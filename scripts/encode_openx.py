import os
import torch
import lightning as pl
from PIL import Image
from diffsynth import WanVideoReCamMasterPipeline, ModelManager
import json
import imageio
from torchvision.transforms import v2
from einops import rearrange
import argparse
import numpy as np
from tqdm import tqdm

# ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¾ç½®ç¯å¢ƒå˜é‡é¿å…GCSè¿æ¥
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["TFDS_DISABLE_GCS"] = "1"

import tensorflow_datasets as tfds
import tensorflow as tf

class VideoEncoder(pl.LightningModule):
    def __init__(self, text_encoder_path, vae_path, tiled=True, tile_size=(34, 34), tile_stride=(18, 16)):
        super().__init__()
        model_manager = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        model_manager.load_models([text_encoder_path, vae_path])
        self.pipe = WanVideoReCamMasterPipeline.from_model_manager(model_manager)
        self.tiler_kwargs = {"tiled": tiled, "tile_size": tile_size, "tile_stride": tile_stride}
        
        self.frame_process = v2.Compose([
            v2.ToTensor(),
            v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
        ])
    
    def crop_and_resize(self, image, target_width=832, target_height=480):
        """è°ƒæ•´å›¾åƒå°ºå¯¸"""
        image = v2.functional.resize(
            image,
            (target_height, target_width),
            interpolation=v2.InterpolationMode.BILINEAR
        )
        return image

    def load_episode_frames(self, episode_data, max_frames=300):
        """ğŸ”§ ä»fractalæ•°æ®é›†åŠ è½½è§†é¢‘å¸§ - åŸºäºå®é™…observationå­—æ®µä¼˜åŒ–"""
        frames = []
        
        steps = episode_data['steps']
        frame_count = 0
        
        print(f"å¼€å§‹æå–å¸§ï¼Œæœ€å¤š {max_frames} å¸§...")
        
        for step_idx, step in enumerate(steps):
            if frame_count >= max_frames:
                break
            
            try:
                obs = step['observation']
                
                # ğŸ”§ åŸºäºå®é™…çš„observationå­—æ®µï¼Œä¼˜å…ˆä½¿ç”¨'image'
                img_data = None
                image_keys_to_try = [
                    'image',                 # âœ… ç¡®è®¤å­˜åœ¨çš„ä¸»è¦å›¾åƒå­—æ®µ
                    'rgb',                   # å¤‡ç”¨RGBå›¾åƒ
                    'camera_image',          # å¤‡ç”¨ç›¸æœºå›¾åƒ
                    'exterior_image_1_left', # å¯èƒ½çš„å¤–éƒ¨æ‘„åƒå¤´
                    'wrist_image',           # å¯èƒ½çš„æ‰‹è…•æ‘„åƒå¤´
                ]
                
                for img_key in image_keys_to_try:
                    if img_key in obs:
                        try:
                            img_tensor = obs[img_key]
                            img_data = img_tensor.numpy()
                            if step_idx < 3:  # åªä¸ºå‰å‡ ä¸ªæ­¥éª¤æ‰“å°
                                print(f"âœ… æ‰¾åˆ°å›¾åƒå­—æ®µ: {img_key}, å½¢çŠ¶: {img_data.shape}")
                            break
                        except Exception as e:
                            if step_idx < 3:
                                print(f"å°è¯•å­—æ®µ {img_key} å¤±è´¥: {e}")
                            continue
                
                if img_data is not None:
                    # ç¡®ä¿å›¾åƒæ•°æ®æ ¼å¼æ­£ç¡®
                    if len(img_data.shape) == 3:  # [H, W, C]
                        if img_data.dtype == np.uint8:
                            frame = Image.fromarray(img_data)
                        else:
                            # å¦‚æœæ˜¯å½’ä¸€åŒ–çš„æµ®ç‚¹æ•°ï¼Œè½¬æ¢ä¸ºuint8
                            if img_data.max() <= 1.0:
                                img_data = (img_data * 255).astype(np.uint8)
                            else:
                                img_data = img_data.astype(np.uint8)
                            frame = Image.fromarray(img_data)
                        
                        # è½¬æ¢ä¸ºRGBå¦‚æœéœ€è¦
                        if frame.mode != 'RGB':
                            frame = frame.convert('RGB')
                        
                        frame = self.crop_and_resize(frame)
                        frame = self.frame_process(frame)
                        frames.append(frame)
                        frame_count += 1
                        
                        if frame_count % 50 == 0:
                            print(f"å·²å¤„ç† {frame_count} å¸§")
                    else:
                        if step_idx < 5:
                            print(f"æ­¥éª¤ {step_idx}: å›¾åƒå½¢çŠ¶ä¸æ­£ç¡® {img_data.shape}")
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°å›¾åƒï¼Œæ‰“å°å¯ç”¨çš„è§‚æµ‹é”®
                    if step_idx < 5:  # åªä¸ºå‰å‡ ä¸ªæ­¥éª¤æ‰“å°
                        available_keys = list(obs.keys())
                        print(f"æ­¥éª¤ {step_idx}: æœªæ‰¾åˆ°å›¾åƒï¼Œå¯ç”¨é”®: {available_keys}")
                        
            except Exception as e:
                print(f"å¤„ç†æ­¥éª¤ {step_idx} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æˆåŠŸæå– {len(frames)} å¸§")
        
        if len(frames) == 0:
            return None
            
        frames = torch.stack(frames, dim=0)
        frames = rearrange(frames, "T C H W -> C T H W")
        return frames

    def extract_camera_poses(self, episode_data, num_frames):
        """ğŸ”§ ä»fractalæ•°æ®é›†æå–ç›¸æœºä½å§¿ä¿¡æ¯ - åŸºäºå®é™…observationå’Œactionå­—æ®µä¼˜åŒ–"""
        camera_poses = []
        
        steps = episode_data['steps']
        frame_count = 0
        
        print("æå–ç›¸æœºä½å§¿ä¿¡æ¯...")
        
        # ğŸ”§ ç´¯ç§¯ä½å§¿ä¿¡æ¯
        cumulative_translation = np.array([0.0, 0.0, 0.0], dtype=np.float32)
        cumulative_rotation = np.array([0.0, 0.0, 0.0], dtype=np.float32)  # æ¬§æ‹‰è§’
        
        for step_idx, step in enumerate(steps):
            if frame_count >= num_frames:
                break
                
            try:
                obs = step['observation']
                action = step.get('action', {})
                
                # ğŸ”§ åŸºäºå®é™…çš„å­—æ®µæå–ä½å§¿å˜åŒ–
                pose_data = {}
                found_pose = False
                
                # 1. ä¼˜å…ˆä½¿ç”¨actionä¸­çš„world_vectorï¼ˆä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç§»ï¼‰
                if 'world_vector' in action:
                    try:
                        world_vector = action['world_vector'].numpy()
                        if len(world_vector) == 3:
                            # ç´¯ç§¯ä¸–ç•Œåæ ‡ä½ç§»
                            cumulative_translation += world_vector
                            pose_data['translation'] = cumulative_translation.copy()
                            found_pose = True
                            
                            if step_idx < 3:
                                print(f"ä½¿ç”¨action.world_vector: {world_vector}, ç´¯ç§¯ä½ç§»: {cumulative_translation}")
                    except Exception as e:
                        if step_idx < 3:
                            print(f"action.world_vectoræå–å¤±è´¥: {e}")
                
                # 2. ä½¿ç”¨actionä¸­çš„rotation_deltaï¼ˆæ—‹è½¬å˜åŒ–ï¼‰
                if 'rotation_delta' in action:
                    try:
                        rotation_delta = action['rotation_delta'].numpy()
                        if len(rotation_delta) == 3:
                            # ç´¯ç§¯æ—‹è½¬å˜åŒ–
                            cumulative_rotation += rotation_delta
                            
                            # è½¬æ¢ä¸ºå››å…ƒæ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                            euler_angles = cumulative_rotation
                            # æ¬§æ‹‰è§’è½¬å››å…ƒæ•°ï¼ˆZYXé¡ºåºï¼‰
                            roll, pitch, yaw = euler_angles[0], euler_angles[1], euler_angles[2]
                            
                            # ç®€åŒ–çš„æ¬§æ‹‰è§’åˆ°å››å…ƒæ•°è½¬æ¢
                            cy = np.cos(yaw * 0.5)
                            sy = np.sin(yaw * 0.5)
                            cp = np.cos(pitch * 0.5)
                            sp = np.sin(pitch * 0.5)
                            cr = np.cos(roll * 0.5)
                            sr = np.sin(roll * 0.5)
                            
                            qw = cr * cp * cy + sr * sp * sy
                            qx = sr * cp * cy - cr * sp * sy
                            qy = cr * sp * cy + sr * cp * sy
                            qz = cr * cp * sy - sr * sp * cy
                            
                            pose_data['rotation'] = np.array([qw, qx, qy, qz], dtype=np.float32)
                            found_pose = True
                            
                            if step_idx < 3:
                                print(f"ä½¿ç”¨action.rotation_delta: {rotation_delta}, ç´¯ç§¯æ—‹è½¬: {cumulative_rotation}")
                    except Exception as e:
                        if step_idx < 3:
                            print(f"action.rotation_deltaæå–å¤±è´¥: {e}")
                
                # ç¡®ä¿rotationå­—æ®µå­˜åœ¨
                if 'rotation' not in pose_data:
                    # ä½¿ç”¨å½“å‰ç´¯ç§¯çš„æ—‹è½¬è®¡ç®—å››å…ƒæ•°
                    roll, pitch, yaw = cumulative_rotation[0], cumulative_rotation[1], cumulative_rotation[2]
                    
                    cy = np.cos(yaw * 0.5)
                    sy = np.sin(yaw * 0.5)
                    cp = np.cos(pitch * 0.5)
                    sp = np.sin(pitch * 0.5)
                    cr = np.cos(roll * 0.5)
                    sr = np.sin(roll * 0.5)
                    
                    qw = cr * cp * cy + sr * sp * sy
                    qx = sr * cp * cy - cr * sp * sy
                    qy = cr * sp * cy + sr * cp * sy
                    qz = cr * cp * sy - sr * sp * cy
                    
                    pose_data['rotation'] = np.array([qw, qx, qy, qz], dtype=np.float32)
                
                camera_poses.append(pose_data)
                frame_count += 1
                
            except Exception as e:
                print(f"æå–ä½å§¿æ­¥éª¤ {step_idx} æ—¶å‡ºé”™: {e}")
                # æ·»åŠ é»˜è®¤ä½å§¿
                pose_data = {
                    'translation': cumulative_translation.copy(),
                    'rotation': np.array([1.0, 0.0, 0.0, 0.0], dtype=np.float32)
                }
                camera_poses.append(pose_data)
                frame_count += 1
        
        print(f"æå–äº† {len(camera_poses)} ä¸ªä½å§¿")
        print(f"æœ€ç»ˆç´¯ç§¯ä½ç§»: {cumulative_translation}")
        print(f"æœ€ç»ˆç´¯ç§¯æ—‹è½¬: {cumulative_rotation}")
        
        return camera_poses

    def create_camera_matrices(self, camera_poses):
        """å°†ä½å§¿è½¬æ¢ä¸º4x4å˜æ¢çŸ©é˜µ"""
        matrices = []
        
        for pose in camera_poses:
            matrix = np.eye(4, dtype=np.float32)
            
            # è®¾ç½®å¹³ç§»
            matrix[:3, 3] = pose['translation']
            
            # è®¾ç½®æ—‹è½¬ - å‡è®¾æ˜¯å››å…ƒæ•° [w, x, y, z]
            if len(pose['rotation']) == 4:
                # å››å…ƒæ•°è½¬æ—‹è½¬çŸ©é˜µ
                q = pose['rotation']
                w, x, y, z = q[0], q[1], q[2], q[3]
                
                # å››å…ƒæ•°åˆ°æ—‹è½¬çŸ©é˜µçš„è½¬æ¢
                matrix[0, 0] = 1 - 2*(y*y + z*z)
                matrix[0, 1] = 2*(x*y - w*z)
                matrix[0, 2] = 2*(x*z + w*y)
                matrix[1, 0] = 2*(x*y + w*z)
                matrix[1, 1] = 1 - 2*(x*x + z*z)
                matrix[1, 2] = 2*(y*z - w*x)
                matrix[2, 0] = 2*(x*z - w*y)
                matrix[2, 1] = 2*(y*z + w*x)
                matrix[2, 2] = 1 - 2*(x*x + y*y)
            elif len(pose['rotation']) == 3:
                # æ¬§æ‹‰è§’è½¬æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
                pass
            
            matrices.append(matrix)
        
        return np.array(matrices)

def encode_fractal_dataset(dataset_path, text_encoder_path, vae_path, output_dir, max_episodes=None):
    """ğŸ”§ ç¼–ç fractal20220817_dataæ•°æ®é›† - åŸºäºå®é™…å­—æ®µç»“æ„ä¼˜åŒ–"""
    
    encoder = VideoEncoder(text_encoder_path, vae_path)
    encoder = encoder.cuda()
    encoder.pipe.device = "cuda"
    
    os.makedirs(output_dir, exist_ok=True)
    
    processed_count = 0
    prompt_emb = None
        
    try:
        # ğŸ”§ ä½¿ç”¨ä½ æä¾›çš„æˆåŠŸæ–¹æ³•åŠ è½½æ•°æ®é›†
        ds = tfds.load(
            "fractal20220817_data",
            split="train",
            data_dir=dataset_path,
        )
        
        print(f"âœ… æˆåŠŸåŠ è½½fractal20220817_dataæ•°æ®é›†")
        
        # é™åˆ¶å¤„ç†çš„episodeæ•°é‡
        if max_episodes:
            ds = ds.take(max_episodes)
            print(f"é™åˆ¶å¤„ç†episodesæ•°é‡: {max_episodes}")
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return
    
    for episode_idx, episode in enumerate(tqdm(ds, desc="å¤„ç†episodes")):
        try:
            episode_name = f"episode_{episode_idx:06d}"
            save_episode_dir = os.path.join(output_dir, episode_name)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
            encoded_path = os.path.join(save_episode_dir, "encoded_video.pth")
            if os.path.exists(encoded_path):
                print(f"Episode {episode_name} å·²å¤„ç†ï¼Œè·³è¿‡...")
                processed_count += 1
                continue
            
            os.makedirs(save_episode_dir, exist_ok=True)
            
            print(f"\nğŸ”§ å¤„ç†episode {episode_name}...")
            
            # ğŸ”§ åˆ†æepisodeç»“æ„ï¼ˆä»…å¯¹å‰å‡ ä¸ªepisodeï¼‰
            if episode_idx < 2:
                print("Episodeç»“æ„åˆ†æ:")
                for key in episode.keys():
                    print(f"  - {key}: {type(episode[key])}")
                
                # åˆ†æç¬¬ä¸€ä¸ªstepçš„ç»“æ„
                steps = episode['steps']
                for step in steps.take(1):
                    print("ç¬¬ä¸€ä¸ªstepç»“æ„:")
                    for key in step.keys():
                        print(f"    - {key}: {type(step[key])}")
                    
                    if 'observation' in step:
                        obs = step['observation']
                        print("  observationé”®:")
                        print(f"    ğŸ” å¯ç”¨å­—æ®µ: {list(obs.keys())}")
                        
                        # é‡ç‚¹æ£€æŸ¥å›¾åƒå’Œä½å§¿ç›¸å…³å­—æ®µ
                        key_fields = ['image', 'vector_to_go', 'rotation_delta_to_go', 'base_pose_tool_reached']
                        for key in key_fields:
                            if key in obs:
                                try:
                                    value = obs[key]
                                    if hasattr(value, 'shape'):
                                        print(f"      âœ… {key}: {type(value)}, shape: {value.shape}")
                                    else:
                                        print(f"      âœ… {key}: {type(value)}")
                                except Exception as e:
                                    print(f"      âŒ {key}: æ— æ³•è®¿é—® ({e})")
                    
                    if 'action' in step:
                        action = step['action']
                        print("  actioné”®:")
                        print(f"    ğŸ” å¯ç”¨å­—æ®µ: {list(action.keys())}")
                        
                        # é‡ç‚¹æ£€æŸ¥ä½å§¿ç›¸å…³å­—æ®µ
                        key_fields = ['world_vector', 'rotation_delta', 'base_displacement_vector']
                        for key in key_fields:
                            if key in action:
                                try:
                                    value = action[key]
                                    if hasattr(value, 'shape'):
                                        print(f"      âœ… {key}: {type(value)}, shape: {value.shape}")
                                    else:
                                        print(f"      âœ… {key}: {type(value)}")
                                except Exception as e:
                                    print(f"      âŒ {key}: æ— æ³•è®¿é—® ({e})")
            
            # åŠ è½½è§†é¢‘å¸§
            video_frames = encoder.load_episode_frames(episode)
            if video_frames is None:
                print(f"âŒ æ— æ³•åŠ è½½episode {episode_name}çš„è§†é¢‘å¸§")
                continue
            
            print(f"âœ… Episode {episode_name} è§†é¢‘å½¢çŠ¶: {video_frames.shape}")
            
            # æå–ç›¸æœºä½å§¿
            num_frames = video_frames.shape[1]
            camera_poses = encoder.extract_camera_poses(episode, num_frames)
            camera_matrices = encoder.create_camera_matrices(camera_poses)
            
            print(f"ğŸ”§ ç¼–ç episode {episode_name}...")
            
            # å‡†å¤‡ç›¸æœºæ•°æ®
            cam_emb = {
                'extrinsic': camera_matrices,
                'intrinsic': np.eye(3, dtype=np.float32)
            }
            
            # ç¼–ç è§†é¢‘
            frames_batch = video_frames.unsqueeze(0).to("cuda", dtype=torch.bfloat16)
            
            with torch.no_grad():
                latents = encoder.pipe.encode_video(frames_batch, **encoder.tiler_kwargs)[0]
                
                # ç¼–ç æ–‡æœ¬promptï¼ˆç¬¬ä¸€æ¬¡ï¼‰
                if prompt_emb is None:
                    print('ğŸ”§ ç¼–ç prompt...')
                    prompt_emb = encoder.pipe.encode_prompt(
                        "A video of robotic manipulation task with camera movement"
                    )
                    # é‡Šæ”¾prompterä»¥èŠ‚çœå†…å­˜
                    del encoder.pipe.prompter
                
                # ä¿å­˜ç¼–ç ç»“æœ
                encoded_data = {
                    "latents": latents.cpu(),
                    "prompt_emb": {k: v.cpu() if isinstance(v, torch.Tensor) else v 
                                 for k, v in prompt_emb.items()},
                    "cam_emb": cam_emb,
                    "episode_info": {
                        "episode_idx": episode_idx,
                        "total_frames": video_frames.shape[1],
                        "pose_extraction_method": "observation_action_based"
                    }
                }
                
                torch.save(encoded_data, encoded_path)
                print(f"âœ… ä¿å­˜ç¼–ç æ•°æ®: {encoded_path}")
            
            processed_count += 1
            print(f"âœ… å·²å¤„ç† {processed_count} ä¸ªepisodes")
            
        except Exception as e:
            print(f"âŒ å¤„ç†episode {episode_idx}æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"ğŸ‰ ç¼–ç å®Œæˆ! æ€»å…±å¤„ç†äº† {processed_count} ä¸ªepisodes")
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Encode Open-X Fractal20220817 Dataset - Based on Real Structure")
    parser.add_argument("--dataset_path", type=str, 
                       default="/share_zhuyixuan05/public_datasets/open-x/0.1.0",
                       help="Path to tensorflow_datasets directory")
    parser.add_argument("--text_encoder_path", type=str, 
                       default="models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth")
    parser.add_argument("--vae_path", type=str,
                       default="models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth")
    parser.add_argument("--output_dir", type=str,
                       default="/share_zhuyixuan05/zhuyixuan05/openx-fractal-encoded")
    parser.add_argument("--max_episodes", type=int, default=10000,
                       help="Maximum number of episodes to process (default: 10 for testing)")
    
    args = parser.parse_args()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("ğŸš€ å¼€å§‹ç¼–ç Open-X Fractalæ•°æ®é›† (åŸºäºå®é™…å­—æ®µç»“æ„)...")
    print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {args.dataset_path}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ”¢ æœ€å¤§å¤„ç†episodes: {args.max_episodes}")
    print("ğŸ”§ åŸºäºå®é™…observationå’Œactionå­—æ®µçš„ä½å§¿æå–æ–¹æ³•")
    print("âœ… ä¼˜å…ˆä½¿ç”¨ 'image' å­—æ®µè·å–å›¾åƒæ•°æ®")

    encode_fractal_dataset(
        args.dataset_path,
        args.text_encoder_path, 
        args.vae_path,
        args.output_dir,
        args.max_episodes
    )