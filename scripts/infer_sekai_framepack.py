import os
import torch
import torch.nn as nn
import numpy as np
from PIL import Image
import imageio
import json
from diffsynth import WanVideoReCamMasterPipeline, ModelManager
import argparse
from torchvision.transforms import v2
from einops import rearrange
import copy


def load_encoded_video_from_pth(pth_path, start_frame=0, num_frames=10):
    """ä»pthæ–‡ä»¶åŠ è½½é¢„ç¼–ç çš„è§†é¢‘æ•°æ®"""
    print(f"Loading encoded video from {pth_path}")
    
    encoded_data = torch.load(pth_path, weights_only=False, map_location="cpu")
    full_latents = encoded_data['latents']  # [C, T, H, W]
    
    print(f"Full latents shape: {full_latents.shape}")
    print(f"Extracting frames {start_frame} to {start_frame + num_frames}")
    
    if start_frame + num_frames > full_latents.shape[1]:
        raise ValueError(f"Not enough frames: requested {start_frame + num_frames}, available {full_latents.shape[1]}")
    
    condition_latents = full_latents[:, start_frame:start_frame + num_frames, :, :]
    print(f"Extracted condition latents shape: {condition_latents.shape}")
    
    return condition_latents, encoded_data


def compute_relative_pose(pose_a, pose_b, use_torch=False):
    """è®¡ç®—ç›¸æœºBç›¸å¯¹äºç›¸æœºAçš„ç›¸å¯¹ä½å§¿çŸ©é˜µ"""
    assert pose_a.shape == (4, 4), f"ç›¸æœºAå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_a.shape}"
    assert pose_b.shape == (4, 4), f"ç›¸æœºBå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_b.shape}"
    
    if use_torch:
        if not isinstance(pose_a, torch.Tensor):
            pose_a = torch.from_numpy(pose_a).float()
        if not isinstance(pose_b, torch.Tensor):
            pose_b = torch.from_numpy(pose_b).float()
        
        pose_a_inv = torch.inverse(pose_a)
        relative_pose = torch.matmul(pose_b, pose_a_inv)
    else:
        if not isinstance(pose_a, np.ndarray):
            pose_a = np.array(pose_a, dtype=np.float32)
        if not isinstance(pose_b, np.ndarray):
            pose_b = np.array(pose_b, dtype=np.float32)
        
        pose_a_inv = np.linalg.inv(pose_a)
        relative_pose = np.matmul(pose_b, pose_a_inv)
    
    return relative_pose

def replace_dit_model_in_manager():
    """æ›¿æ¢DiTæ¨¡å‹ç±»ä¸ºFramePackç‰ˆæœ¬"""
    from diffsynth.models.wan_video_dit_recam_future import WanModelFuture
    from diffsynth.configs.model_config import model_loader_configs
    
    for i, config in enumerate(model_loader_configs):
        keys_hash, keys_hash_with_shape, model_names, model_classes, model_resource = config
        
        if 'wan_video_dit' in model_names:
            new_model_names = []
            new_model_classes = []
            
            for name, cls in zip(model_names, model_classes):
                if name == 'wan_video_dit':
                    new_model_names.append(name)
                    new_model_classes.append(WanModelFuture)
                    print(f"âœ… æ›¿æ¢äº†æ¨¡å‹ç±»: {name} -> WanModelFuture")
                else:
                    new_model_names.append(name)
                    new_model_classes.append(cls)
            
            model_loader_configs[i] = (keys_hash, keys_hash_with_shape, new_model_names, new_model_classes, model_resource)


def add_framepack_components(dit_model):
    """æ·»åŠ FramePackç›¸å…³ç»„ä»¶"""
    if not hasattr(dit_model, 'clean_x_embedder'):
        inner_dim = dit_model.blocks[0].self_attn.q.weight.shape[0]
        
        class CleanXEmbedder(nn.Module):
            def __init__(self, inner_dim):
                super().__init__()
                self.proj = nn.Conv3d(16, inner_dim, kernel_size=(1, 2, 2), stride=(1, 2, 2))
                self.proj_2x = nn.Conv3d(16, inner_dim, kernel_size=(2, 4, 4), stride=(2, 4, 4))
                self.proj_4x = nn.Conv3d(16, inner_dim, kernel_size=(4, 8, 8), stride=(4, 8, 8))
            
            def forward(self, x, scale="1x"):
                if scale == "1x":
                    x = x.to(self.proj.weight.dtype)
                    return self.proj(x)
                elif scale == "2x":
                    x = x.to(self.proj_2x.weight.dtype)
                    return self.proj_2x(x)
                elif scale == "4x":
                    x = x.to(self.proj_4x.weight.dtype)
                    return self.proj_4x(x)
                else:
                    raise ValueError(f"Unsupported scale: {scale}")
        
        dit_model.clean_x_embedder = CleanXEmbedder(inner_dim)
        model_dtype = next(dit_model.parameters()).dtype
        dit_model.clean_x_embedder = dit_model.clean_x_embedder.to(dtype=model_dtype)
        print("âœ… æ·»åŠ äº†FramePackçš„clean_x_embedderç»„ä»¶")
        
def generate_camera_embeddings_sliding(cam_data, start_frame, current_history_length, new_frames, total_generated, use_real_poses=True):
    """ğŸ”§ ä¸ºæ»‘åŠ¨çª—å£ç”Ÿæˆcamera embeddings - ä¿®æ­£é•¿åº¦è®¡ç®—ï¼Œç¡®ä¿åŒ…å«start_latentå¸§"""
    time_compression_ratio = 4
    
    # ğŸ”§ è®¡ç®—FramePackå®é™…éœ€è¦çš„cameraå¸§æ•°
    # FramePackç»“æ„: 1(start) + 16(4x) + 2(2x) + 1(1x) + target_frames
    framepack_needed_frames = 1 + 16 + 2 + 1 + new_frames
    
    if use_real_poses and cam_data is not None and 'extrinsic' in cam_data:
        print("ğŸ”§ ä½¿ç”¨çœŸå®cameraæ•°æ®")
        cam_extrinsic = cam_data['extrinsic']
        
        # ğŸ”§ ç¡®ä¿ç”Ÿæˆè¶³å¤Ÿé•¿çš„cameraåºåˆ—
        # éœ€è¦è€ƒè™‘ï¼šå½“å‰å†å²ä½ç½® + FramePackæ‰€éœ€çš„å®Œæ•´ç»“æ„
        max_needed_frames = max(
            start_frame + current_history_length + new_frames,  # åŸºç¡€éœ€æ±‚
            framepack_needed_frames,  # FramePackç»“æ„éœ€æ±‚
            30  # æœ€å°ä¿è¯é•¿åº¦
        )
        
        print(f"ğŸ”§ è®¡ç®—cameraåºåˆ—é•¿åº¦:")
        print(f"  - åŸºç¡€éœ€æ±‚: {start_frame + current_history_length + new_frames}")
        print(f"  - FramePackéœ€æ±‚: {framepack_needed_frames}")
        print(f"  - æœ€ç»ˆç”Ÿæˆ: {max_needed_frames}")
        
        relative_poses = []
        for i in range(max_needed_frames):
            # è®¡ç®—å½“å‰å¸§åœ¨åŸå§‹åºåˆ—ä¸­çš„ä½ç½®
            frame_idx = i * time_compression_ratio
            next_frame_idx = frame_idx + time_compression_ratio
            
            if next_frame_idx < len(cam_extrinsic):
                cam_prev = cam_extrinsic[frame_idx]
                cam_next = cam_extrinsic[next_frame_idx]
                relative_pose = compute_relative_pose(cam_prev, cam_next)
                relative_poses.append(torch.as_tensor(relative_pose[:3, :]))
            else:
                # è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é›¶è¿åŠ¨
                print(f"âš ï¸ å¸§{frame_idx}è¶…å‡ºcameraæ•°æ®èŒƒå›´ï¼Œä½¿ç”¨é›¶è¿åŠ¨")
                relative_poses.append(torch.zeros(3, 4))
        
        pose_embedding = torch.stack(relative_poses, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        
        # ğŸ”§ åˆ›å»ºå¯¹åº”é•¿åº¦çš„maskåºåˆ—
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        # ä»start_frameåˆ°current_history_lengthæ ‡è®°ä¸ºcondition
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_embedding, mask], dim=1)
        print(f"ğŸ”§ çœŸå®camera embedding shape: {camera_embedding.shape} (æ€»é•¿åº¦:{max_needed_frames})")
        return camera_embedding.to(torch.bfloat16)
        
    else:
        print("ğŸ”§ ä½¿ç”¨åˆæˆcameraæ•°æ®")
        
        # ğŸ”§ ç¡®ä¿åˆæˆæ•°æ®ä¹Ÿæœ‰è¶³å¤Ÿé•¿åº¦
        max_needed_frames = max(
            start_frame + current_history_length + new_frames,
            framepack_needed_frames,
            30
        )
        
        print(f"ğŸ”§ ç”Ÿæˆåˆæˆcameraå¸§æ•°: {max_needed_frames}")
        print(f"  - FramePackéœ€æ±‚: {framepack_needed_frames}")
        
        relative_poses = []
        for i in range(max_needed_frames):
            # ğŸ”§ æŒç»­å·¦è½¬è¿åŠ¨æ¨¡å¼
            # æ¯å¸§æ—‹è½¬ä¸€ä¸ªå›ºå®šè§’åº¦ï¼ŒåŒæ—¶å‰è¿›
            yaw_per_frame = -0.05  # æ¯å¸§å·¦è½¬ï¼ˆæ­£è§’åº¦è¡¨ç¤ºå·¦è½¬ï¼‰
            forward_speed = 0.005  # æ¯å¸§å‰è¿›è·ç¦»
            
            # è®¡ç®—å½“å‰ç´¯ç§¯è§’åº¦
            current_yaw = i * yaw_per_frame
            
            # åˆ›å»ºç›¸å¯¹å˜æ¢çŸ©é˜µï¼ˆä»ç¬¬iå¸§åˆ°ç¬¬i+1å¸§çš„å˜æ¢ï¼‰
            pose = np.eye(4, dtype=np.float32)
            
            # æ—‹è½¬çŸ©é˜µï¼ˆç»•Yè½´å·¦è½¬ï¼‰
            cos_yaw = np.cos(yaw_per_frame)
            sin_yaw = np.sin(yaw_per_frame)
            
            pose[0, 0] = cos_yaw
            pose[0, 2] = sin_yaw
            pose[2, 0] = -sin_yaw
            pose[2, 2] = cos_yaw
            
            # å¹³ç§»ï¼ˆåœ¨æ—‹è½¬åçš„å±€éƒ¨åæ ‡ç³»ä¸­å‰è¿›ï¼‰
            pose[2, 3] = -forward_speed  # å±€éƒ¨Zè½´è´Ÿæ–¹å‘ï¼ˆå‰è¿›ï¼‰
            
            # å¯é€‰ï¼šæ·»åŠ è½»å¾®çš„å‘å¿ƒè¿åŠ¨ï¼Œæ¨¡æ‹Ÿåœ†å½¢è½¨è¿¹
            radius_drift = 0.002  # å‘åœ†å¿ƒçš„è½»å¾®æ¼‚ç§»
            pose[0, 3] = radius_drift  # å±€éƒ¨Xè½´è´Ÿæ–¹å‘ï¼ˆå‘å·¦ï¼‰
            
            relative_pose = pose[:3, :]
            relative_poses.append(torch.as_tensor(relative_pose))
        
        pose_embedding = torch.stack(relative_poses, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        
        # åˆ›å»ºå¯¹åº”é•¿åº¦çš„maskåºåˆ—
        mask = torch.zeros(max_needed_frames, 1, dtype=torch.float32)
        condition_end = min(start_frame + current_history_length, max_needed_frames)
        mask[start_frame:condition_end] = 1.0
        
        camera_embedding = torch.cat([pose_embedding, mask], dim=1)
        print(f"ğŸ”§ åˆæˆcamera embedding shape: {camera_embedding.shape} (æ€»é•¿åº¦:{max_needed_frames})")
        return camera_embedding.to(torch.bfloat16)

def prepare_framepack_sliding_window_with_camera(history_latents, target_frames_to_generate, camera_embedding_full, start_frame, max_history_frames=49):
    """ğŸ”§ FramePackæ»‘åŠ¨çª—å£æœºåˆ¶ - ä¿®æ­£camera maskæ›´æ–°é€»è¾‘"""
    # history_latents: [C, T, H, W] å½“å‰çš„å†å²latents
    C, T, H, W = history_latents.shape
    
    # ğŸ”§ å›ºå®šç´¢å¼•ç»“æ„ï¼ˆè¿™å†³å®šäº†éœ€è¦çš„cameraå¸§æ•°ï¼‰
    total_indices_length = 1 + 16 + 2 + 1 + target_frames_to_generate
    indices = torch.arange(0, total_indices_length)
    split_sizes = [1, 16, 2, 1, target_frames_to_generate]
    clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = \
        indices.split(split_sizes, dim=0)
    clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=0)
    
    # ğŸ”§ æ£€æŸ¥cameraé•¿åº¦æ˜¯å¦è¶³å¤Ÿ
    if camera_embedding_full.shape[0] < total_indices_length:
        shortage = total_indices_length - camera_embedding_full.shape[0]
        padding = torch.zeros(shortage, camera_embedding_full.shape[1], 
                            dtype=camera_embedding_full.dtype, device=camera_embedding_full.device)
        camera_embedding_full = torch.cat([camera_embedding_full, padding], dim=0)
    
    # ğŸ”§ ä»å®Œæ•´cameraåºåˆ—ä¸­é€‰å–å¯¹åº”éƒ¨åˆ†
    combined_camera = camera_embedding_full[:total_indices_length, :].clone()  # clone to avoid modifying original
    
    # ğŸ”§ å…³é”®ä¿®æ­£ï¼šæ ¹æ®å½“å‰history lengthé‡æ–°è®¾ç½®mask
    # combined_cameraçš„ç»“æ„å¯¹åº”: [1(start) + 16(4x) + 2(2x) + 1(1x) + target_frames]
    # å‰19å¸§å¯¹åº”clean latentsï¼Œåé¢å¯¹åº”target
    
    # æ¸…ç©ºæ‰€æœ‰maskï¼Œé‡æ–°è®¾ç½®
    combined_camera[:, -1] = 0.0  # å…ˆå…¨éƒ¨è®¾ä¸ºtarget (0)
    
    # è®¾ç½®condition maskï¼šå‰19å¸§æ ¹æ®å®é™…å†å²é•¿åº¦å†³å®š
    if T > 0:
        # æ ¹æ®clean_latentsçš„å¡«å……é€»è¾‘ï¼Œç¡®å®šå“ªäº›ä½ç½®åº”è¯¥æ˜¯condition
        available_frames = min(T, 19)
        start_pos = 19 - available_frames
        
        # å¯¹åº”çš„cameraä½ç½®ä¹Ÿåº”è¯¥æ ‡è®°ä¸ºcondition
        combined_camera[start_pos:19, -1] = 1.0  # å°†æœ‰æ•ˆçš„clean latentså¯¹åº”çš„cameraæ ‡è®°ä¸ºcondition
    
    # targetéƒ¨åˆ†ä¿æŒä¸º0ï¼ˆå·²ç»åœ¨ä¸Šé¢è®¾ç½®ï¼‰
    
    print(f"ğŸ”§ Camera maskæ›´æ–°:")
    print(f"  - å†å²å¸§æ•°: {T}")
    print(f"  - æœ‰æ•ˆconditionå¸§æ•°: {available_frames if T > 0 else 0}")
    print(f"  - Condition mask (å‰19å¸§): {combined_camera[:19, -1].cpu().tolist()}")
    print(f"  - Target mask (å{target_frames_to_generate}å¸§): {combined_camera[19:, -1].cpu().tolist()}")    
    # å…¶ä½™å¤„ç†é€»è¾‘ä¿æŒä¸å˜...
    clean_latents_combined = torch.zeros(C, 19, H, W, dtype=history_latents.dtype, device=history_latents.device)
    
    if T > 0:
        available_frames = min(T, 19)
        start_pos = 19 - available_frames
        clean_latents_combined[:, start_pos:, :, :] = history_latents[:, -available_frames:, :, :]
    
    clean_latents_4x = clean_latents_combined[:, 0:16, :, :]
    clean_latents_2x = clean_latents_combined[:, 16:18, :, :]
    clean_latents_1x = clean_latents_combined[:, 18:19, :, :]
    
    if T > 0:
        start_latent = history_latents[:, 0:1, :, :]
    else:
        start_latent = torch.zeros(C, 1, H, W, dtype=history_latents.dtype, device=history_latents.device)
    
    clean_latents = torch.cat([start_latent, clean_latents_1x], dim=1)
    
    return {
        'latent_indices': latent_indices,
        'clean_latents': clean_latents,
        'clean_latents_2x': clean_latents_2x,
        'clean_latents_4x': clean_latents_4x,
        'clean_latent_indices': clean_latent_indices,
        'clean_latent_2x_indices': clean_latent_2x_indices,
        'clean_latent_4x_indices': clean_latent_4x_indices,
        'camera_embedding': combined_camera,  # ğŸ”§ ç°åœ¨åŒ…å«æ­£ç¡®æ›´æ–°çš„mask
        'current_length': T,
        'next_length': T + target_frames_to_generate
    }

def inference_sekai_framepack_sliding_window(
    condition_pth_path,
    dit_path,
    output_path="sekai/infer_results/output_sekai_framepack_sliding.mp4",
    start_frame=0,
    initial_condition_frames=8,
    frames_per_generation=4,
    total_frames_to_generate=32,
    max_history_frames=49,
    device="cuda",
    prompt="A video of a scene shot using a pedestrian's front camera while walking",
    use_real_poses=True,
    synthetic_direction="forward",
    # ğŸ”§ æ–°å¢CFGå‚æ•°
    use_camera_cfg=True,
    camera_guidance_scale=2.0,
    text_guidance_scale=7.5
):
    """
    ğŸ”§ FramePackæ»‘åŠ¨çª—å£è§†é¢‘ç”Ÿæˆ - æ”¯æŒCamera CFG
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    print(f"ğŸ”§ FramePackæ»‘åŠ¨çª—å£ç”Ÿæˆå¼€å§‹...")
    print(f"Camera CFG: {use_camera_cfg}, Camera guidance scale: {camera_guidance_scale}")
    print(f"Text guidance scale: {text_guidance_scale}")
    print(f"åˆå§‹æ¡ä»¶å¸§: {initial_condition_frames}, æ¯æ¬¡ç”Ÿæˆ: {frames_per_generation}, æ€»ç”Ÿæˆ: {total_frames_to_generate}")
    print(f"ä½¿ç”¨çœŸå®å§¿æ€: {use_real_poses}")
    if not use_real_poses:
        print(f"åˆæˆcameraæ–¹å‘: {synthetic_direction}")
    
    # 1-3. æ¨¡å‹åˆå§‹åŒ–å’Œç»„ä»¶æ·»åŠ ï¼ˆä¿æŒä¸å˜ï¼‰
    replace_dit_model_in_manager()
    
    model_manager = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
    model_manager.load_models([
        "models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors",
        "models/Wan-AI/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth",
        "models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth",
    ])
    pipe = WanVideoReCamMasterPipeline.from_model_manager(model_manager, device="cuda")

    dim = pipe.dit.blocks[0].self_attn.q.weight.shape[0]
    for block in pipe.dit.blocks:
        block.cam_encoder = nn.Linear(13, dim)
        block.projector = nn.Linear(dim, dim)
        block.cam_encoder.weight.data.zero_()
        block.cam_encoder.bias.data.zero_()
        block.projector.weight = nn.Parameter(torch.eye(dim))
        block.projector.bias = nn.Parameter(torch.zeros(dim))
    
    add_framepack_components(pipe.dit)
    
    dit_state_dict = torch.load(dit_path, map_location="cpu")
    pipe.dit.load_state_dict(dit_state_dict, strict=True)
    pipe = pipe.to(device)
    model_dtype = next(pipe.dit.parameters()).dtype
    
    if hasattr(pipe.dit, 'clean_x_embedder'):
        pipe.dit.clean_x_embedder = pipe.dit.clean_x_embedder.to(dtype=model_dtype)
    
    pipe.scheduler.set_timesteps(50)
    
    # 4. åŠ è½½åˆå§‹æ¡ä»¶
    print("Loading initial condition frames...")
    initial_latents, encoded_data = load_encoded_video_from_pth(
        condition_pth_path, 
        start_frame=start_frame,
        num_frames=initial_condition_frames
    )
    
    # ç©ºé—´è£å‰ª
    target_height, target_width = 60, 104
    C, T, H, W = initial_latents.shape
    
    if H > target_height or W > target_width:
        h_start = (H - target_height) // 2
        w_start = (W - target_width) // 2
        initial_latents = initial_latents[:, :, h_start:h_start+target_height, w_start:w_start+target_width]
        H, W = target_height, target_width
    
    history_latents = initial_latents.to(device, dtype=model_dtype)
    
    print(f"åˆå§‹history_latents shape: {history_latents.shape}")
    
    # ç¼–ç prompt - æ”¯æŒCFG
    if text_guidance_scale > 1.0:
        # ç¼–ç positive prompt
        prompt_emb_pos = pipe.encode_prompt(prompt)
        # ç¼–ç negative prompt (ç©ºå­—ç¬¦ä¸²)
        prompt_emb_neg = pipe.encode_prompt("")
        print(f"ä½¿ç”¨Text CFGï¼Œguidance scale: {text_guidance_scale}")
    else:
        prompt_emb_pos = pipe.encode_prompt(prompt)
        prompt_emb_neg = None
        print("ä¸ä½¿ç”¨Text CFG")
    
    # é¢„ç”Ÿæˆå®Œæ•´çš„camera embeddingåºåˆ—
    camera_embedding_full = generate_camera_embeddings_sliding(
        encoded_data.get('cam_emb', None),
        0,
        max_history_frames,
        0,
        0,
        use_real_poses=use_real_poses
    ).to(device, dtype=model_dtype)
    
    print(f"å®Œæ•´cameraåºåˆ—shape: {camera_embedding_full.shape}")
    
    # ğŸ”§ ä¸ºCamera CFGåˆ›å»ºæ— æ¡ä»¶çš„camera embedding
    if use_camera_cfg:
        # åˆ›å»ºé›¶camera embeddingï¼ˆæ— æ¡ä»¶ï¼‰
        camera_embedding_uncond = torch.zeros_like(camera_embedding_full)
        print(f"åˆ›å»ºæ— æ¡ä»¶camera embeddingç”¨äºCFG")
    
    # æ»‘åŠ¨çª—å£ç”Ÿæˆå¾ªç¯
    total_generated = 0
    all_generated_frames = []
    
    while total_generated < total_frames_to_generate:
        current_generation = min(frames_per_generation, total_frames_to_generate - total_generated)
        print(f"\nğŸ”§ ç”Ÿæˆæ­¥éª¤ {total_generated // frames_per_generation + 1}")
        print(f"å½“å‰å†å²é•¿åº¦: {history_latents.shape[1]}, æœ¬æ¬¡ç”Ÿæˆ: {current_generation}")
        
        # FramePackæ•°æ®å‡†å¤‡
        framepack_data = prepare_framepack_sliding_window_with_camera(
            history_latents,
            current_generation,
            camera_embedding_full,
            start_frame,
            max_history_frames
        )
        
        # å‡†å¤‡è¾“å…¥
        clean_latents = framepack_data['clean_latents'].unsqueeze(0)
        clean_latents_2x = framepack_data['clean_latents_2x'].unsqueeze(0)
        clean_latents_4x = framepack_data['clean_latents_4x'].unsqueeze(0)
        camera_embedding = framepack_data['camera_embedding'].unsqueeze(0)
        
        # ğŸ”§ ä¸ºCFGå‡†å¤‡æ— æ¡ä»¶camera embedding
        if use_camera_cfg:
            camera_embedding_uncond_batch = camera_embedding_uncond[:camera_embedding.shape[1], :].unsqueeze(0)
        
        # ç´¢å¼•å¤„ç†
        latent_indices = framepack_data['latent_indices'].unsqueeze(0).cpu()
        clean_latent_indices = framepack_data['clean_latent_indices'].unsqueeze(0).cpu()
        clean_latent_2x_indices = framepack_data['clean_latent_2x_indices'].unsqueeze(0).cpu()
        clean_latent_4x_indices = framepack_data['clean_latent_4x_indices'].unsqueeze(0).cpu()
        
        # åˆå§‹åŒ–è¦ç”Ÿæˆçš„latents
        new_latents = torch.randn(
            1, C, current_generation, H, W,
            device=device, dtype=model_dtype
        )
        
        extra_input = pipe.prepare_extra_input(new_latents)
        
        print(f"Camera embedding shape: {camera_embedding.shape}")
        print(f"Camera maskåˆ†å¸ƒ - condition: {torch.sum(camera_embedding[0, :, -1] == 1.0).item()}, target: {torch.sum(camera_embedding[0, :, -1] == 0.0).item()}")
        
        # å»å™ªå¾ªç¯ - æ”¯æŒCFG
        timesteps = pipe.scheduler.timesteps
        
        for i, timestep in enumerate(timesteps):
            if i % 10 == 0:
                print(f"  å»å™ªæ­¥éª¤ {i+1}/{len(timesteps)}")
            
            timestep_tensor = timestep.unsqueeze(0).to(device, dtype=model_dtype)
            
            with torch.no_grad():
                # ğŸ”§ CFGæ¨ç†
                if use_camera_cfg and camera_guidance_scale > 1.0:
                    # æ¡ä»¶é¢„æµ‹ï¼ˆæœ‰cameraï¼‰
                    noise_pred_cond = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_pos,
                        **extra_input
                    )
                    
                    # æ— æ¡ä»¶é¢„æµ‹ï¼ˆæ— cameraï¼‰
                    noise_pred_uncond = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding_uncond_batch,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **(prompt_emb_neg if prompt_emb_neg else prompt_emb_pos),
                        **extra_input
                    )
                    
                    # Camera CFG
                    noise_pred = noise_pred_uncond + camera_guidance_scale * (noise_pred_cond - noise_pred_uncond)
                    
                    # å¦‚æœåŒæ—¶ä½¿ç”¨Text CFG
                    if text_guidance_scale > 1.0 and prompt_emb_neg:
                        # è¿˜éœ€è¦è®¡ç®—textæ— æ¡ä»¶é¢„æµ‹
                        noise_pred_text_uncond = pipe.dit(
                            new_latents,
                            timestep=timestep_tensor,
                            cam_emb=camera_embedding,
                            latent_indices=latent_indices,
                            clean_latents=clean_latents,
                            clean_latent_indices=clean_latent_indices,
                            clean_latents_2x=clean_latents_2x,
                            clean_latent_2x_indices=clean_latent_2x_indices,
                            clean_latents_4x=clean_latents_4x,
                            clean_latent_4x_indices=clean_latent_4x_indices,
                            **prompt_emb_neg,
                            **extra_input
                        )
                        
                        # åº”ç”¨Text CFGåˆ°å·²ç»åº”ç”¨Camera CFGçš„ç»“æœ
                        noise_pred = noise_pred_text_uncond + text_guidance_scale * (noise_pred - noise_pred_text_uncond)
                
                elif text_guidance_scale > 1.0 and prompt_emb_neg:
                    # åªä½¿ç”¨Text CFG
                    noise_pred_cond = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_pos,
                        **extra_input
                    )
                    
                    noise_pred_uncond = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_neg,
                        **extra_input
                    )
                    
                    noise_pred = noise_pred_uncond + text_guidance_scale * (noise_pred_cond - noise_pred_uncond)
                
                else:
                    # æ ‡å‡†æ¨ç†ï¼ˆæ— CFGï¼‰
                    noise_pred = pipe.dit(
                        new_latents,
                        timestep=timestep_tensor,
                        cam_emb=camera_embedding,
                        latent_indices=latent_indices,
                        clean_latents=clean_latents,
                        clean_latent_indices=clean_latent_indices,
                        clean_latents_2x=clean_latents_2x,
                        clean_latent_2x_indices=clean_latent_2x_indices,
                        clean_latents_4x=clean_latents_4x,
                        clean_latent_4x_indices=clean_latent_4x_indices,
                        **prompt_emb_pos,
                        **extra_input
                    )
            
            new_latents = pipe.scheduler.step(noise_pred, timestep, new_latents)
        
        # æ›´æ–°å†å²
        new_latents_squeezed = new_latents.squeeze(0)
        history_latents = torch.cat([history_latents, new_latents_squeezed], dim=1)
        
        # ç»´æŠ¤æ»‘åŠ¨çª—å£
        if history_latents.shape[1] > max_history_frames:
            first_frame = history_latents[:, 0:1, :, :]
            recent_frames = history_latents[:, -(max_history_frames-1):, :, :]
            history_latents = torch.cat([first_frame, recent_frames], dim=1)
            print(f"å†å²çª—å£å·²æ»¡ï¼Œä¿ç•™ç¬¬ä¸€å¸§+æœ€æ–°{max_history_frames-1}å¸§")
        
        print(f"æ›´æ–°åhistory_latents shape: {history_latents.shape}")
        
        all_generated_frames.append(new_latents_squeezed)
        total_generated += current_generation
        
        print(f"âœ… å·²ç”Ÿæˆ {total_generated}/{total_frames_to_generate} å¸§")
    
    # 7. è§£ç å’Œä¿å­˜
    print("\nğŸ”§ è§£ç ç”Ÿæˆçš„è§†é¢‘...")
    
    all_generated = torch.cat(all_generated_frames, dim=1)
    final_video = torch.cat([initial_latents.to(all_generated.device), all_generated], dim=1).unsqueeze(0)
    
    print(f"æœ€ç»ˆè§†é¢‘shape: {final_video.shape}")
    
    decoded_video = pipe.decode_video(final_video, tiled=True, tile_size=(34, 34), tile_stride=(18, 16))
    
    print(f"Saving video to {output_path}")
    
    video_np = decoded_video[0].to(torch.float32).permute(1, 2, 3, 0).cpu().numpy()
    video_np = (video_np * 0.5 + 0.5).clip(0, 1)
    video_np = (video_np * 255).astype(np.uint8)

    with imageio.get_writer(output_path, fps=20) as writer:
        for frame in video_np:
            writer.append_data(frame)

    print(f"ğŸ”§ FramePackæ»‘åŠ¨çª—å£ç”Ÿæˆå®Œæˆ! ä¿å­˜åˆ°: {output_path}")
    print(f"æ€»å…±ç”Ÿæˆäº† {total_generated} å¸§ (å‹ç¼©å), å¯¹åº”åŸå§‹ {total_generated * 4} å¸§")
    
def main():
    parser = argparse.ArgumentParser(description="Sekai FramePackæ»‘åŠ¨çª—å£è§†é¢‘ç”Ÿæˆ - æ”¯æŒCFG")
    parser.add_argument("--condition_pth", type=str,
                       default="/share_zhuyixuan05/zhuyixuan05/sekai-game-walking/00100100001_0004650_0004950/encoded_video.pth")
    parser.add_argument("--start_frame", type=int, default=0)
    parser.add_argument("--initial_condition_frames", type=int, default=16)
    parser.add_argument("--frames_per_generation", type=int, default=8)
    parser.add_argument("--total_frames_to_generate", type=int, default=40)
    parser.add_argument("--max_history_frames", type=int, default=100)
    parser.add_argument("--use_real_poses", action="store_true", default=False)
    parser.add_argument("--dit_path", type=str, 
                       default="/share_zhuyixuan05/zhuyixuan05/ICLR2026/sekai/sekai_walking_framepack/step1000_framepack.ckpt")
    parser.add_argument("--output_path", type=str, 
                       default='/home/zhuyixuan05/ReCamMaster/sekai/infer_framepack_results/output_sekai_framepack_sliding.mp4')
    parser.add_argument("--prompt", type=str, 
                       default="A drone flying scene in a game world")
    parser.add_argument("--device", type=str, default="cuda")
    
    # ğŸ”§ æ–°å¢CFGå‚æ•°
    parser.add_argument("--use_camera_cfg", default=True,
                       help="ä½¿ç”¨Camera CFG")
    parser.add_argument("--camera_guidance_scale", type=float, default=2.0,
                       help="Camera guidance scale for CFG")
    parser.add_argument("--text_guidance_scale", type=float, default=1.0,
                       help="Text guidance scale for CFG")
    
    args = parser.parse_args()

    print(f"ğŸ”§ FramePack CFGç”Ÿæˆè®¾ç½®:")
    print(f"Camera CFG: {args.use_camera_cfg}")
    if args.use_camera_cfg:
        print(f"Camera guidance scale: {args.camera_guidance_scale}")
    print(f"Text guidance scale: {args.text_guidance_scale}")
    
    inference_sekai_framepack_sliding_window(
        condition_pth_path=args.condition_pth,
        dit_path=args.dit_path,
        output_path=args.output_path,
        start_frame=args.start_frame,
        initial_condition_frames=args.initial_condition_frames,
        frames_per_generation=args.frames_per_generation,
        total_frames_to_generate=args.total_frames_to_generate,
        max_history_frames=args.max_history_frames,
        device=args.device,
        prompt=args.prompt,
        use_real_poses=args.use_real_poses,
        # ğŸ”§ CFGå‚æ•°
        use_camera_cfg=args.use_camera_cfg,
        camera_guidance_scale=args.camera_guidance_scale,
        text_guidance_scale=args.text_guidance_scale
    )

if __name__ == "__main__":
    main()