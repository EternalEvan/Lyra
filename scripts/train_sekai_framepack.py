import torch
import torch.nn as nn
import lightning as pl
import wandb
import os
import copy
from diffsynth import WanVideoReCamMasterPipeline, ModelManager
import os
import json
import torch
import numpy as np
from PIL import Image
import imageio
import random
from torchvision.transforms import v2
from einops import rearrange
from pose_classifier import PoseClassifier

# cam_c2w, [N * 4 * 4]
# stride, frame stride
def get_traj_position_change(cam_c2w, stride=1):
    positions = cam_c2w[:, :3, 3]
    
    traj_coord = []
    tarj_angle = []
    for i in range(0, len(positions) - 2 * stride):
        v1 = positions[i + stride] - positions[i]
        v2 = positions[i + 2 * stride] - positions[i + stride]

        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        if norm1 < 1e-6 or norm2 < 1e-6:
            continue

        cos_angle = np.dot(v1, v2) / (norm1 * norm2)
        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))

        traj_coord.append(v1)
        tarj_angle.append(angle)
    
    return traj_coord, tarj_angle

def get_traj_rotation_change(cam_c2w, stride=1):
    rotations = cam_c2w[:, :3, :3]
    
    traj_rot_angle = []
    for i in range(0, len(rotations) - stride):
        z1 = rotations[i][:, 2]
        z2 = rotations[i + stride][:, 2]

        norm1 = np.linalg.norm(z1)
        norm2 = np.linalg.norm(z2)
        if norm1 < 1e-6 or norm2 < 1e-6:
            continue

        cos_angle = np.dot(z1, z2) / (norm1 * norm2)
        angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))
        traj_rot_angle.append(angle)

    return traj_rot_angle

def compute_relative_pose(pose_a, pose_b, use_torch=False):
    """
    è®¡ç®—ç›¸æœºBç›¸å¯¹äºç›¸æœºAçš„ç›¸å¯¹ä½å§¿çŸ©é˜µ
    """
    assert pose_a.shape == (4, 4), f"ç›¸æœºAå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_a.shape}"
    assert pose_b.shape == (4, 4), f"ç›¸æœºBå¤–å‚çŸ©é˜µå½¢çŠ¶åº”ä¸º(4,4)ï¼Œå®é™…ä¸º{pose_b.shape}"
    
    if use_torch:
        if not isinstance(pose_a, torch.Tensor):
            pose_a = torch.from_numpy(pose_a).float()
        if not isinstance(pose_b, torch.Tensor):
            pose_b = torch.from_numpy(pose_b).float()
        
        pose_a_inv = torch.inverse(pose_a)
        relative_pose = torch.matmul(pose_b, pose_a_inv)
    else:
        if not isinstance(pose_a, np.ndarray):
            pose_a = np.array(pose_a, dtype=np.float32)
        if not isinstance(pose_b, np.ndarray):
            pose_b = np.array(pose_b, dtype=np.float32)
        
        pose_a_inv = np.linalg.inv(pose_a)
        relative_pose = np.matmul(pose_b, pose_a_inv)
    
    return relative_pose

class DynamicSekaiDataset(torch.utils.data.Dataset):
    """æ”¯æŒFramePackæœºåˆ¶çš„åŠ¨æ€å†å²é•¿åº¦æ•°æ®é›† - æ”¯æŒå¤šä¸ªæ•°æ®é›†"""
    
    def __init__(self, base_paths, steps_per_epoch, 
                 min_condition_frames=10, max_condition_frames=40,
                 target_frames=10, height=900, width=1600):
        # ğŸ”§ ä¿®æ”¹ï¼šæ”¯æŒå¤šä¸ªæ•°æ®é›†è·¯å¾„
        if isinstance(base_paths, str):
            base_paths = [base_paths]  # å¦‚æœæ˜¯å•ä¸ªè·¯å¾„ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
        
        self.base_paths = base_paths
        self.min_condition_frames = min_condition_frames
        self.max_condition_frames = max_condition_frames
        self.target_frames = target_frames
        self.height = height
        self.width = width
        self.steps_per_epoch = steps_per_epoch
        self.pose_classifier = PoseClassifier()
        
        # VAEæ—¶é—´å‹ç¼©æ¯”ä¾‹
        self.time_compression_ratio = 4  # VAEå°†æ—¶é—´ç»´åº¦å‹ç¼©4å€
        
        # ğŸ”§ ä¿®æ”¹ï¼šæŸ¥æ‰¾æ‰€æœ‰æ•°æ®é›†ä¸­çš„å¤„ç†å¥½çš„åœºæ™¯
        self.scene_dirs = []
        self.dataset_info = {}  # è®°å½•æ¯ä¸ªåœºæ™¯å±äºå“ªä¸ªæ•°æ®é›†
        
        for base_path in self.base_paths:
            dataset_name = os.path.basename(base_path)  # è·å–æ•°æ®é›†åç§°
            print(f"ğŸ”§ æ‰«ææ•°æ®é›†: {dataset_name} ({base_path})")
            
            if os.path.exists(base_path):
                dataset_scenes = []
                for item in os.listdir(base_path):
                    scene_dir = os.path.join(base_path, item)
                    if os.path.isdir(scene_dir):
                        encoded_path = os.path.join(scene_dir, "encoded_video.pth")
                        if os.path.exists(encoded_path):
                            self.scene_dirs.append(scene_dir)
                            dataset_scenes.append(scene_dir)
                            self.dataset_info[scene_dir] = dataset_name
                
                print(f"  âœ… æ‰¾åˆ° {len(dataset_scenes)} ä¸ªåœºæ™¯")
            else:
                print(f"  âš ï¸ è·¯å¾„ä¸å­˜åœ¨: {base_path}")
        
        print(f"ğŸ”§ æ€»å…±æ‰¾åˆ° {len(self.scene_dirs)} ä¸ªåœºæ™¯")
        for dataset_name in set(self.dataset_info.values()):
            count = sum(1 for v in self.dataset_info.values() if v == dataset_name)
            print(f"  - {dataset_name}: {count} ä¸ªåœºæ™¯")
        
        assert len(self.scene_dirs) > 0, "No encoded scenes found!"

    def select_dynamic_segment_framepack(self, full_latents):
        """ğŸ”§ FramePacké£æ ¼çš„åŠ¨æ€é€‰æ‹©æ¡ä»¶å¸§å’Œç›®æ ‡å¸§ - ä¿®æ­£ç‰ˆï¼Œè€ƒè™‘å®é™…conditioné•¿åº¦"""
        total_lens = full_latents.shape[1]
        
        min_condition_compressed = self.min_condition_frames // self.time_compression_ratio
        max_condition_compressed = self.max_condition_frames // self.time_compression_ratio
        target_frames_compressed = self.target_frames // self.time_compression_ratio
        
        ratio = random.random()
        if ratio < 0.15:
            condition_frames_compressed = 1
        elif 0.15 <= ratio < 0.9:
            condition_frames_compressed = random.randint(min_condition_compressed, max_condition_compressed)
        else:
            condition_frames_compressed = target_frames_compressed
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å¸§æ•°
        min_required_frames = condition_frames_compressed + target_frames_compressed
        if total_lens < min_required_frames:
            return None
        
        start_frame_compressed = random.randint(0, total_lens - min_required_frames - 1)
        condition_end_compressed = start_frame_compressed + condition_frames_compressed
        target_end_compressed = condition_end_compressed + target_frames_compressed

        # ğŸ”§ ä¿®æ­£ï¼šFramePacké£æ ¼çš„ç´¢å¼•å¤„ç†
        latent_indices = torch.arange(condition_end_compressed, target_end_compressed)  # åªé¢„æµ‹æœªæ¥å¸§
        
        # ğŸ”§ ä¿®æ­£ï¼šæ ¹æ®å®é™…çš„condition_frames_compressedç”Ÿæˆç´¢å¼•
        # 1xå¸§ï¼šèµ·å§‹å¸§ + æœ€å1å¸§
        clean_latent_indices_start = torch.tensor([start_frame_compressed])
        clean_latent_1x_indices = torch.tensor([condition_end_compressed - 1])
        clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices])
        
        # ğŸ”§ 2xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®š
        if condition_frames_compressed >= 2:
            # å–æœ€å2å¸§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            clean_latent_2x_start = max(start_frame_compressed, condition_end_compressed - 2-1)
            clean_latent_2x_indices = torch.arange(clean_latent_2x_start, condition_end_compressed-1)
        else:
            # å¦‚æœconditionå¸§æ•°ä¸è¶³2å¸§ï¼Œåˆ›å»ºç©ºç´¢å¼•
            clean_latent_2x_indices = torch.tensor([], dtype=torch.long)
        
        # ğŸ”§ 4xå¸§ï¼šæ ¹æ®å®é™…conditioné•¿åº¦ç¡®å®šï¼Œæœ€å¤š16å¸§
        if condition_frames_compressed > 3:
            # å–æœ€å¤š16å¸§çš„å†å²ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            clean_4x_start = max(start_frame_compressed, condition_end_compressed - 16-3)
            clean_latent_4x_indices = torch.arange(clean_4x_start, condition_end_compressed-3)
        else:
            clean_latent_4x_indices = torch.tensor([], dtype=torch.long)
        
        # å¯¹åº”çš„åŸå§‹å…³é”®å¸§ç´¢å¼•
        keyframe_original_idx = []
        for compressed_idx in range(start_frame_compressed, target_end_compressed):
            keyframe_original_idx.append(compressed_idx * 4)
        
        return {
            'start_frame': start_frame_compressed,
            'condition_frames': condition_frames_compressed,
            'target_frames': target_frames_compressed,
            'condition_range': (start_frame_compressed, condition_end_compressed),
            'target_range': (condition_end_compressed, target_end_compressed),
            
            # FramePacké£æ ¼çš„ç´¢å¼•
            'latent_indices': latent_indices,
            'clean_latent_indices': clean_latent_indices,
            'clean_latent_2x_indices': clean_latent_2x_indices,
            'clean_latent_4x_indices': clean_latent_4x_indices,
            
            'keyframe_original_idx': keyframe_original_idx,
            'original_condition_frames': condition_frames_compressed * self.time_compression_ratio,
            'original_target_frames': target_frames_compressed * self.time_compression_ratio,
        }

    def create_pose_embeddings(self, cam_data, segment_info):
        """ğŸ”§ åˆ›å»ºpose embeddings - ä¸ºæ‰€æœ‰å¸§ï¼ˆcondition + targetï¼‰æå–cameraä¿¡æ¯ï¼Œæ”¯æŒ0å¡«å……"""
        cam_data_seq = cam_data['extrinsic']   # 300 * 4 * 4
        
        # ğŸ”§ ä¿®æ­£ï¼šä¸ºæ‰€æœ‰å¸§ï¼ˆcondition + targetï¼‰è®¡ç®—camera embedding
        start_frame = segment_info['start_frame'] * self.time_compression_ratio
        end_frame = segment_info['target_range'][1] * self.time_compression_ratio
        
        # ä¸ºæ‰€æœ‰å¸§è®¡ç®—ç›¸å¯¹pose
        all_keyframe_indices = []
        for compressed_idx in range(segment_info['start_frame'], segment_info['target_range'][1]):
            all_keyframe_indices.append(compressed_idx * 4)
        
        relative_cams = []
        for idx in all_keyframe_indices:
            cam_prev = cam_data_seq[idx]
            cam_next = cam_data_seq[idx + 4]
            relative_cam = compute_relative_pose(cam_prev, cam_next)
            relative_cams.append(torch.as_tensor(relative_cam[:3, :]))
        
        pose_embedding = torch.stack(relative_cams, dim=0)
        pose_embedding = rearrange(pose_embedding, 'b c d -> b (c d)')
        pose_embedding = pose_embedding.to(torch.bfloat16)

        return pose_embedding

    def prepare_framepack_inputs(self, full_latents, segment_info):
        """ğŸ”§ å‡†å¤‡FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥ - ä¿®æ­£ç‰ˆï¼Œæ­£ç¡®å¤„ç†ç©ºç´¢å¼•"""
        # ğŸ”§ ä¿®æ­£ï¼šå¤„ç†4ç»´è¾“å…¥ [C, T, H, W]ï¼Œæ·»åŠ batchç»´åº¦
        if len(full_latents.shape) == 4:
            full_latents = full_latents.unsqueeze(0)  # [C, T, H, W] -> [1, C, T, H, W]
            B, C, T, H, W = full_latents.shape
        else:
            B, C, T, H, W = full_latents.shape
        
        # ä¸»è¦latentsï¼ˆç”¨äºå»å™ªé¢„æµ‹ï¼‰
        latent_indices = segment_info['latent_indices']
        main_latents = full_latents[:, :, latent_indices, :, :]  # æ³¨æ„ç»´åº¦é¡ºåº
        
        # ğŸ”§ 1xæ¡ä»¶å¸§ï¼ˆèµ·å§‹å¸§ + æœ€å1å¸§ï¼‰
        clean_latent_indices = segment_info['clean_latent_indices']
        clean_latents = full_latents[:, :, clean_latent_indices, :, :]  # æ³¨æ„ç»´åº¦é¡ºåº
        
        # ğŸ”§ 4xæ¡ä»¶å¸§ - æ€»æ˜¯16å¸§ï¼Œç›´æ¥ç”¨çœŸå®ç´¢å¼• + 0å¡«å……
        clean_latent_4x_indices = segment_info['clean_latent_4x_indices']
        
        # åˆ›å»ºå›ºå®šé•¿åº¦16çš„latentsï¼Œåˆå§‹åŒ–ä¸º0
        clean_latents_4x = torch.zeros(B, C, 16, H, W, dtype=full_latents.dtype)
        clean_latent_4x_indices_final = torch.full((16,), -1, dtype=torch.long)  # -1è¡¨ç¤ºpadding
        
        # ğŸ”§ ä¿®æ­£ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„4xç´¢å¼•
        if len(clean_latent_4x_indices) > 0:
            actual_4x_frames = len(clean_latent_4x_indices)
            # ä»åå¾€å‰å¡«å……ï¼Œç¡®ä¿æœ€æ–°çš„å¸§åœ¨æœ€å
            start_pos = max(0, 16 - actual_4x_frames)
            end_pos = 16
            actual_start = max(0, actual_4x_frames - 16)  # å¦‚æœè¶…è¿‡16å¸§ï¼Œåªå–æœ€å16å¸§
            
            clean_latents_4x[:, :, start_pos:end_pos, :, :] = full_latents[:, :, clean_latent_4x_indices[actual_start:], :, :]
            clean_latent_4x_indices_final[start_pos:end_pos] = clean_latent_4x_indices[actual_start:]
        
        # ğŸ”§ 2xæ¡ä»¶å¸§ - æ€»æ˜¯2å¸§ï¼Œç›´æ¥ç”¨çœŸå®ç´¢å¼• + 0å¡«å……
        clean_latent_2x_indices = segment_info['clean_latent_2x_indices']
        
        # åˆ›å»ºå›ºå®šé•¿åº¦2çš„latentsï¼Œåˆå§‹åŒ–ä¸º0
        clean_latents_2x = torch.zeros(B, C, 2, H, W, dtype=full_latents.dtype)
        clean_latent_2x_indices_final = torch.full((2,), -1, dtype=torch.long)  # -1è¡¨ç¤ºpadding
        
        # ğŸ”§ ä¿®æ­£ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„2xç´¢å¼•
        if len(clean_latent_2x_indices) > 0:
            actual_2x_frames = len(clean_latent_2x_indices)
            # ä»åå¾€å‰å¡«å……ï¼Œç¡®ä¿æœ€æ–°çš„å¸§åœ¨æœ€å
            start_pos = max(0, 2 - actual_2x_frames)
            end_pos = 2
            actual_start = max(0, actual_2x_frames - 2)  # å¦‚æœè¶…è¿‡2å¸§ï¼Œåªå–æœ€å2å¸§
            
            clean_latents_2x[:, :, start_pos:end_pos, :, :] = full_latents[:, :, clean_latent_2x_indices[actual_start:], :, :]
            clean_latent_2x_indices_final[start_pos:end_pos] = clean_latent_2x_indices[actual_start:]
        
        # ğŸ”§ ç§»é™¤æ·»åŠ çš„batchç»´åº¦ï¼Œè¿”å›åŸå§‹æ ¼å¼
        if B == 1:
            main_latents = main_latents.squeeze(0)  # [1, C, T, H, W] -> [C, T, H, W]
            clean_latents = clean_latents.squeeze(0)
            clean_latents_2x = clean_latents_2x.squeeze(0)
            clean_latents_4x = clean_latents_4x.squeeze(0)
        
        return {
            'latents': main_latents,
            'clean_latents': clean_latents,
            'clean_latents_2x': clean_latents_2x,
            'clean_latents_4x': clean_latents_4x,
            'latent_indices': segment_info['latent_indices'],
            'clean_latent_indices': segment_info['clean_latent_indices'],
            'clean_latent_2x_indices': clean_latent_2x_indices_final,  # ğŸ”§ ä½¿ç”¨çœŸå®ç´¢å¼•ï¼ˆå«-1å¡«å……ï¼‰
            'clean_latent_4x_indices': clean_latent_4x_indices_final,  # ğŸ”§ ä½¿ç”¨çœŸå®ç´¢å¼•ï¼ˆå«-1å¡«å……ï¼‰
        }

    def __getitem__(self, index):
        while True:
            try:
                # ğŸ”§ ä¿®æ”¹ï¼šéšæœºé€‰æ‹©ä¸€ä¸ªåœºæ™¯ï¼ˆä»æ‰€æœ‰æ•°æ®é›†ä¸­ï¼‰
                scene_dir = random.choice(self.scene_dirs)
                dataset_name = self.dataset_info[scene_dir]  # è·å–è¯¥åœºæ™¯æ‰€å±çš„æ•°æ®é›†
                
                # åŠ è½½ç¼–ç çš„è§†é¢‘æ•°æ®
                encoded_data = torch.load(
                    os.path.join(scene_dir, "encoded_video.pth"),
                    weights_only=False,
                    map_location="cpu"
                )
                
                full_latents = encoded_data['latents']  # [C, T, H, W]
                cam_data = encoded_data['cam_emb']
                
                # ğŸ”§ ä½¿ç”¨FramePacké£æ ¼çš„æ®µè½é€‰æ‹©
                segment_info = self.select_dynamic_segment_framepack(full_latents)
                if segment_info is None:
                    continue
                
                # ğŸ”§ ä¿®æ­£ï¼šä¸ºæ‰€æœ‰å¸§åˆ›å»ºpose embeddingsï¼ˆä¸å¸¦maskï¼‰
                all_camera_embeddings = self.create_pose_embeddings(cam_data, segment_info)
                if all_camera_embeddings is None:
                    continue
                
                # ğŸ”§ å‡†å¤‡FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥ï¼ˆåœ¨è¿™é‡Œå¤„ç†0å¡«å……ï¼‰
                framepack_inputs = self.prepare_framepack_inputs(full_latents, segment_info)
                
                n = segment_info["condition_frames"]
                m = segment_info['target_frames']
                
                # ğŸ”§ ç®€åŒ–ï¼šåƒtrain_sekai_walkingä¸€æ ·å¤„ç†camera embedding with mask
                mask = torch.zeros(n+m, dtype=torch.float32)
                mask[:n] = 1.0  # conditionå¸§æ ‡è®°ä¸º1
                mask = mask.view(-1, 1)
                
                # æ·»åŠ maskåˆ°camera embeddings
                camera_with_mask = torch.cat([all_camera_embeddings, mask], dim=1)
                
                result = {
                    # ğŸ”§ FramePacké£æ ¼çš„å¤šå°ºåº¦è¾“å…¥ - ç°åœ¨éƒ½æœ‰å›ºå®šé•¿åº¦
                    "latents": framepack_inputs['latents'],  # ä¸»è¦é¢„æµ‹ç›®æ ‡
                    "clean_latents": framepack_inputs['clean_latents'],  # æ¡ä»¶å¸§(2å¸§)
                    "clean_latents_2x": framepack_inputs['clean_latents_2x'],  # 2xæ¡ä»¶å¸§(2å¸§ï¼Œä¸è¶³ç”¨0å¡«å……)
                    "clean_latents_4x": framepack_inputs['clean_latents_4x'],  # 4xæ¡ä»¶å¸§(16å¸§ï¼Œä¸è¶³ç”¨0å¡«å……)
                    "latent_indices": framepack_inputs['latent_indices'],
                    "clean_latent_indices": framepack_inputs['clean_latent_indices'],
                    "clean_latent_2x_indices": framepack_inputs['clean_latent_2x_indices'],  # å›ºå®šé•¿åº¦
                    "clean_latent_4x_indices": framepack_inputs['clean_latent_4x_indices'],  # å›ºå®šé•¿åº¦
                    
                    # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä¼ é€’å¸¦maskçš„camera embeddings
                    "camera": camera_with_mask,  # æ‰€æœ‰å¸§çš„camera embeddingsï¼ˆå¸¦maskï¼‰
                    
                    "prompt_emb": encoded_data["prompt_emb"],
                    "image_emb": encoded_data.get("image_emb", {}),
                    
                    "condition_frames": n,
                    "target_frames": m,
                    "scene_name": os.path.basename(scene_dir),
                    "dataset_name": dataset_name,  # ğŸ”§ æ–°å¢ï¼šè®°å½•æ•°æ®é›†åç§°
                    "original_condition_frames": segment_info['original_condition_frames'],
                    "original_target_frames": segment_info['original_target_frames'],
                }
                
                return result
                
            except Exception as e:
                print(f"Error loading sample: {e}")
                import traceback
                traceback.print_exc()
                continue

    def __len__(self):
        return self.steps_per_epoch

def replace_dit_model_in_manager():
    """åœ¨æ¨¡å‹åŠ è½½å‰æ›¿æ¢DiTæ¨¡å‹ç±»"""
    from diffsynth.models.wan_video_dit_recam_future import WanModelFuture
    from diffsynth.configs.model_config import model_loader_configs
    
    # ä¿®æ”¹model_loader_configsä¸­çš„é…ç½®
    for i, config in enumerate(model_loader_configs):
        keys_hash, keys_hash_with_shape, model_names, model_classes, model_resource = config
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«wan_video_ditæ¨¡å‹
        if 'wan_video_dit' in model_names:
            # æ‰¾åˆ°wan_video_ditçš„ç´¢å¼•å¹¶æ›¿æ¢ä¸ºWanModelFuture
            new_model_names = []
            new_model_classes = []
            
            for name, cls in zip(model_names, model_classes):
                if name == 'wan_video_dit':
                    new_model_names.append(name)  # ä¿æŒåç§°ä¸å˜
                    new_model_classes.append(WanModelFuture)  # æ›¿æ¢ä¸ºæ–°çš„ç±»
                    print(f"âœ… æ›¿æ¢äº†æ¨¡å‹ç±»: {name} -> WanModelFuture")
                else:
                    new_model_names.append(name)
                    new_model_classes.append(cls)
            
            # æ›´æ–°é…ç½®
            model_loader_configs[i] = (keys_hash, keys_hash_with_shape, new_model_names, new_model_classes, model_resource)
    
class DynamicLightningModelForTrain(pl.LightningModule):
    def __init__(
        self,
        dit_path,
        learning_rate=1e-5,
        use_gradient_checkpointing=True,
        use_gradient_checkpointing_offload=False,
        resume_ckpt_path=None
    ):
        super().__init__()
        replace_dit_model_in_manager()  # åœ¨è¿™é‡Œè°ƒç”¨
        model_manager = ModelManager(torch_dtype=torch.bfloat16, device="cpu")
        if os.path.isfile(dit_path):
            model_manager.load_models([dit_path])
        else:
            dit_path = dit_path.split(",")
            model_manager.load_models([dit_path])
        model_manager.load_models(["models/Wan-AI/Wan2.1-T2V-1.3B/Wan2.1_VAE.pth"])
        
        self.pipe = WanVideoReCamMasterPipeline.from_model_manager(model_manager)
        self.pipe.scheduler.set_timesteps(1000, training=True)

        # ğŸ”§ æ·»åŠ FramePackçš„clean_x_embedder - å‚è€ƒhunyuan_video_packed.py
        self.add_framepack_components()

        # æ·»åŠ ç›¸æœºç¼–ç å™¨
        dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
        for block in self.pipe.dit.blocks:
            block.cam_encoder = nn.Linear(13 , dim)
            block.projector = nn.Linear(dim, dim)
            block.cam_encoder.weight.data.zero_()
            block.cam_encoder.bias.data.zero_()
            block.projector.weight = nn.Parameter(torch.eye(dim))
            block.projector.bias = nn.Parameter(torch.zeros(dim))
        
        if resume_ckpt_path is not None:
            state_dict = torch.load(resume_ckpt_path, map_location="cpu")
            self.pipe.dit.load_state_dict(state_dict, strict=True)
            print('load checkpoint:', resume_ckpt_path)

        self.freeze_parameters()
        
        # åªè®­ç»ƒç›¸æœºç›¸å…³å’Œæ³¨æ„åŠ›æ¨¡å—ä»¥åŠFramePackç›¸å…³ç»„ä»¶
        for name, module in self.pipe.denoising_model().named_modules():
            if any(keyword in name for keyword in ["cam_encoder", "projector", "self_attn", "clean_x_embedder"]):
                for param in module.parameters():
                    param.requires_grad = True
        
        self.learning_rate = learning_rate
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.use_gradient_checkpointing_offload = use_gradient_checkpointing_offload
        
        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        self.vis_dir = "sekai_dynamic/visualizations_dynamic"
        os.makedirs(self.vis_dir, exist_ok=True)

    def add_framepack_components(self):
        """ğŸ”§ æ·»åŠ FramePackç›¸å…³ç»„ä»¶ - å‚è€ƒhunyuan_video_packed.py"""
        if not hasattr(self.pipe.dit, 'clean_x_embedder'):
            inner_dim = self.pipe.dit.blocks[0].self_attn.q.weight.shape[0]
            
            class CleanXEmbedder(nn.Module):
                def __init__(self, inner_dim):
                    super().__init__()
                    # å‚è€ƒhunyuan_video_packed.pyçš„è®¾è®¡
                    self.proj = nn.Conv3d(16, inner_dim, kernel_size=(1, 2, 2), stride=(1, 2, 2))
                    self.proj_2x = nn.Conv3d(16, inner_dim, kernel_size=(2, 4, 4), stride=(2, 4, 4))
                    self.proj_4x = nn.Conv3d(16, inner_dim, kernel_size=(4, 8, 8), stride=(4, 8, 8))
                
                def forward(self, x, scale="1x"):
                    if scale == "1x":
                        return self.proj(x)
                    elif scale == "2x":
                        return self.proj_2x(x)
                    elif scale == "4x":
                        return self.proj_4x(x)
                    else:
                        raise ValueError(f"Unsupported scale: {scale}")
            
            self.pipe.dit.clean_x_embedder = CleanXEmbedder(inner_dim)
            print("âœ… æ·»åŠ äº†FramePackçš„clean_x_embedderç»„ä»¶")
        
    def freeze_parameters(self):
        self.pipe.requires_grad_(False)
        self.pipe.eval()
        self.pipe.denoising_model().train()

    def training_step(self, batch, batch_idx):
        """ğŸ”§ ä½¿ç”¨FramePacké£æ ¼çš„è®­ç»ƒæ­¥éª¤ - ä¿®æ­£ç»´åº¦å¤„ç†"""
        condition_frames = batch["condition_frames"][0].item()
        target_frames = batch["target_frames"][0].item()
        
        original_condition_frames = batch.get("original_condition_frames", [condition_frames * 4])[0]
        original_target_frames = batch.get("original_target_frames", [target_frames * 4])[0]

        dataset_name = batch.get("dataset_name", ["unknown"])[0]
        scene_name = batch.get("scene_name", ["unknown"])[0]        
        # ğŸ”§ å‡†å¤‡FramePacké£æ ¼çš„è¾“å…¥ - ç¡®ä¿æœ‰batchç»´åº¦
        latents = batch["latents"].to(self.device)
        if len(latents.shape) == 4:  # [C, T, H, W]
            latents = latents.unsqueeze(0)  # -> [1, C, T, H, W]
        
        # ğŸ”§ æ¡ä»¶è¾“å…¥ï¼ˆå¤„ç†ç©ºå¼ é‡å’Œç»´åº¦ï¼‰
        clean_latents = batch["clean_latents"].to(self.device) if batch["clean_latents"].numel() > 0 else None
        if clean_latents is not None and len(clean_latents.shape) == 4:
            clean_latents = clean_latents.unsqueeze(0)
        
        clean_latents_2x = batch["clean_latents_2x"].to(self.device) if batch["clean_latents_2x"].numel() > 0 else None
        if clean_latents_2x is not None and len(clean_latents_2x.shape) == 4:
            clean_latents_2x = clean_latents_2x.unsqueeze(0)
        
        clean_latents_4x = batch["clean_latents_4x"].to(self.device) if batch["clean_latents_4x"].numel() > 0 else None
        if clean_latents_4x is not None and len(clean_latents_4x.shape) == 4:
            clean_latents_4x = clean_latents_4x.unsqueeze(0)
        
        # ğŸ”§ ç´¢å¼•ï¼ˆå¤„ç†ç©ºå¼ é‡ï¼‰
        latent_indices = batch["latent_indices"].to(self.device)
        clean_latent_indices = batch["clean_latent_indices"].to(self.device) if batch["clean_latent_indices"].numel() > 0 else None
        clean_latent_2x_indices = batch["clean_latent_2x_indices"].to(self.device) if batch["clean_latent_2x_indices"].numel() > 0 else None
        clean_latent_4x_indices = batch["clean_latent_4x_indices"].to(self.device) if batch["clean_latent_4x_indices"].numel() > 0 else None
        
        # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å¸¦maskçš„camera embeddings
        cam_emb = batch["camera"].to(self.device)
        camera_dropout_prob = 0.1  # 10%æ¦‚ç‡ä¸¢å¼ƒcameraæ¡ä»¶
        if random.random() < camera_dropout_prob:
            # åˆ›å»ºé›¶camera embedding
            cam_emb = torch.zeros_like(cam_emb)
            print("åº”ç”¨camera dropout for CFG training")
        
        prompt_emb = batch["prompt_emb"]
        prompt_emb["context"] = prompt_emb["context"][0].to(self.device)
        image_emb = batch["image_emb"]

        if "clip_feature" in image_emb:
            image_emb["clip_feature"] = image_emb["clip_feature"][0].to(self.device)
        if "y" in image_emb:
            image_emb["y"] = image_emb["y"][0].to(self.device)

        # Lossè®¡ç®—
        self.pipe.device = self.device
        noise = torch.randn_like(latents)
        timestep_id = torch.randint(0, self.pipe.scheduler.num_train_timesteps, (1,))
        timestep = self.pipe.scheduler.timesteps[timestep_id].to(dtype=self.pipe.torch_dtype, device=self.pipe.device)
        
        # ğŸ”§ FramePacké£æ ¼çš„å™ªå£°å¤„ç†
        noisy_condition_latents = None
        if clean_latents is not None:
            noisy_condition_latents = copy.deepcopy(clean_latents)
            is_add_noise = random.random()
            if is_add_noise > 0.2:  # 80%æ¦‚ç‡æ·»åŠ å™ªå£°
                noise_cond = torch.randn_like(clean_latents)
                timestep_id_cond = torch.randint(0, self.pipe.scheduler.num_train_timesteps//4*3, (1,))
                timestep_cond = self.pipe.scheduler.timesteps[timestep_id_cond].to(dtype=self.pipe.torch_dtype, device=self.pipe.device)
                noisy_condition_latents = self.pipe.scheduler.add_noise(clean_latents, noise_cond, timestep_cond)

        extra_input = self.pipe.prepare_extra_input(latents)
        origin_latents = copy.deepcopy(latents)
        noisy_latents = self.pipe.scheduler.add_noise(latents, noise, timestep)
        
        training_target = self.pipe.scheduler.training_target(latents, noise, timestep)
        
        # ğŸ”§ ä½¿ç”¨FramePacké£æ ¼çš„forwardè°ƒç”¨
        noise_pred = self.pipe.denoising_model()(
            noisy_latents, 
            timestep=timestep, 
            cam_emb=cam_emb,  # ğŸ”§ ç®€åŒ–ï¼šç›´æ¥ä¼ é€’å¸¦maskçš„camera embeddings
            # ğŸ”§ FramePacké£æ ¼çš„æ¡ä»¶è¾“å…¥
            latent_indices=latent_indices,
            clean_latents=noisy_condition_latents if noisy_condition_latents is not None else clean_latents,
            clean_latent_indices=clean_latent_indices,
            clean_latents_2x=clean_latents_2x,
            clean_latent_2x_indices=clean_latent_2x_indices,
            clean_latents_4x=clean_latents_4x,
            clean_latent_4x_indices=clean_latent_4x_indices,
            **prompt_emb, 
            **extra_input, 
            **image_emb,
            use_gradient_checkpointing=self.use_gradient_checkpointing,
            use_gradient_checkpointing_offload=self.use_gradient_checkpointing_offload
        )
        
        # è®¡ç®—lossï¼ˆç°åœ¨noise_predåªåŒ…å«é¢„æµ‹ç›®æ ‡ï¼Œä¸åŒ…å«æ¡ä»¶éƒ¨åˆ†ï¼‰
        loss = torch.nn.functional.mse_loss(noise_pred.float(), training_target.float())
        loss = loss * self.pipe.scheduler.training_weight(timestep)
        print('--------loss------------:', loss)

        return loss

    def configure_optimizers(self):
        trainable_modules = filter(lambda p: p.requires_grad, self.pipe.denoising_model().parameters())
        optimizer = torch.optim.AdamW(trainable_modules, lr=self.learning_rate)
        return optimizer
    
    def on_save_checkpoint(self, checkpoint):
        checkpoint_dir = "/share_zhuyixuan05/zhuyixuan05/ICLR2026/sekai/sekai_walking_framepack"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        current_step = self.global_step
        checkpoint.clear()
        
        state_dict = self.pipe.denoising_model().state_dict()
        torch.save(state_dict, os.path.join(checkpoint_dir, f"step{current_step}_framepack.ckpt"))
        print(f"Saved FramePack model checkpoint: step{current_step}_framepack.ckpt")

def train_dynamic(args):
    """è®­ç»ƒæ”¯æŒFramePackæœºåˆ¶çš„åŠ¨æ€å†å²é•¿åº¦æ¨¡å‹ - æ”¯æŒå¤šæ•°æ®é›†"""
    # ğŸ”§ ä¿®æ”¹ï¼šæ”¯æŒå¤šä¸ªæ•°æ®é›†è·¯å¾„
    dataset_paths = [
        "/share_zhuyixuan05/zhuyixuan05/sekai-game-drone",
        "/share_zhuyixuan05/zhuyixuan05/sekai-game-walking"
    ]
    
    dataset = DynamicSekaiDataset(
        dataset_paths,  # ğŸ”§ ä¼ å…¥å¤šä¸ªæ•°æ®é›†è·¯å¾„
        steps_per_epoch=args.steps_per_epoch,
        min_condition_frames=args.min_condition_frames,
        max_condition_frames=args.max_condition_frames,
        target_frames=args.target_frames,
    )
    
    dataloader = torch.utils.data.DataLoader(
        dataset,
        shuffle=True,
        batch_size=1,
        num_workers=args.dataloader_num_workers
    )
    
    model = DynamicLightningModelForTrain(
        dit_path=args.dit_path,
        learning_rate=args.learning_rate,
        use_gradient_checkpointing=args.use_gradient_checkpointing,
        use_gradient_checkpointing_offload=args.use_gradient_checkpointing_offload,
        resume_ckpt_path=args.resume_ckpt_path,
    )

    # wandb.init(
    #     project="sekai-multi-dataset-framepack-recam",  # ğŸ”§ ä¿®æ”¹é¡¹ç›®åç§°
    #     name=f"multi-dataset-framepack-{args.min_condition_frames}-{args.max_condition_frames}",
    # )

    trainer = pl.Trainer(
        max_epochs=args.max_epochs,
        accelerator="gpu",
        devices="auto",
        precision="bf16",
        strategy=args.training_strategy,
        default_root_dir=args.output_path,
        accumulate_grad_batches=args.accumulate_grad_batches,
        logger=False,
        callbacks=[],
    )
    trainer.fit(model, dataloader)

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description="Train FramePack Dynamic ReCamMaster with Multiple Datasets")
    # ğŸ”§ ä¿®æ”¹ï¼šdataset_pathå‚æ•°ç°åœ¨åœ¨ä»£ç ä¸­ç¡¬ç¼–ç ï¼Œä½†ä¿ç•™ä»¥ä¾¿å…¼å®¹
    parser.add_argument("--dataset_path", type=str, 
                       default="/share_zhuyixuan05/zhuyixuan05/sekai-game-walking",
                       help="ä¸»æ•°æ®é›†è·¯å¾„ï¼ˆå®é™…ä¼šä½¿ç”¨ä»£ç ä¸­çš„å¤šæ•°æ®é›†é…ç½®ï¼‰")
    parser.add_argument("--dit_path", type=str, default="models/Wan-AI/Wan2.1-T2V-1.3B/diffusion_pytorch_model.safetensors")
    parser.add_argument("--output_path", type=str, default="./")
    parser.add_argument("--learning_rate", type=float, default=1e-5)
    parser.add_argument("--steps_per_epoch", type=int, default=8000)
    parser.add_argument("--max_epochs", type=int, default=3000)
    parser.add_argument("--min_condition_frames", type=int, default=8, help="æœ€å°æ¡ä»¶å¸§æ•°")
    parser.add_argument("--max_condition_frames", type=int, default=120, help="æœ€å¤§æ¡ä»¶å¸§æ•°")
    parser.add_argument("--target_frames", type=int, default=32, help="ç›®æ ‡å¸§æ•°")
    parser.add_argument("--dataloader_num_workers", type=int, default=4)
    parser.add_argument("--accumulate_grad_batches", type=int, default=1)
    parser.add_argument("--training_strategy", type=str, default="deepspeed_stage_1")
    parser.add_argument("--use_gradient_checkpointing", action="store_true")
    parser.add_argument("--use_gradient_checkpointing_offload", action="store_true")
    parser.add_argument("--resume_ckpt_path", type=str, default="/share_zhuyixuan05/zhuyixuan05/ICLR2026/sekai/sekai_walking_framepack/step9144_framepack.ckpt")
    
    args = parser.parse_args()
    
    print("ğŸ”§ ä½¿ç”¨å¤šæ•°æ®é›†è®­ç»ƒ:")
    print("  - /share_zhuyixuan05/zhuyixuan05/sekai-game-drone")
    print("  - /share_zhuyixuan05/zhuyixuan05/sekai-game-walking")
    
    train_dynamic(args)